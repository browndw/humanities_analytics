# Python Packages for This Course {.unnumbered}

This course leverages several specialized Python packages designed for humanities text analysis. These tools have been developed specifically for computational work in linguistics, literary studies, and digital humanities research.

::: {.callout-note}
## Installation in Colab
All mini labs include installation instructions at the beginning. When using Google Colab, you'll install packages using:
```python
!pip install package_name
```
:::

## Core Packages

### docuscospacy

**Purpose:** Rhetorical and functional text analysis using DocuScope categories

**What it does:**  
`docuscospacy` provides automated tagging of texts using the DocuScope dictionary, a comprehensive taxonomy of rhetorical and functional categories developed by David Kaufer and Suguru Ishizaki. The package includes a specially-trained spaCy model that tags text for both part-of-speech and rhetorical categories.

**Key features:**

- 67 million+ pattern dictionary covering rhetorical moves, speech acts, and functional language
- Pre-trained spaCy model (`en_docusco_spacy`) with CLAWS7 tagset
- High-performance processing with Polars DataFrames
- Built-in corpus analysis functions (frequencies, keyness, collocations)
- Integration with topic modeling and visualization tools

**Example use cases:**

- Analyzing persuasive strategies in political speeches
- Comparing formality levels across academic disciplines
- Identifying stance markers in student writing
- Tracking rhetorical patterns in literary genres

**Documentation:** [docuscospacy.readthedocs.org](https://docuscospacy.readthedocs.org)  
**GitHub:** [github.com/browndw/docuscospacy](https://github.com/browndw/docuscospacy)  
**Interactive Demo:** [docuscope-ca.eberly.cmu.edu](https://docuscope-ca.eberly.cmu.edu/)

**Installation:**
```python
# Install the package
pip install docuscospacy

# Install the pre-trained model
pip install "en_docusco_spacy @ https://huggingface.co/browndw/en_docusco_spacy/resolve/main/en_docusco_spacy-1.5-py3-none-any.whl"
```

---

### pybiber

**Purpose:** Multi-Dimensional Analysis (MDA) of text registers and genres

**What it does:**  
`pybiber` implements Douglas Biber's influential methodology for analyzing linguistic variation across text types. It automatically extracts 67 lexicogrammatical features (tense markers, pronouns, subordination, modals, etc.) and performs factor analysis to identify underlying dimensions of variation.

**Key features:**

- Automated extraction of 67 linguistic features from Biber (1988)
- Full implementation of Multi-Dimensional Analysis methodology
- Principal Component Analysis (PCA) as alternative to MDA
- Built-in visualization tools (scree plots, dimension plots, biplots)
- High-performance processing with spaCy and Polars
- Support for custom corpora and comparative analysis

**Example use cases:**

- Comparing academic writing across disciplines
- Analyzing register variation in social media
- Tracking language change over time
- Classifying text genres based on linguistic features
- Studying differences between spoken and written language

**Documentation:** [browndw.github.io/pybiber](https://browndw.github.io/pybiber)  
**GitHub:** [github.com/browndw/pybiber](https://github.com/browndw/pybiber)  
**Related:** See [pseudobibeR](https://cran.r-project.org/web/packages/pseudobibeR/index.html) for R implementation

**Installation:**
```python
pip install pybiber
python -m spacy download en_core_web_sm
```

---

### moodswing

**Purpose:** Sentiment trajectory analysis for narrative texts

**What it does:**  
Inspired by Matthew Jockers' `syuzhet` R package, `moodswing` tracks emotional arcs over the course of narratives. It provides both dictionary-based and neural approaches to sentiment analysis, with specialized tools for smoothing and visualizing sentiment trajectories.

**Key features:**

- Four proven sentiment lexicons (Syuzhet, AFINN, Bing, NRC)
- Dictionary-based and spaCy-based sentiment scoring
- DCT (Discrete Cosine Transform) smoothing for narrative structure
- Rolling window smoothing options
- Publication-ready visualization tools
- Support for multi-language analysis (via NRC lexicon)

**Example use cases:**

- Visualizing emotional arcs in novels and memoirs
- Comparing narrative structures across genres
- Analyzing sentiment patterns in social media narratives
- Studying emotional progression in political speeches
- Identifying plot structure through sentiment

**Documentation:** [browndw.github.io/moodswing](https://browndw.github.io/moodswing)  
**GitHub:** [github.com/browndw/moodswing](https://github.com/browndw/moodswing)

**Installation:**
```python
pip install moodswing
# Optional: for spaCy support
python -m spacy download en_core_web_sm
```

---

### google_ngrams

**Purpose:** Diachronic analysis using Google Books Ngram data

**What it does:**  
`google_ngrams` provides tools for downloading, processing, and analyzing the Google Books Ngram dataset, enabling large-scale studies of language change over centuries. It includes functions for smoothing, clustering, and visualizing temporal trends.

**Key features:**

- Direct access to Google Books Ngram data (1500-2019)
- Multiple corpus options (English, Spanish, German, French, etc.)
- Time-series analysis and visualization
- Clustering algorithms for grouping similar trajectories
- Statistical smoothing functions
- Export capabilities for further analysis

**Example use cases:**

- Tracking word frequency changes over centuries
- Comparing usage patterns across historical periods
- Identifying semantic shifts in vocabulary
- Analyzing the rise and fall of cultural concepts
- Studying historical discourse patterns

**Documentation:** [browndw.github.io/google_ngrams](https://browndw.github.io/google_ngrams) (coming soon)  
**GitHub:** [github.com/browndw/google_ngrams](https://github.com/browndw/google_ngrams)

**Installation:**
```python
pip install google-ngrams
```

---

## Essential Dependencies

These packages rely on several widely-used Python libraries that you should be familiar with:

### spaCy
**Purpose:** Industrial-strength Natural Language Processing  
**Website:** [spacy.io](https://spacy.io)  
**Use in course:** Part-of-speech tagging, dependency parsing, named entity recognition  
**Installation:** `pip install spacy` + `python -m spacy download en_core_web_sm`

### Polars
**Purpose:** Fast DataFrame library (similar to pandas but faster)  
**Website:** [pola.rs](https://pola.rs)  
**Use in course:** Data manipulation, corpus processing, feature extraction  
**Installation:** `pip install polars`

---

## Machine Learning & Statistics

### scikit-learn
**Purpose:** Machine learning and statistical modeling  
**Website:** [scikit-learn.org](https://scikit-learn.org)

**What it does:**  
Python's most popular machine learning library, providing tools for classification, clustering, dimensionality reduction, and model evaluation.

**Use in course:**
- **Classification** (Mini Lab 12, Classification tutorial): Random Forest classifiers, train/test splits, accuracy metrics
- **Clustering** (Cluster Analysis tutorial): K-means clustering, hierarchical clustering, silhouette scores
- **Dimensionality reduction** (Contextual Embeddings, Cluster Analysis): PCA for visualization, TruncatedSVD for topic modeling
- **Text vectorization**: TF-IDF vectorization for document similarity
- **Model evaluation**: Confusion matrices, classification reports, cross-validation

**Key modules used:**
- `sklearn.ensemble.RandomForestClassifier` - Classification
- `sklearn.cluster.KMeans` - K-means clustering
- `sklearn.decomposition.PCA` - Principal Component Analysis
- `sklearn.model_selection.train_test_split` - Data splitting
- `sklearn.metrics` - Accuracy, precision, recall, F1-score

**Installation:** `pip install scikit-learn`

### sentence-transformers
**Purpose:** State-of-the-art sentence and document embeddings  
**Website:** [sbert.net](https://www.sbert.net)

**What it does:**  
Provides pre-trained models for generating semantic embeddings of sentences and documents. Built on transformer models like BERT, these embeddings capture meaning and enable semantic similarity comparisons.

**Use in course:**
- **Contextual embeddings** (Contextual Embeddings tutorial): Generating sentence embeddings with `all-MiniLM-L6-v2`
- **Semantic similarity**: Finding similar documents based on meaning
- **Clustering**: Grouping texts by semantic content
- **Classification**: Using embeddings as features for genre prediction

**Example models:**
- `all-MiniLM-L6-v2` - Fast, lightweight, good for most tasks
- `paraphrase-MiniLM-L6-v2` - Optimized for paraphrase detection

**Installation:** `pip install sentence-transformers`

### statsmodels
**Purpose:** Statistical modeling and hypothesis testing  
**Website:** [statsmodels.org](https://www.statsmodels.org)

**What it does:**  
Provides statistical tests, regression models, and diagnostic tools for rigorous statistical analysis.

**Use in course:**
- **VIF (Variance Inflation Factor)**: Testing for multicollinearity in regression (Correlation tutorial)
- **Time series analysis**: ARIMA models, trend detection (Time Series tutorial)
- **ANOVA**: Testing group differences (ANOVA tutorial)
- **Regression diagnostics**: Checking model assumptions

**Installation:** `pip install statsmodels`

### scipy
**Purpose:** Scientific computing and statistical functions  
**Website:** [scipy.org](https://scipy.org)

**What it does:**  
Core library for scientific computing, providing statistical tests, optimization, and hierarchical clustering.

**Use in course:**
- **Hierarchical clustering** (Cluster Analysis tutorial): `scipy.cluster.hierarchy` for dendrograms, linkage methods
- **Statistical tests**: T-tests, chi-square, correlation tests
- **Distance metrics**: Cosine distance, Euclidean distance for clustering

**Installation:** `pip install scipy`

---

## Network & Visualization

### networkx
**Purpose:** Network analysis and graph visualization  
**Website:** [networkx.org](https://networkx.org)

**What it does:**  
Creates and analyzes network structures, useful for visualizing relationships between linguistic features or texts.

**Use in course:**
- **Correlation networks** (Correlation tutorial): Visualizing feature correlations as network graphs
- **Word co-occurrence networks**: Showing which words appear together
- **Document similarity networks**: Connecting similar texts

**Installation:** `pip install networkx`

### matplotlib
**Purpose:** Fundamental plotting and visualization  
**Website:** [matplotlib.org](https://matplotlib.org)  
**Use in course:** Creating charts, graphs, and visualizations  
**Installation:** `pip install matplotlib`

### seaborn
**Purpose:** Statistical data visualization (built on matplotlib)  
**Website:** [seaborn.pydata.org](https://seaborn.pydata.org)  
**Use in course:** Heatmaps, boxplots, distribution plots, styled figures  
**Installation:** `pip install seaborn`

### great_tables
**Purpose:** Publication-quality table formatting  
**Website:** [posit-dev.github.io/great_tables](https://posit-dev.github.io/great_tables)  
**Use in course:** Formatting results for reports and papers, LaTeX export  
**Installation:** `pip install great_tables`

---

## Word Embeddings (Optional)

### gensim
**Purpose:** Topic modeling and word embeddings  
**Website:** [radimrehurek.com/gensim](https://radimrehurek.com/gensim)

**What it does:**  
Provides implementations of word2vec, doc2vec, and other embedding algorithms for learning word representations from text.

**Use in course:**
- **Word2Vec** (Mini Lab 9): Training word embeddings on custom corpora
- **Semantic similarity**: Finding words with similar meanings
- **Analogies**: Exploring semantic relationships (king - man + woman â‰ˆ queen)

**Installation:** `pip install gensim`

**Note:** While covered in Mini Lab 9, most semantic tasks in tutorials use `sentence-transformers` instead, which provides pre-trained models without requiring corpus-specific training.

---

## Learning Path

**Weeks 1-3:** Core Text Processing

- `spaCy` + `docuscospacy`: Basic NLP and rhetorical analysis
- `polars`: Data manipulation
- `matplotlib` + `seaborn`: Visualization basics
- Learn frequency analysis, keyness, collocations

**Weeks 4-6:** Sentiment & Register Analysis

- `moodswing`: Sentiment trajectories and narrative arcs
- `pybiber`: Multi-dimensional analysis of registers
- `scipy`: Statistical tests (t-tests, ANOVA)
- `statsmodels`: Correlation, VIF, regression diagnostics

**Weeks 7-9:** Advanced Methods

- `scikit-learn`: Classification, clustering, PCA
- `sentence-transformers`: Contextual embeddings
- `networkx`: Network visualization
- `great_tables`: Professional reporting

**Weeks 10-12:** Integration & Projects

- Combine multiple methods for robust analysis
- Optional: `gensim` for custom word2vec models
- Optional: `google_ngrams` for diachronic studies
- Apply full toolkit to final projects

---

## Getting Help

Each package has comprehensive documentation with tutorials, API references, and examples. If you encounter issues:

1. **Check the documentation** - Most questions are answered there
2. **Review mini labs** - See working examples of each package
3. **Consult the tutorials** - Conceptual background on methods
4. **Ask questions** - Use course discussion boards or office hours
5. **GitHub Issues** - Report bugs or request features on GitHub

::: {.callout-tip}
## Pro Tip: Version Management
When reporting issues or asking for help, always include:
- Package version (e.g., `docuscospacy==0.3.0`)
- Python version
- Error messages (if any)
- Minimal code example that reproduces the problem
:::
