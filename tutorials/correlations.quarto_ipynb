{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "# Correlation and Multi-Collinearity\n",
        "\n",
        "## Introduction\n",
        "\n",
        "**Correlation** measures the strength and direction of the linear relationship between two variables. In text analysis, correlation helps us understand which linguistic features vary together—a fundamental question for understanding language use.\n",
        "\n",
        "**Why correlation matters for humanities research:**\n",
        "\n",
        "- **Feature relationships**: Do passives increase when nominalizations increase?\n",
        "- **Multi-collinearity detection**: Which features are redundant (measuring the same thing)?\n",
        "- **Dimension reduction**: Can we combine correlated features into composite variables?\n",
        "- **Assumption checking**: Many statistical models assume features are independent—are they?\n",
        "- **Theoretical insights**: Correlation patterns reveal functional groupings (features that serve similar communicative purposes)\n",
        "\n",
        "This tutorial introduces correlation analysis with special attention to **multi-collinearity**: when predictor variables are highly correlated with each other. We'll explore:\n",
        "\n",
        "1. **When multi-collinearity is a problem** (regression, classification models assume independence)\n",
        "2. **When multi-collinearity is useful** (factor analysis, PCA leverage it for dimension reduction)\n",
        "3. **How to detect and interpret correlation patterns** using linguistic features from the `pybiber` package\n",
        "\n",
        "## Core Concepts\n",
        "\n",
        "### What is Correlation?\n",
        "\n",
        "**Correlation coefficient** (Pearson's *r*) ranges from -1 to +1:\n",
        "\n",
        "- **r = +1**: Perfect positive relationship (as X increases, Y increases proportionally)\n",
        "- **r = 0**: No linear relationship\n",
        "- **r = -1**: Perfect negative relationship (as X increases, Y decreases proportionally)\n",
        "\n",
        "**Example**:\n",
        "\n",
        "- Positive correlation: **Nominalizations** and **attributive adjectives** (both increase in informational writing)\n",
        "- Negative correlation: **Personal pronouns** and **word length** (pronouns are short; informational writing uses long words)\n",
        "- No correlation: **Past tense** and **type-token ratio** (independent dimensions of variation)\n",
        "\n",
        "### Interpreting Correlation Strength\n",
        "\n",
        "**Conventional benchmarks** (Cohen, 1988):\n",
        "\n",
        "- **|r| < 0.3**: Weak correlation\n",
        "- **0.3 ≤ |r| < 0.5**: Moderate correlation  \n",
        "- **|r| ≥ 0.5**: Strong correlation\n",
        "\n",
        "**Context matters**: In social sciences, r = 0.3 might be meaningful. In physics, r = 0.9 might be insufficient. For linguistic features, correlations around 0.4-0.6 are common and theoretically important.\n",
        "\n",
        "::: {.callout-warning}\n",
        "## Correlation ≠ Causation\n",
        "\n",
        "Correlation measures **co-occurrence**, not **causation**. If passives and nominalizations correlate, it doesn't mean one causes the other—both may be responses to a third factor (e.g., genre constraints requiring informational density).\n",
        ":::\n",
        "\n",
        "### Types of Correlation\n",
        "\n",
        "**Pearson correlation** (parametric):\n",
        "- Measures **linear** relationships\n",
        "- Assumes normally distributed variables\n",
        "- Most common, what we'll use\n",
        "\n",
        "**Spearman correlation** (non-parametric):\n",
        "- Measures **monotonic** relationships (consistently increasing or decreasing, but not necessarily linear)\n",
        "- Based on ranks, not raw values\n",
        "- Better for non-normal distributions or ordinal data\n",
        "\n",
        "**Kendall's tau** (non-parametric):\n",
        "- Also rank-based\n",
        "- More robust to outliers than Spearman\n",
        "- Better for small samples\n",
        "\n",
        "For most text analysis with count-based features (especially normalized frequencies), **Pearson correlation** is appropriate.\n",
        "\n",
        "## Multi-Collinearity Explained\n",
        "\n",
        "### What is Multi-Collinearity?\n",
        "\n",
        "**Multi-collinearity** occurs when predictor variables in a model are highly correlated with each other. This creates problems for some statistical procedures but enables others.\n",
        "\n",
        "**Example**: If we're predicting genre from linguistic features, and **nominalizations** (r = 0.85) and **prepositions** are highly correlated, they provide redundant information—both measure similar underlying dimension (informativeness).\n",
        "\n",
        "### When Multi-Collinearity is a Problem\n",
        "\n",
        "#### 1. Regression Models\n",
        "\n",
        "**Why it's problematic:**\n",
        "\n",
        "- **Unstable coefficients**: Small changes in data cause large changes in estimated effects\n",
        "- **Inflated standard errors**: Confidence intervals become unreliable\n",
        "- **Difficulty interpreting**: Can't tell which variable is \"really\" important\n",
        "- **Variance inflation**: Collinear variables \"compete\" to explain the same variance\n",
        "\n",
        "**Example**: If nominalizations and prepositions both predict academic writing (r = 0.85 with each other), their individual regression coefficients become unreliable. Adding or removing one changes the other's coefficient dramatically.\n",
        "\n",
        "**Diagnostic**: **Variance Inflation Factor (VIF)**:\n",
        "\n",
        "- VIF = 1 / (1 - R²) where R² is from regressing predictor X on all other predictors\n",
        "- VIF > 10 indicates severe multi-collinearity\n",
        "- VIF > 5 suggests caution\n",
        "\n",
        "**Solutions:**\n",
        "\n",
        "- Drop redundant variables (keep most theoretically important)\n",
        "- Combine correlated variables into composite scores\n",
        "- Use dimension reduction (PCA) to create orthogonal predictors\n",
        "- Use regularization (Ridge regression, Lasso) which penalizes large coefficients\n",
        "\n",
        "#### 2. Classification Models\n",
        "\n",
        "**Some algorithms struggle with multi-collinearity:**\n",
        "\n",
        "- **Logistic regression**: Same issues as linear regression (unstable coefficients, inflated SE)\n",
        "- **Linear Discriminant Analysis**: Assumes features are independent\n",
        "\n",
        "**Some algorithms handle it well:**\n",
        "\n",
        "- **Random Forest**: Tree-based methods are robust to multi-collinearity\n",
        "- **Naive Bayes**: Assumes independence but often works despite violations\n",
        "- **Neural networks**: Can learn despite multi-collinearity (but may be inefficient)\n",
        "\n",
        "**Practical impact**: Multi-collinearity often reduces interpretability more than predictive accuracy. A Random Forest might classify well despite correlated features, but you can't interpret feature importance reliably.\n",
        "\n",
        "### When Multi-Collinearity is Useful\n",
        "\n",
        "#### 1. Factor Analysis\n",
        "\n",
        "**Factor analysis** (used in Multi-Dimensional Analysis) **requires** correlations among variables:\n",
        "\n",
        "**Core principle**: If variables don't correlate, they can't reflect underlying latent factors.\n",
        "\n",
        "**Example**: \n",
        "\n",
        "- **Observed variables**: First-person pronouns, present tense, contractions (all correlated r ≈ 0.5-0.7)\n",
        "- **Latent factor**: \"Involved production\" (personal interaction)\n",
        "\n",
        "Factor analysis identifies groups of correlated variables and represents them as latent dimensions. Without correlation, there's nothing to reduce.\n",
        "\n",
        "**Requirements**:\n",
        "\n",
        "- **Sufficient correlations**: Need some |r| > 0.3 among variables\n",
        "- **Not perfect correlations**: Avoid |r| > 0.9 (redundancy, not shared factor)\n",
        "- **Bartlett's test**: Tests if correlation matrix differs from identity matrix (p < 0.05 indicates adequate correlations)\n",
        "- **KMO (Kaiser-Meyer-Olkin)**: Measures sampling adequacy (> 0.6 acceptable, > 0.8 good)\n",
        "\n",
        "#### 2. Principal Component Analysis (PCA)\n",
        "\n",
        "**PCA** creates new variables (components) that are **linear combinations** of correlated original variables:\n",
        "\n",
        "**Example**:\n",
        "\n",
        "- **Component 1**: 0.8×nominalizations + 0.7×prepositions + 0.6×attributive_adjectives\n",
        "- This component captures shared variance (informational density)\n",
        "\n",
        "**Why correlation helps**: PCA maximizes variance explained by components. If variables don't correlate, each component captures variance from just one variable (inefficient).\n",
        "\n",
        "**Difference from factor analysis**:\n",
        "\n",
        "- **PCA**: Reduces dimensions by explaining total variance (data reduction)\n",
        "- **Factor Analysis**: Identifies latent factors explaining correlations (theory discovery)\n",
        "\n",
        "See [MDA tutorial](#sec-mda) for how this works with linguistic features.\n",
        "\n",
        "## Correlation Analysis with Pybiber Features\n",
        "\n",
        "Let's analyze correlations among **Biber's 67 lexicogrammatical features** using the Brown Corpus. These features capture functional linguistic categories that often correlate because they serve related communicative purposes.\n",
        "\n",
        "### Load and Prepare Data"
      ],
      "id": "538e9032"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import polars as pl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pybiber as pb\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import pearsonr\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load Brown Corpus with genre labels\n",
        "brown_corpus = pl.read_parquet(\n",
        "    \"https://github.com/browndw/humanities_analytics/raw/refs/heads/main/data/data_tables/brown_corpus.parquet\"\n",
        ")\n",
        "\n",
        "# Select document IDs and texts\n",
        "bc = brown_corpus.select(\"doc_id\", \"text\")\n",
        "\n",
        "# Load spaCy model (disable NER for speed)\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n",
        "\n",
        "# Parse corpus with spaCy (this takes ~30 seconds with n_process=4)\n",
        "print(\"Parsing corpus with spaCy...\")\n",
        "df_spacy = pb.spacy_parse(corp=bc, nlp_model=nlp, n_process=4)\n",
        "\n",
        "# Aggregate into 67 Biber feature categories\n",
        "print(\"Extracting Biber features...\")\n",
        "dfm_biber = pb.biber(df_spacy)\n",
        "\n",
        "# Add genre labels from original corpus\n",
        "dfm_biber = dfm_biber.join(\n",
        "    brown_corpus.select(\"doc_id\", \"text_type\"), \n",
        "    on=\"doc_id\"\n",
        ")\n",
        "\n",
        "print(f\"Data shape: {dfm_biber.shape}\")\n",
        "print(f\"Number of features: {len([col for col in dfm_biber.columns if col.startswith('f_')])}\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(dfm_biber.head())"
      ],
      "id": "b6c91993",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Extract Feature Columns"
      ],
      "id": "0eedc34f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Get all Biber feature columns (f_01, f_02, etc.)\n",
        "feature_cols = [col for col in dfm_biber.columns if col.startswith('f_')]\n",
        "\n",
        "print(f\"Analyzing {len(feature_cols)} linguistic features\")\n",
        "print(f\"\\nSample features:\")\n",
        "for i, col in enumerate(feature_cols[:10], 1):\n",
        "    print(f\"  {i}. {col}\")"
      ],
      "id": "291c2c92",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Feature examples** (see [pybiber documentation](https://browndw.github.io/pybiber/feature-categories.html) for full list):\n",
        "\n",
        "- `f_01_past_tense`: Past tense verb forms\n",
        "- `f_02_perfect_aspect`: Perfect aspect (have/has + past participle)\n",
        "- `f_18_first_person_pronouns`: I, me, my, we, us, our\n",
        "- `f_19_second_person_pronouns`: you, your, yourself\n",
        "- `f_42_nominalizations`: -tion, -ment, -ness, -ity endings\n",
        "- `f_43_type_token`: Lexical diversity (MATTR)\n",
        "- `f_44_mean_word_length`: Average word length in characters\n",
        "\n",
        "### Compute Correlation Matrix"
      ],
      "id": "ff697d08"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Extract features as numpy array and compute correlations\n",
        "features_array = dfm_biber.select(feature_cols).to_numpy().astype(np.float64)\n",
        "\n",
        "# Compute pairwise Pearson correlations using numpy\n",
        "correlation_matrix = np.corrcoef(features_array, rowvar=False)\n",
        "\n",
        "# Convert to pandas DataFrame for easier manipulation and visualization\n",
        "correlation_matrix = pd.DataFrame(\n",
        "    correlation_matrix,\n",
        "    index=feature_cols,\n",
        "    columns=feature_cols\n",
        ")\n",
        "\n",
        "print(f\"Correlation matrix shape: {correlation_matrix.shape}\")\n",
        "print(f\"Total pairwise correlations: {correlation_matrix.shape[0] * (correlation_matrix.shape[0] - 1) // 2}\")"
      ],
      "id": "4d55a163",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize: Correlation Heatmap"
      ],
      "id": "5258bcff"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 14,
        "fig-height": 12
      },
      "source": [
        "#| fig-cap: Correlation matrix for Biber's 67 linguistic features. Red indicates positive correlation, blue indicates negative correlation. Patterns reveal functional groupings.\n",
        "#| label: fig-correlation-heatmap\n",
        "\n",
        "plt.figure(figsize=(14, 12))\n",
        "\n",
        "# Create heatmap\n",
        "sns.heatmap(\n",
        "    correlation_matrix,\n",
        "    cmap='RdBu_r',  # Red-blue diverging colormap\n",
        "    center=0,        # Center colormap at 0\n",
        "    vmin=-1,\n",
        "    vmax=1,\n",
        "    square=True,\n",
        "    linewidths=0.5,\n",
        "    cbar_kws={'label': 'Pearson r', 'shrink': 0.8}\n",
        ")\n",
        "\n",
        "plt.title('Correlation Matrix: Biber Features (Brown Corpus)', fontsize=16, pad=20)\n",
        "plt.xlabel('Linguistic Features', fontsize=12)\n",
        "plt.ylabel('Linguistic Features', fontsize=12)\n",
        "plt.xticks(rotation=90, fontsize=7)\n",
        "plt.yticks(rotation=0, fontsize=7)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-correlation-heatmap",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Reading the heatmap:**\n",
        "\n",
        "- **Dark red**: Strong positive correlation (features co-occur)\n",
        "- **Dark blue**: Strong negative correlation (features anti-correlate)\n",
        "- **White**: No correlation (independent variation)\n",
        "- **Clusters**: Blocks of similar colors indicate groups of related features\n",
        "\n",
        "### Identify Strongest Correlations"
      ],
      "id": "a22539d9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Extract upper triangle (avoid duplicates and diagonal)\n",
        "upper_tri = correlation_matrix.where(\n",
        "    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
        ")\n",
        "\n",
        "# Convert to long format for sorting\n",
        "correlations_long = []\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "        correlations_long.append({\n",
        "            'feature_1': correlation_matrix.columns[i],\n",
        "            'feature_2': correlation_matrix.columns[j],\n",
        "            'correlation': correlation_matrix.iloc[i, j]\n",
        "        })\n",
        "\n",
        "corr_df = pd.DataFrame(correlations_long)\n",
        "\n",
        "# Sort by absolute correlation\n",
        "corr_df['abs_corr'] = corr_df['correlation'].abs()\n",
        "corr_df_sorted = corr_df.sort_values('abs_corr', ascending=False)\n",
        "\n",
        "print(\"Top 20 strongest correlations (positive and negative):\\n\")\n",
        "print(corr_df_sorted.head(20).to_string(index=False))"
      ],
      "id": "ee46f3f1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation questions:**\n",
        "\n",
        "- Which features show strongest positive correlations? (Likely features from same functional category)\n",
        "- Which features show strongest negative correlations? (Likely features from opposing communicative modes)\n",
        "- Are there surprising pairs? (Features you wouldn't expect to correlate based on linguistic theory)\n",
        "\n",
        "### Examine Specific Correlation\n",
        "\n",
        "Let's examine one strong positive correlation in detail:"
      ],
      "id": "60901993"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example: Correlation between nominalizations and prepositions\n",
        "# (both typical of informational writing)\n",
        "\n",
        "feature_x = 'f_42_nominalizations'\n",
        "feature_y = 'f_35_total_prepositions'\n",
        "\n",
        "# Get correlation coefficient and p-value\n",
        "r, p = pearsonr(dfm_biber[feature_x].to_numpy(), dfm_biber[feature_y].to_numpy())\n",
        "\n",
        "print(f\"\\nCorrelation Analysis:\")\n",
        "print(f\"  Feature 1: {feature_x}\")\n",
        "print(f\"  Feature 2: {feature_y}\")\n",
        "print(f\"  Pearson r: {r:.3f}\")\n",
        "print(f\"  P-value: {p:.4f}\")\n",
        "print(f\"  Interpretation: {'Significant' if p < 0.05 else 'Not significant'}\")\n",
        "print(f\"  Strength: \", end=\"\")\n",
        "if abs(r) < 0.3:\n",
        "    print(\"Weak\")\n",
        "elif abs(r) < 0.5:\n",
        "    print(\"Moderate\")\n",
        "else:\n",
        "    print(\"Strong\")"
      ],
      "id": "7278a189",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize: Scatterplot"
      ],
      "id": "0b4b5860"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 8,
        "fig-height": 6
      },
      "source": [
        "#| fig-cap: Scatterplot showing relationship between two correlated features. Each point is a document; trend line shows direction and strength of correlation.\n",
        "#| label: fig-correlation-scatter\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "# Extract data as numpy arrays\n",
        "x_data = dfm_biber[feature_x].to_numpy()\n",
        "y_data = dfm_biber[feature_y].to_numpy()\n",
        "\n",
        "plt.scatter(\n",
        "    x_data,\n",
        "    y_data,\n",
        "    alpha=0.5,\n",
        "    s=30,\n",
        "    edgecolors='black',\n",
        "    linewidths=0.5\n",
        ")\n",
        "\n",
        "# Add trend line\n",
        "z = np.polyfit(x_data, y_data, 1)\n",
        "p = np.poly1d(z)\n",
        "x_line = np.linspace(x_data.min(), x_data.max(), 100)\n",
        "plt.plot(x_line, p(x_line), \"r--\", alpha=0.8, linewidth=2, label=f'Trend line (r={r:.3f})')\n",
        "\n",
        "plt.xlabel(feature_x.replace('_', ' ').title(), fontsize=12)\n",
        "plt.ylabel(feature_y.replace('_', ' ').title(), fontsize=12)\n",
        "plt.title(f'Correlation: {feature_x} vs {feature_y}', fontsize=14)\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "fig-correlation-scatter",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Pattern recognition:**\n",
        "\n",
        "- Tight clustering around trend line → strong correlation\n",
        "- Wide spread → weak correlation\n",
        "- Upward slope → positive correlation\n",
        "- Downward slope → negative correlation\n",
        "\n",
        "## Detecting Multi-Collinearity\n",
        "\n",
        "### Correlation Threshold\n",
        "\n",
        "**Common rule**: Features with |r| > 0.7 or 0.8 are considered highly collinear."
      ],
      "id": "62cf107e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Find pairs with |r| > 0.7\n",
        "high_collinearity = corr_df[corr_df['abs_corr'] > 0.7].sort_values('abs_corr', ascending=False)\n",
        "\n",
        "print(f\"\\nFeature pairs with |r| > 0.7 (high multi-collinearity):\")\n",
        "print(f\"  Count: {len(high_collinearity)} pairs\\n\")\n",
        "print(high_collinearity.head(15).to_string(index=False))"
      ],
      "id": "7c88d3c7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Decision**: If building a regression model, consider:\n",
        "\n",
        "- Dropping one feature from each pair\n",
        "- Combining them into composite score (average or sum)\n",
        "- Using dimension reduction (PCA, factor analysis)\n",
        "\n",
        "### Variance Inflation Factor (VIF)\n",
        "\n",
        "**VIF** quantifies how much a predictor's variance is inflated by multi-collinearity:"
      ],
      "id": "dd567e3f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Compute VIF for each feature\n",
        "vif_data = pd.DataFrame()\n",
        "vif_data['Feature'] = feature_cols\n",
        "vif_data['VIF'] = [\n",
        "    variance_inflation_factor(features_df.values, i) \n",
        "    for i in range(len(feature_cols))\n",
        "]\n",
        "\n",
        "vif_data = vif_data.sort_values('VIF', ascending=False)\n",
        "\n",
        "print(\"\\nTop 20 features by VIF (highest multi-collinearity):\\n\")\n",
        "print(vif_data.head(20).to_string(index=False))\n",
        "\n",
        "print(f\"\\nSummary:\")\n",
        "print(f\"  Features with VIF > 10 (severe): {(vif_data['VIF'] > 10).sum()}\")\n",
        "print(f\"  Features with VIF > 5 (moderate): {(vif_data['VIF'] > 5).sum()}\")\n",
        "print(f\"  Features with VIF < 5 (acceptable): {(vif_data['VIF'] < 5).sum()}\")"
      ],
      "id": "e42ad3d5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation:**\n",
        "\n",
        "- **VIF = 1**: No multi-collinearity (feature independent of all others)\n",
        "- **VIF = 5**: Feature's variance inflated 5× by correlations with other features\n",
        "- **VIF = 10**: Severe multi-collinearity (80% of variance explained by other features: 1 - 1/10 = 0.9)\n",
        "\n",
        "**High VIF is expected** for linguistic features because they naturally cluster by function. This is a *problem* for regression interpretation but an *opportunity* for dimension reduction.\n",
        "\n",
        "## Multi-Collinearity and Dimension Reduction\n",
        "\n",
        "### Why Correlation Enables Factor Analysis\n",
        "\n",
        "**Factor analysis** (used in MDA) leverages multi-collinearity to discover latent dimensions:\n",
        "\n",
        "**Logic**:\n",
        "\n",
        "1. **Identify correlated clusters**: Features that correlate form functional groups\n",
        "2. **Extract latent factors**: Each cluster reflects an underlying dimension\n",
        "3. **Interpret factors**: Give communicative/functional label based on loadings\n",
        "\n",
        "**Example** (from [MDA tutorial](#sec-mda)):\n",
        "\n",
        "**Dimension 1: Involved vs. Informational Production**\n",
        "\n",
        "- **Positive loadings** (involved): first-person pronouns, contractions, present tense (correlated with each other)\n",
        "- **Negative loadings** (informational): nominalizations, prepositions, attributive adjectives (correlated with each other)\n",
        "\n",
        "These two clusters are **negatively correlated** with each other (involved features decrease when informational features increase), forming opposite poles of one dimension.\n",
        "\n",
        "### Correlation Matrix for Factor Analysis"
      ],
      "id": "f2d54f77"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Assess suitability for factor analysis\n",
        "\n",
        "# 1. Count correlations above threshold\n",
        "sufficient_corrs = (correlation_matrix.abs() > 0.3).sum().sum() - len(feature_cols)  # Subtract diagonal\n",
        "total_corrs = len(feature_cols) * (len(feature_cols) - 1)\n",
        "\n",
        "print(\"Factor Analysis Suitability:\\n\")\n",
        "print(f\"  1. Sufficient correlations (|r| > 0.3): {sufficient_corrs}/{total_corrs} ({sufficient_corrs/total_corrs:.1%})\")\n",
        "\n",
        "# 2. Check for perfect multi-collinearity (bad even for FA)\n",
        "perfect_corrs = ((correlation_matrix.abs() > 0.95) & (correlation_matrix.abs() < 1.0)).sum().sum() // 2\n",
        "print(f\"  2. Perfect multi-collinearity (|r| > 0.95): {perfect_corrs} pairs (should be 0)\")\n",
        "\n",
        "# 3. Bartlett's test of sphericity\n",
        "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
        "\n",
        "chi_square, p_value = calculate_bartlett_sphericity(features_df)\n",
        "print(f\"  3. Bartlett's test: χ² = {chi_square:.1f}, p = {p_value:.4f}\")\n",
        "print(f\"     {'✓ Adequate correlations' if p_value < 0.05 else '✗ Insufficient correlations'}\")\n",
        "\n",
        "# 4. KMO measure of sampling adequacy\n",
        "from factor_analyzer.factor_analyzer import calculate_kmo\n",
        "\n",
        "kmo_all, kmo_model = calculate_kmo(features_df)\n",
        "print(f\"  4. KMO measure: {kmo_model:.3f}\")\n",
        "if kmo_model >= 0.9:\n",
        "    print(\"     ✓ Excellent sampling adequacy\")\n",
        "elif kmo_model >= 0.8:\n",
        "    print(\"     ✓ Good sampling adequacy\")\n",
        "elif kmo_model >= 0.6:\n",
        "    print(\"     ✓ Adequate sampling adequacy\")\n",
        "else:\n",
        "    print(\"     ✗ Poor sampling adequacy\")"
      ],
      "id": "1817fbc3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Conclusion**: If Bartlett's test is significant (p < 0.05) and KMO > 0.6, the correlation matrix is suitable for factor analysis. This confirms that multi-collinearity exists and can be leveraged for dimension reduction.\n",
        "\n",
        "### Visualize: Correlation Network"
      ],
      "id": "440e61ce"
    },
    {
      "cell_type": "code",
      "metadata": {
        "fig-width": 12,
        "fig-height": 10
      },
      "source": [
        "#| fig-cap: Network visualization of feature correlations. Nodes are features; edges connect correlated pairs (|r| > 0.4). Clusters reveal functional groupings.\n",
        "#| label: fig-correlation-network\n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "# Create network graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Add nodes (features)\n",
        "G.add_nodes_from(feature_cols)\n",
        "\n",
        "# Add edges for correlations above threshold\n",
        "threshold = 0.4\n",
        "for _, row in corr_df[corr_df['abs_corr'] > threshold].iterrows():\n",
        "    G.add_edge(row['feature_1'], row['feature_2'], weight=row['correlation'])\n",
        "\n",
        "# Layout\n",
        "pos = nx.spring_layout(G, k=0.5, iterations=50, seed=42)\n",
        "\n",
        "# Draw\n",
        "plt.figure(figsize=(12, 10))\n",
        "nx.draw_networkx_nodes(G, pos, node_size=100, node_color='lightblue', alpha=0.7)\n",
        "nx.draw_networkx_edges(G, pos, alpha=0.3, width=1)\n",
        "nx.draw_networkx_labels(G, pos, font_size=6)\n",
        "\n",
        "plt.title(f'Correlation Network (|r| > {threshold})', fontsize=16)\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nNetwork statistics:\")\n",
        "print(f\"  Nodes (features): {G.number_of_nodes()}\")\n",
        "print(f\"  Edges (correlations > {threshold}): {G.number_of_edges()}\")\n",
        "print(f\"  Density: {nx.density(G):.3f}\")"
      ],
      "id": "fig-correlation-network",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation:**\n",
        "\n",
        "- **Dense clusters**: Groups of tightly correlated features (potential factors)\n",
        "- **Bridge nodes**: Features that connect multiple clusters\n",
        "- **Isolated nodes**: Features with few strong correlations (may not load on major factors)\n",
        "\n",
        "## Practical Applications\n",
        "\n",
        "### 1. Feature Selection for Classification\n",
        "\n",
        "**Problem**: You're building a classifier to distinguish academic from journalistic writing. You have 67 Biber features, but many are highly correlated.\n",
        "\n",
        "**Solution**:"
      ],
      "id": "8bb3d79c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 1: Identify redundant features (|r| > 0.8)\n",
        "redundant_pairs = corr_df[corr_df['abs_corr'] > 0.8].copy()\n",
        "\n",
        "# Step 2: For each pair, keep the feature with higher variance (more information)\n",
        "features_to_drop = set()\n",
        "\n",
        "for _, row in redundant_pairs.iterrows():\n",
        "    feat1, feat2 = row['feature_1'], row['feature_2']\n",
        "    \n",
        "    # Compare variances\n",
        "    var1 = dfm_biber[feat1].to_numpy().var()\n",
        "    var2 = dfm_biber[feat2].to_numpy().var()\n",
        "    \n",
        "    # Drop lower-variance feature\n",
        "    to_drop = feat1 if var1 < var2 else feat2\n",
        "    features_to_drop.add(to_drop)\n",
        "\n",
        "print(f\"\\nFeature Selection for Classification:\")\n",
        "print(f\"  Original features: {len(feature_cols)}\")\n",
        "print(f\"  Features to drop (redundant): {len(features_to_drop)}\")\n",
        "print(f\"  Retained features: {len(feature_cols) - len(features_to_drop)}\")\n",
        "\n",
        "print(f\"\\nFeatures dropped:\")\n",
        "for feat in sorted(features_to_drop)[:10]:\n",
        "    print(f\"  - {feat}\")\n",
        "if len(features_to_drop) > 10:\n",
        "    print(f\"  ... and {len(features_to_drop) - 10} more\")"
      ],
      "id": "f9205bc5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Alternative**: Use **PCA** to create orthogonal components (no multi-collinearity by design).\n",
        "\n",
        "### 2. Understanding Functional Groupings\n",
        "\n",
        "**Problem**: Which features co-occur to serve similar communicative functions?\n",
        "\n",
        "**Solution**: Examine high-correlation clusters:"
      ],
      "id": "af6aa003"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example: Find all features strongly correlated with first-person pronouns\n",
        "target_feature = 'f_18_first_person_pronouns'\n",
        "\n",
        "# Get correlations with target\n",
        "target_corrs = correlation_matrix[target_feature].drop(target_feature)\n",
        "target_corrs_sorted = target_corrs.abs().sort_values(ascending=False)\n",
        "\n",
        "print(f\"\\nFeatures most correlated with {target_feature}:\\n\")\n",
        "for feat, corr in target_corrs_sorted.head(10).items():\n",
        "    direction = \"+\" if correlation_matrix.loc[feat, target_feature] > 0 else \"-\"\n",
        "    print(f\"  {direction} {feat}: r = {correlation_matrix.loc[feat, target_feature]:.3f}\")"
      ],
      "id": "a79bbfe8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation**: Features that correlate with first-person pronouns likely characterize **involved, personal discourse**. This clustering motivates the **Involved Production** dimension in MDA.\n",
        "\n",
        "### 3. Hypothesis Testing\n",
        "\n",
        "**Problem**: You hypothesize that nominalized writing (high nominalizations) uses more prepositions (due to complex noun phrases).\n",
        "\n",
        "**Solution**: Test correlation significance:"
      ],
      "id": "766c4c23"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test hypothesis\n",
        "feat_x = 'f_42_nominalizations'\n",
        "feat_y = 'f_35_total_prepositions'\n",
        "\n",
        "r, p = pearsonr(dfm_biber[feat_x].to_numpy(), dfm_biber[feat_y].to_numpy())\n",
        "\n",
        "print(f\"\\nHypothesis Test: Nominalizations ~ Prepositions\")\n",
        "print(f\"  H₀: No correlation (r = 0)\")\n",
        "print(f\"  H₁: Positive correlation (r > 0)\")\n",
        "print(f\"  Observed r: {r:.3f}\")\n",
        "print(f\"  P-value: {p:.6f}\")\n",
        "print(f\"  Result: {'Reject H₀' if p < 0.05 else 'Fail to reject H₀'}\")\n",
        "print(f\"  Conclusion: {'Significant positive correlation supports hypothesis' if (p < 0.05 and r > 0) else 'Hypothesis not supported'}\")"
      ],
      "id": "bc5ea94b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Comparing Correlation Across Corpora\n",
        "\n",
        "**Problem**: Do feature correlations differ across registers?\n",
        "\n",
        "**Solution**: Compute separate correlation matrices for genres:"
      ],
      "id": "12159c54"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Assume we have genre labels\n",
        "# dfm_biber = dfm_biber.with_columns(pl.lit(\"genre_column\").alias(\"text_type\"))\n",
        "\n",
        "# Example: Compare academic vs. fiction\n",
        "# academic_features = dfm_biber.filter(pl.col('text_type') == 'learned').select(feature_cols).to_pandas()\n",
        "# fiction_features = dfm_biber.filter(pl.col('text_type') == 'fiction').select(feature_cols).to_pandas()\n",
        "\n",
        "# corr_academic = academic_features.corr()\n",
        "# corr_fiction = fiction_features.corr()\n",
        "\n",
        "# # Compare specific correlation\n",
        "# feat_pair = ('f_18_first_person_pronouns', 'f_01_past_tense')\n",
        "# r_academic = corr_academic.loc[feat_pair[0], feat_pair[1]]\n",
        "# r_fiction = corr_fiction.loc[feat_pair[0], feat_pair[1]]\n",
        "\n",
        "# print(f\"Correlation: {feat_pair[0]} ~ {feat_pair[1]}\")\n",
        "# print(f\"  Academic: r = {r_academic:.3f}\")\n",
        "# print(f\"  Fiction: r = {r_fiction:.3f}\")\n",
        "# print(f\"  Difference: Δr = {abs(r_academic - r_fiction):.3f}\")\n",
        "\n",
        "print(\"\\n(Example code—requires genre-labeled data)\")"
      ],
      "id": "eb946b4c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Interpretation**: Different correlations across genres suggest that features serve different functions in different contexts.\n",
        "\n",
        "## Methodological Considerations\n",
        "\n",
        "### 1. Sample Size\n",
        "\n",
        "**Correlation stability** depends on sample size:\n",
        "\n",
        "- **n < 30**: Correlations unstable, prone to sampling error\n",
        "- **n = 100**: Moderate stability, r ≈ 0.25 detectable\n",
        "- **n = 500**: Good stability, r ≈ 0.12 detectable\n",
        "- **n > 1000**: Excellent stability, small correlations detectable\n",
        "\n",
        "**Brown Corpus** (n ≈ 500) provides stable estimates for moderate-to-strong correlations.\n",
        "\n",
        "**Significance vs. meaningfulness**: With large n, even tiny correlations (r = 0.05) can be statistically significant but substantively meaningless.\n",
        "\n",
        "### 2. Outliers\n",
        "\n",
        "**Outliers** can distort correlations:\n",
        "\n",
        "- One extreme document can artificially inflate or deflate r\n",
        "- Check scatterplots visually\n",
        "- Consider robust alternatives (Spearman correlation)\n",
        "\n",
        "**Solution**: Examine influential points, consider removing or transforming extreme values.\n",
        "\n",
        "### 3. Linearity Assumption\n",
        "\n",
        "**Pearson correlation** assumes linear relationships. If relationship is non-linear (curved), Pearson underestimates strength.\n",
        "\n",
        "**Check**: Examine scatterplots for curvature.\n",
        "\n",
        "**Solution**: Use Spearman (handles monotonic non-linear relationships) or transform variables (log, square root).\n",
        "\n",
        "### 4. Range Restriction\n",
        "\n",
        "**Problem**: If you only analyze academic writing (restricted range on informational features), correlations may be weaker than in full population.\n",
        "\n",
        "**Example**: In a corpus of only academic papers, nominalizations and prepositions may show weak correlation (both are consistently high). In a mixed corpus (academic + fiction), correlation is stronger.\n",
        "\n",
        "**Solution**: Ensure sufficient variation in both variables for meaningful correlation estimates.\n",
        "\n",
        "### 5. Shared Method Variance\n",
        "\n",
        "**Problem**: Features computed from same source (same linguistic annotation) may correlate due to measurement error, not true relationship.\n",
        "\n",
        "**Example**: If POS tagger makes systematic errors, features based on POS tags may show spurious correlations.\n",
        "\n",
        "**Solution**: Validate key findings with independent annotation or alternative feature extraction methods.\n",
        "\n",
        "## Connections to Other Methods\n",
        "\n",
        "### Correlation and Keyness\n",
        "\n",
        "**Keyness** identifies distinctive features between corpora. **Correlation** reveals which distinctive features co-occur.\n",
        "\n",
        "**Combined approach**:\n",
        "\n",
        "1. Use keyness to identify features distinguishing Genre A from Genre B\n",
        "2. Compute correlations among key features\n",
        "3. Discover functional groupings of distinctive features\n",
        "\n",
        "**Example**: Academic writing is key for nominalizations, passives, and attributive adjectives. Correlation analysis reveals these all correlate (r ≈ 0.5-0.7), forming an \"informational production\" dimension.\n",
        "\n",
        "### Correlation and Classification\n",
        "\n",
        "**Feature engineering**:\n",
        "\n",
        "1. Compute correlations among features\n",
        "2. Create composite features (sum/average of correlated features)\n",
        "3. Use composites as classifier inputs (reduces dimensionality, improves interpretability)\n",
        "\n",
        "**Example**: Instead of 10 correlated pronoun features, create one \"pronoun density\" composite.\n",
        "\n",
        "### Correlation and Embeddings\n",
        "\n",
        "**Contextual embeddings** (from transformers) capture semantic relationships implicitly. **Correlation** makes them explicit.\n",
        "\n",
        "**Workflow**:\n",
        "\n",
        "1. Extract embeddings for documents\n",
        "2. Compute pairwise document similarities (cosine)\n",
        "3. Check if similarity correlates with metadata (genre, time period)\n",
        "\n",
        "**Example**: Do documents with high embedding similarity also have similar first-person pronoun rates? If yes, embeddings capture \"personal involvement\" dimension.\n",
        "\n",
        "### Correlation and Time Series\n",
        "\n",
        "**Diachronic analysis**: Do feature correlations change over time?\n",
        "\n",
        "**Example**: In 19th-century texts, passives and nominalizations may correlate weakly (independent choices). In modern academic writing, they correlate strongly (package deal for informational style).\n",
        "\n",
        "**Interpretation**: Evolving correlations reveal changing functional relationships in language use.\n",
        "\n",
        "## Ethical Considerations\n",
        "\n",
        "### 1. Correlation Fishing\n",
        "\n",
        "**Risk**: Testing hundreds of correlations and reporting only significant ones (\"p-hacking\").\n",
        "\n",
        "**Problem**: With 67 features, there are 2,211 pairwise correlations. By chance, ~110 will be \"significant\" at p < 0.05 even if no true relationships exist.\n",
        "\n",
        "**Best practice**:\n",
        "\n",
        "- **Pre-register hypotheses**: Specify predicted correlations before analysis\n",
        "- **Bonferroni correction**: Adjust α = 0.05 / 2,211 ≈ 0.000023 for multiple comparisons\n",
        "- **Report all tests**: Show full correlation matrix, not cherry-picked results\n",
        "\n",
        "### 2. Spurious Correlations\n",
        "\n",
        "**Risk**: Interpreting meaningful relationships from coincidental patterns.\n",
        "\n",
        "**Example**: Number of Nicholas Cage films correlates with swimming pool drownings (r = 0.67)—clearly spurious.\n",
        "\n",
        "**In text analysis**: Two features may correlate in your corpus by chance (both happen to be common in Genre X) without reflecting functional relationship.\n",
        "\n",
        "**Best practice**:\n",
        "\n",
        "- **Theory first**: Only interpret correlations with plausible linguistic motivation\n",
        "- **Replicate**: Check if correlation holds in independent corpus\n",
        "- **Causal skepticism**: Remember correlation ≠ causation\n",
        "\n",
        "### 3. Ecological Fallacy\n",
        "\n",
        "**Risk**: Assuming document-level correlations apply to within-document variation.\n",
        "\n",
        "**Example**: Across documents, nominalizations and prepositions correlate (academic texts have more of both). Within a single document, they might not correlate (varied sentence structures).\n",
        "\n",
        "**Best practice**: Specify level of analysis (document, paragraph, sentence) and don't generalize across levels.\n",
        "\n",
        "### 4. Reification of Dimensions\n",
        "\n",
        "**Risk**: Treating statistically-derived dimensions (factors, PCs) as real psychological or linguistic entities.\n",
        "\n",
        "**Example**: \"Dimension 1\" from factor analysis is a mathematical construct explaining variance. Calling it \"Involved Production\" is an *interpretation*, not a discovery of natural category.\n",
        "\n",
        "**Best practice**:\n",
        "\n",
        "- Present dimensions as analytical tools, not ontological claims\n",
        "- Acknowledge alternative interpretations\n",
        "- Validate dimensions against external criteria (genre labels, expert judgments)\n",
        "\n",
        "## Summary\n",
        "\n",
        "**Correlation analysis** reveals relationships among variables:\n",
        "\n",
        "**Key concepts**:\n",
        "\n",
        "- **Pearson r**: Measures linear relationship strength (-1 to +1)\n",
        "- **Significance**: p-value tests if r differs from 0\n",
        "- **Strength**: |r| > 0.5 strong, 0.3-0.5 moderate, < 0.3 weak\n",
        "\n",
        "**Multi-collinearity**:\n",
        "\n",
        "- **Problem for regression/classification**: Unstable coefficients, inflated variance (VIF > 5-10)\n",
        "- **Opportunity for dimension reduction**: Factor analysis and PCA leverage correlations\n",
        "- **Detection**: Correlation matrix (|r| > 0.7), VIF (> 5)\n",
        "\n",
        "**Applications**:\n",
        "\n",
        "- **Feature selection**: Drop redundant features for classification\n",
        "- **Dimension reduction**: Identify correlated clusters for factor analysis (MDA)\n",
        "- **Theory development**: Discover functional groupings of linguistic features\n",
        "- **Hypothesis testing**: Test predicted relationships (e.g., nominalizations ~ prepositions)\n",
        "\n",
        "**Best practices**:\n",
        "\n",
        "1. **Visualize first**: Heatmaps, scatterplots reveal patterns\n",
        "2. **Consider context**: Strong correlation in one corpus may be weak in another\n",
        "3. **Check assumptions**: Linearity, outliers, sample size\n",
        "4. **Report transparently**: Full correlation matrix, not just significant pairs\n",
        "5. **Interpret cautiously**: Correlation ≠ causation; validate theoretically\n",
        "\n",
        "**Connections to MDA**:\n",
        "\n",
        "- Multi-collinearity among linguistic features is expected (functional groupings)\n",
        "- Factor analysis identifies latent dimensions from correlation patterns\n",
        "- See [MDA tutorial](#sec-mda) for how `pybiber` leverages correlation for dimension reduction\n",
        "\n",
        "**Next steps**: Apply correlation analysis to your data. Use it to inform feature selection (drop redundant features) or dimension reduction (identify correlated clusters for factor analysis). Combine with other methods (keyness, classification, MDA) for richer analysis.\n",
        "\n",
        "## Further Reading\n",
        "\n",
        "**Foundational:**\n",
        "\n",
        "- Cohen, J. (1988). *Statistical Power Analysis for the Behavioral Sciences* (2nd ed.). Routledge. (Correlation effect sizes)\n",
        "- Tabachnick, B. G., & Fidell, L. S. (2013). *Using Multivariate Statistics* (6th ed.). Pearson. (Chapter 4: Cleaning up your act—screening data, including multi-collinearity)\n",
        "\n",
        "**Multi-collinearity:**\n",
        "\n",
        "- Dormann, C. F., et al. (2013). Collinearity: A review of methods to deal with it and a simulation study evaluating their performance. *Ecography*, 36(1), 27-46. [DOI](https://doi.org/10.1111/j.1600-0587.2012.07348.x)\n",
        "- O'Brien, R. M. (2007). A caution regarding rules of thumb for variance inflation factors. *Quality & Quantity*, 41(5), 673-690. [DOI](https://doi.org/10.1007/s11135-006-9018-6)\n",
        "\n",
        "**Linguistic applications:**\n",
        "\n",
        "- Biber, D. (1988). *Variation Across Speech and Writing*. Cambridge University Press. (Chapters 3-4: Factor analysis of correlated linguistic features)\n",
        "- Gries, S. T. (2013). *Statistics for Linguistics with R* (2nd ed.). De Gruyter Mouton. (Chapter 5: Correlations)\n",
        "\n",
        "**Dimension reduction:**\n",
        "\n",
        "- Baayen, R. H. (2008). *Analyzing Linguistic Data: A Practical Introduction to Statistics Using R*. Cambridge University Press. (Chapter 6: PCA and factor analysis)\n",
        "- Jolliffe, I. T., & Cadima, J. (2016). Principal component analysis: A review and recent developments. *Philosophical Transactions of the Royal Society A*, 374(2065). [DOI](https://doi.org/10.1098/rsta.2015.0202)\n",
        "\n",
        "**Practical guides:**\n",
        "\n",
        "- Scikit-learn: [Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html)\n",
        "- Statsmodels: [Variance Inflation Factor](https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html)\n",
        "- Seaborn: [Visualizing Correlation Matrices](https://seaborn.pydata.org/examples/many_pairwise_correlations.html)"
      ],
      "id": "3b0a88ad"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/davidwestbrown/miniconda3/envs/moodswing_env/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}