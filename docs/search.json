[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Methods in Humanities Analytics",
    "section": "",
    "text": "Welcome\nWelcome to Methods in Humanities Analytics, a course designed for upper-level undergraduate and graduate students interested in computational approaches to analyzing texts. This course emphasizes computational reasoning and flexible, adaptable research tools over rote programming or mathematical formulas.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#course-philosophy",
    "href": "index.html#course-philosophy",
    "title": "Methods in Humanities Analytics",
    "section": "Course Philosophy",
    "text": "Course Philosophy\n\nPython as a Research Tool\nThis course uses Python in Google Colab as the primary computing environment. This approach serves two audiences:\nFor students with coding experience:\n\nBuild on your introductory Python knowledge\nLearn specialized packages for text analysis\nAdapt code examples for your own research projects\nDevelop custom analytical workflows\n\nFor students without coding experience:\n\nSimply run each code cell to generate results\nFocus on interpreting output and understanding concepts\nExport tables and visualizations to Google Sheets or Drive\n\nUse computational tools without being limited by pre-built interfaces\n\n\n\n\n\n\n\nThe Goal\n\n\n\nThe goal is not to make you expert programmers. The goal is to give you access to flexible analytical tools that you can use to explore questions you find compelling, without being constrained by someone elseâ€™s user interface.\n\n\n\n\nComputational Reasoning &gt; Code Memorization\nYou donâ€™t need to understand every line of code or every mathematical formula. What matters is:\n\nUnderstanding what methods do (conceptually)\nKnowing when to use them (methodologically)\nInterpreting results critically (analytically)\nMaking informed choices about approaches",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#how-this-site-works",
    "href": "index.html#how-this-site-works",
    "title": "Methods in Humanities Analytics",
    "section": "How This Site Works",
    "text": "How This Site Works\nThis documentation is organized into distinct sections, each serving a different purpose:\n\nğŸ“š Tutorials (Read & Learn)\nComprehensive, conceptual explanations of text analysis methods. These are readable documentation with code examples you can reference, but not interactive notebooks. They cover:\n\nFoundations: Basic concepts like sentiment analysis, corpus processing, and pipelines\nCore Methods: Frequency analysis, keyness, collocations, time series\nAdvanced Analysis: Categorical variables, clustering, correlations, multidimensional analysis, vector models\nApplications: Real-world case studies like the Federalist Papers classification\n\n\n\nğŸ”¬ Hands-On Labs (Do & Experiment)\nInteractive Jupyter notebooks designed to run in Google Colab. Each lab:\n\nOpens directly in Colab with one click\nIncludes all necessary setup and data loading\nGuides you through practical analytical tasks\nCan be adapted for your own projects\n\nSee the Hands-On Labs section for the complete list with â€œOpen in Colabâ€ buttons.\n\n\nğŸ“¦ Course Resources\nDocumentation for the specialized Python packages used throughout the course, plus guides for accessing course datasets. No need to read this upfrontâ€”refer to it as needed.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Methods in Humanities Analytics",
    "section": "Getting Started",
    "text": "Getting Started\n\nWhat You Need\nRequired:\n\nA Google account (for Colab access)\nAn internet connection\nA curious mind and interesting research questions\n\nNot Required:\n\nPython installation on your computer\nPrior programming experience\nAdvanced mathematics background\nExpensive software or computing resources\n\n\n\nYour First Steps\n\nBrowse the Tutorials to understand conceptual foundations\nOpen Lab 1 in Colab to see how the environment works\nRun all cells (Runtime â†’ Run all) to see the full analysis\nExplore the output and try modifying a parameter\nSave a copy to your Google Drive for future reference\n\n\n\n\n\n\n\nLearning by Doing\n\n\n\nDonâ€™t try to understand everything at once. Run the code, look at the results, and gradually build understanding through repeated exposure. Computational literacy develops over time.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#course-structure",
    "href": "index.html#course-structure",
    "title": "Methods in Humanities Analytics",
    "section": "Course Structure",
    "text": "Course Structure\n\nProgression Through Topics\nThe course follows a carefully designed progression:\n\nStart with the familiar (sentiment in narratives)\nBuild fundamental skills (tokenization, frequencies)\nAdd statistical methods (keyness, distributions)\nIntroduce advanced techniques (clustering, dimensions, vectors)\nApply to real problems (classification, genre analysis)\n\n\n\nTutorial + Lab Pairs\nMost topics have both:\n\nA tutorial explaining concepts and theory\nA mini lab providing hands-on practice\n\nRead the tutorial to understand why and how, then do the lab to see it in action.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#python-packages-for-this-course",
    "href": "index.html#python-packages-for-this-course",
    "title": "Methods in Humanities Analytics",
    "section": "Python Packages for This Course",
    "text": "Python Packages for This Course\nWe use specialized packages designed for humanities text analysis:\n\nPrimary Packages\n\ndocuscospacy - Rhetorical and functional tagging using DocuScope\npybiber - Multi-dimensional analysis of text registers\nmoodswing - Sentiment trajectory analysis for narratives\ngoogle_ngrams - Diachronic analysis with Google Books data\n\n\n\nSupporting Libraries\n\nspaCy - NLP processing (POS tagging, parsing, NER)\nPolars - Fast data manipulation (alternative to pandas)\nmatplotlib - Visualization and plotting\ngreat_tables - Publication-ready table formatting\n\nSee Python Packages for detailed documentation.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#about-the-data",
    "href": "index.html#about-the-data",
    "title": "Methods in Humanities Analytics",
    "section": "About the Data",
    "text": "About the Data\nAll course data is hosted in this repository and accessible via direct URLsâ€”no downloads required. Datasets include:\n\nSample corpora (MICUSP, Brown, inaugural addresses)\nClassic literary texts (Pride and Prejudice, Madame Bovary, etc.)\nPre-processed word lists and frequency tables\nMetadata for register and genre analysis\n\nSee Course Data Resources for the complete catalog.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#computational-ethics-critical-thinking",
    "href": "index.html#computational-ethics-critical-thinking",
    "title": "Methods in Humanities Analytics",
    "section": "Computational Ethics & Critical Thinking",
    "text": "Computational Ethics & Critical Thinking\n\n\n\n\n\n\nMethods are Tools, Not Truth\n\n\n\nEvery computational method makes assumptions, has limitations, and can be misused. Throughout this course, we emphasize:\n\nCritical interpretation - Question your results\nMethod transparency - Document your choices\nReproducibility - Share your code and data\nEthical awareness - Consider impacts and biases\n\n\n\nComputational text analysis is powerful, but itâ€™s not magic. It augments human interpretation; it doesnâ€™t replace it.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#getting-help",
    "href": "index.html#getting-help",
    "title": "Methods in Humanities Analytics",
    "section": "Getting Help",
    "text": "Getting Help\nWhen youâ€™re stuck or confused:\n\nRead the error message - Often tells you exactly whatâ€™s wrong\nCheck the tutorials - Conceptual background and examples\nReview the relevant lab - See working code in action\nConsult package docs - Detailed API references\nAsk questions - Use course discussion boards or office hours\nSearch online - Stack Overflow, documentation, forums\n\n\n\n\n\n\n\nA Note on AI Assistants\n\n\n\nTools like ChatGPT can help explain concepts or debug code, but:\n\nThey sometimes provide incorrect information\nThey canâ€™t replace understanding of methods\nCopy-pasting without comprehension is a missed learning opportunity\nUnderstanding why code works matters for adapting it to your needs",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Methods in Humanities Analytics",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis course and these materials build on decades of research in:\n\nCorpus linguistics (Biber, Conrad, Reppen, Gries, and many others)\nDigital humanities (Jockers, Moretti, Underwood, Rhody, and many others)\n\nComputational linguistics (Jurafsky, Manning, Bird, and many others)\nRhetoric and composition (Kaufer, Ishizaki, Geisler, and many others)\n\nThe Python packages were developed with support from Carnegie Mellon University and the broader open-source community.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#lets-begin",
    "href": "index.html#lets-begin",
    "title": "Methods in Humanities Analytics",
    "section": "Letâ€™s Begin",
    "text": "Letâ€™s Begin\nReady to start? Head to the Foundations section to begin with sentiment analysis, or jump directly to Hands-On Labs to start experimenting in Colab.\nRemember: You donâ€™t need to be a programmer to do computational text analysis. You just need curiosity, patience, and a willingness to experiment.\n\nCourse: Methods in Humanities Analytics (76-380/780)\nInstitution: Carnegie Mellon University\nInstructor: David Brown\nRepository: github.com/browndw/humanities_analytics",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "tutorials/sentiment-and-syuzhet.html",
    "href": "tutorials/sentiment-and-syuzhet.html",
    "title": "1Â  Texts, Algorithms, and Black-Boxes",
    "section": "",
    "text": "1.1 The Syuzhet Controversy\nIn 2015, Matthew Jockers published work on extracting â€œplot shapesâ€ from novels using sentiment analysis. The approach went viral, appearing in places like The Atlantic and The New York Times. But it also sparked intense debate about whether the methods were sound.\nThe controversy centered on whether the Discrete Cosine Transform (DCT)â€”a mathematical technique Jockers borrowed from signal processingâ€”was appropriate for this task, or whether it was imposing patterns on the data that werenâ€™t really there.\nKey readings:\nThis is a case where the developer didnâ€™t fully understand the statistics he was applying when he created the package. Thatâ€™s not necessarily disqualifyingâ€”but it means we need to be critical consumers of computational methods.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Texts, Algorithms, and Black-Boxes</span>"
    ]
  },
  {
    "objectID": "tutorials/sentiment-and-syuzhet.html#the-syuzhet-controversy",
    "href": "tutorials/sentiment-and-syuzhet.html#the-syuzhet-controversy",
    "title": "1Â  Texts, Algorithms, and Black-Boxes",
    "section": "",
    "text": "Annie Swaffordâ€™s critique: Why Syuzhet Doesnâ€™t Work\nJockersâ€™ response and updates: Matthew Jockersâ€™ blog (archived)\n\n\n\n\n\n\n\n\nWhy Start Here?\n\n\n\nSentiment analysis is intuitiveâ€”everyone understands the idea of emotional tone in a text. That makes it a good entry point for thinking critically about how we convert human experiences (reading, interpreting) into computational processes (scoring, transforming, visualizing).",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Texts, Algorithms, and Black-Boxes</span>"
    ]
  },
  {
    "objectID": "tutorials/sentiment-and-syuzhet.html#load-packages-and-data",
    "href": "tutorials/sentiment-and-syuzhet.html#load-packages-and-data",
    "title": "1Â  Texts, Algorithms, and Black-Boxes",
    "section": "1.2 Load Packages and Data",
    "text": "1.2 Load Packages and Data\nWeâ€™ll use moodswing, the Python implementation inspired by syuzhet, along with some standard data science libraries.\n\n# Sentiment analysis and narrative trajectories\nfrom moodswing import (\n    DictionarySentimentAnalyzer,\n    Sentencizer,\n    DCTTransform,\n    prepare_trajectory,\n    plot_trajectory\n)\nfrom moodswing.data import load_sample_text\n\n# Data manipulation and visualization  \nimport polars as pl\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nThe novels that Jockers used as examples are included in the moodswing package. There are 4 novels available as sample data:\n\n# Available novels\nnovels = [\"madame_bovary\", \"portrait_of_a_lady\", \"dubliners\", \"jude_the_obscure\"]\n\n# For this demonstration, we'll use Madame Bovary\ndoc_id, text = load_sample_text(\"madame_bovary\")\nprint(f\"Loaded: {doc_id}\")\nprint(f\"Length: {len(text):,} characters\")\n\nLoaded: madame_bovary\nLength: 648,257 characters",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Texts, Algorithms, and Black-Boxes</span>"
    ]
  },
  {
    "objectID": "tutorials/sentiment-and-syuzhet.html#prep-the-data-and-calculate-sentiment",
    "href": "tutorials/sentiment-and-syuzhet.html#prep-the-data-and-calculate-sentiment",
    "title": "1Â  Texts, Algorithms, and Black-Boxes",
    "section": "1.3 Prep the Data and Calculate Sentiment",
    "text": "1.3 Prep the Data and Calculate Sentiment\nNext, weâ€™ll split the novel into sentences and calculate a sentiment score for each sentence using the Syuzhet lexicon (the same dictionary used in the original R package).\n\n# Split text into sentences\nsentencizer = Sentencizer()\nsentences = sentencizer.split(text)\n\nprint(f\"Number of sentences: {len(sentences)}\")\n\nNumber of sentences: 6943\n\n\nNow weâ€™ll score each sentence using the Syuzhet lexicon:\n\n# Create sentiment analyzer\nanalyzer = DictionarySentimentAnalyzer()\n\n# Calculate sentiment scores for each sentence\n# method=\"syuzhet\" uses the same dictionary as the original R package\nscores = analyzer.sentence_scores(sentences, method=\"syuzhet\")\n\nprint(f\"First 10 scores: {scores[:10]}\")\n\nFirst 10 scores: [1.2000000000000002, 0.25, 0.0, 1.5, 1.05, 1.2000000000000002, 1.0, -0.25, 0.0, 0.4]\n\n\nLetâ€™s examine these scores in a DataFrame for easier inspection:\n\n# Create a dataframe to view scores\nimport polars as pl\n\ndf_scores = pl.DataFrame({\n    \"sentence_num\": range(1, len(scores) + 1),\n    \"sentiment\": scores,\n    \"sentence\": sentences\n})\n\n# View first 10 rows\nprint(df_scores.head(10))\n\nshape: (10, 3)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ sentence_num â”† sentiment â”† sentence                        â”‚\nâ”‚ ---          â”† ---       â”† ---                             â”‚\nâ”‚ i64          â”† f64       â”† str                             â”‚\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ 1            â”† 1.2       â”† Part I Chapter One We were in â€¦ â”‚\nâ”‚ 2            â”† 0.25      â”† Those who had been asleep wokeâ€¦ â”‚\nâ”‚ 3            â”† 0.0       â”† The head-master made a sign toâ€¦ â”‚\nâ”‚ 4            â”† 1.5       â”† Then, turning to the class-masâ€¦ â”‚\nâ”‚ 5            â”† 1.05      â”† If his work and conduct are saâ€¦ â”‚\nâ”‚ 6            â”† 1.2       â”† The \"new fellow,\" standing in â€¦ â”‚\nâ”‚ 7            â”† 1.0       â”† His hair was cut square on hisâ€¦ â”‚\nâ”‚ 8            â”† -0.25     â”† Although he was not broad-shouâ€¦ â”‚\nâ”‚ 9            â”† 0.0       â”† His legs, in blue stockings, lâ€¦ â”‚\nâ”‚ 10           â”† 0.4       â”† We began repeating the lesson.  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n\n\n\n\nUnderstanding Sentiment Scores\n\n\n\n\nPositive scores indicate positive emotional valence (happy, good, beautiful)\nNegative scores indicate negative emotional valence (sad, bad, ugly)\nZero scores mean either neutral or no sentiment words detected\nScores are based on a dictionary lookupâ€”each word has a pre-assigned value",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Texts, Algorithms, and Black-Boxes</span>"
    ]
  },
  {
    "objectID": "tutorials/sentiment-and-syuzhet.html#transforming-the-data-with-dct",
    "href": "tutorials/sentiment-and-syuzhet.html#transforming-the-data-with-dct",
    "title": "1Â  Texts, Algorithms, and Black-Boxes",
    "section": "1.4 Transforming the Data with DCT",
    "text": "1.4 Transforming the Data with DCT\nThe next step is to transform the data using the Discrete Cosine Transform (DCT). Originally, Jockers used a Fourier transformation, which he described as follows:\n\nAaron introduced me to a mathematical formula from signal processing called the Fourier transformation. The Fourier transformation provides a way of decomposing a time based signal and reconstituting it in the frequency domain. A complex signal (such as the one seen above in the first figure in this post) can be decomposed into series of symmetrical waves of varying frequencies. And one of the magical things about the Fourier equation is that these decomposed component sine waves can be added back together (summed) in order to reproduce the original wave formâ€“this is called a backward or reverse transformation. Fourier provides a way of transforming the sentiment-based plot trajectories into an equivalent data form that is independent of the length of the trajectory from beginning to end. The frequency domain begins to solve the book length problem.\n\nThis introduced some unwanted outcomes, namely that the resulting wave-forms must begin and end at the same point. The updated approach uses a Discrete Cosine Transform (DCT), which is commonly used in data compression (like JPEG images and MP3 audio).\n\n1.4.1 What Does DCT Do?\nThink of DCT as a smoothing filter that:\n\nRemoves high-frequency â€œnoiseâ€ (sentence-to-sentence variation)\nPreserves low-frequency patterns (overall narrative trends)\nCan normalize texts of different lengths to the same scale\n\nBut hereâ€™s the critical question: Is this smoothing revealing patterns that were already there, or creating patterns that werenâ€™t?\n\n# Apply DCT transformation\n# low_pass_size: how much smoothing (lower = more smoothing)\n# output_length: standardize to this many points\n# scale_range: normalize to [-1, 1]\n\ntrajectory = prepare_trajectory(\n    scores,\n    rolling_window=None,  # Optional: can add moving average first\n    dct_transform=DCTTransform(\n        low_pass_size=5,\n        output_length=100,\n        scale_range=True\n    )\n)\n\nprint(f\"Original scores: {len(scores)}\")\nprint(f\"Transformed trajectory: {len(trajectory.dct)}\")\nprint(f\"First 10 transformed values: {trajectory.dct[:10]}\")\n\nOriginal scores: 6943\nTransformed trajectory: 100\nFirst 10 transformed values: [1.         0.99718023 0.99156374 0.9831964  0.9721465  0.95850405\n 0.94237997 0.92390505 0.90322867 0.88051744]\n\n\n/tmp/ipykernel_2581/1286228209.py:6: UserWarning: DCT transform already has scaling enabled (scale_range=True, scale_values=False). Skipping additional normalization of DCT output to prevent double-scaling. Raw and rolling components are still normalized.\n  trajectory = prepare_trajectory(\n\n\nNotice how we went from thousands of individual sentence scores to 100 smoothed trajectory points.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Texts, Algorithms, and Black-Boxes</span>"
    ]
  },
  {
    "objectID": "tutorials/sentiment-and-syuzhet.html#visualizing-the-emotional-arc",
    "href": "tutorials/sentiment-and-syuzhet.html#visualizing-the-emotional-arc",
    "title": "1Â  Texts, Algorithms, and Black-Boxes",
    "section": "1.5 Visualizing the Emotional Arc",
    "text": "1.5 Visualizing the Emotional Arc\nNow we can plot the transformed sentiment trajectory:\n\nplot_trajectory(\n    trajectory,\n    title=\"Madame Bovary: Emotional Arc\",\n    components=[\"dct\"],  # Only show DCT-transformed trajectory\n    colors={\"dct\": \"tomato\"}\n)\nplt.xlabel(\"Normalized Narrative Time\")\nplt.ylabel(\"Emotional Valence\")\nplt.show()\n\n\n\n\nSentiment trajectory in Madame Bovary using DCT transformation.\n\n\n\n\nThis produces the characteristic â€œplot shapeâ€ that Jockers popularizedâ€”a smooth curve showing the emotional journey of the narrative.\nQuestion to ponder: Does this curve tell us something meaningful about Madame Bovary? Or is it just a pretty picture?",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Texts, Algorithms, and Black-Boxes</span>"
    ]
  },
  {
    "objectID": "tutorials/sentiment-and-syuzhet.html#transformed-vs.-non-transformed-data",
    "href": "tutorials/sentiment-and-syuzhet.html#transformed-vs.-non-transformed-data",
    "title": "1Â  Texts, Algorithms, and Black-Boxes",
    "section": "1.6 Transformed vs.Â Non-Transformed Data",
    "text": "1.6 Transformed vs.Â Non-Transformed Data\nTo better understand what the DCT is doing, letâ€™s compare the raw sentence-level scores with the smoothed trajectory.\nFirst, weâ€™ll create a normalized version of the raw scores:\n\n# Normalize raw scores to same scale as DCT output\nraw_sentiment = np.array(scores)\nraw_normalized = 2 * (raw_sentiment - raw_sentiment.min()) / (raw_sentiment.max() - raw_sentiment.min()) - 1\n\n# Create normalized time values\nraw_time = np.arange(len(raw_sentiment)) / len(raw_sentiment) * 100\n\n# DCT time values\ndct_time = np.arange(len(trajectory.dct)) / len(trajectory.dct) * 100\n\nNow plot both together:\n\nplt.figure(figsize=(10, 5))\n\n# Plot raw scores as scatter\nplt.scatter(raw_time, raw_normalized, alpha=0.2, s=1, color='gray', label='Raw scores')\n\n# Plot DCT trajectory as line\nplt.plot(dct_time, trajectory.dct, color='tomato', linewidth=2, label='DCT smoothed')\n\nplt.xlabel('Normalized Narrative Time')\nplt.ylabel('Scaled Sentiment')\nplt.title('Madame Bovary: Raw vs. Transformed Sentiment')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\nRaw sentiment scores (gray points) vs.Â DCT-smoothed trajectory (red line) in Madame Bovary.\n\n\n\n\n\n\n\n\n\n\nThe Critical Question\n\n\n\nLook at that plot carefully. The gray points are the actual sentiment scores from the text. The red line is what DCT produces.\nIs the red line:\nA. Revealing an underlying pattern that was hidden in the noisy data?\nB. Imposing a smooth curve that makes the data look more structured than it really is?\nThis is not just a theoretical questionâ€”itâ€™s about whether we can trust what the algorithm is telling us.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Texts, Algorithms, and Black-Boxes</span>"
    ]
  },
  {
    "objectID": "tutorials/sentiment-and-syuzhet.html#different-lexicons-different-results",
    "href": "tutorials/sentiment-and-syuzhet.html#different-lexicons-different-results",
    "title": "1Â  Texts, Algorithms, and Black-Boxes",
    "section": "1.7 Different Lexicons, Different Results",
    "text": "1.7 Different Lexicons, Different Results\nOne reason to be skeptical: different sentiment lexicons produce different results. The moodswing package includes four lexicons:\n\n# Compare different sentiment lexicons\nlexicons = [\"syuzhet\", \"afinn\", \"bing\", \"nrc\"]\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\nfig.suptitle('Madame Bovary: Different Sentiment Lexicons', fontsize=14, fontweight='bold')\n\nfor idx, lexicon in enumerate(lexicons):\n    # Calculate scores with this lexicon\n    scores_lex = analyzer.sentence_scores(sentences, method=lexicon)\n    \n    # Transform\n    traj_lex = prepare_trajectory(\n        scores_lex,\n        dct_transform=DCTTransform(low_pass_size=5, output_length=100, scale_range=True)\n    )\n    \n    # Plot\n    ax = axes[idx // 2, idx % 2]\n    ax.plot(traj_lex.dct, color='tomato', linewidth=2)\n    ax.set_title(f'{lexicon.upper()} Lexicon')\n    ax.set_xlabel('Narrative Time')\n    ax.set_ylabel('Sentiment')\n    ax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nIf the method were revealing something real about the novelâ€™s emotional structure, wouldnâ€™t all lexicons show roughly the same pattern?",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Texts, Algorithms, and Black-Boxes</span>"
    ]
  },
  {
    "objectID": "tutorials/sentiment-and-syuzhet.html#so-whats-the-point",
    "href": "tutorials/sentiment-and-syuzhet.html#so-whats-the-point",
    "title": "1Â  Texts, Algorithms, and Black-Boxes",
    "section": "1.8 So Whatâ€™s the Point?",
    "text": "1.8 So Whatâ€™s the Point?\nThis brings us back to our opening questions:\n\n1.8.1 Should we use sentiment analysis at all?\nYes, but critically. Itâ€™s a tool, not a magic answer machine.\n\n\n1.8.2 What can we actually learn?\n\nComparative patterns across many texts might be meaningful\nRelative differences between authors, genres, or time periods\nExploratory insights that lead to closer reading\n\n\n\n1.8.3 What should we be skeptical of?\n\nSingle-text â€œfindingsâ€ (one curve doesnâ€™t tell us much)\nOver-interpretation of smooth patterns\nTreating scores as truth rather than rough approximations\nIgnoring what the method assumes about language and emotion\n\n\n\n1.8.4 How much do we need to know about the math?\nYou donâ€™t need to derive the DCT formula. But you should understand:\n\nWhat the method is trying to do (smooth noisy data)\nWhat assumptions it makes (emotions are continuous, texts have structure)\nWhat could go wrong (imposing patterns, missing context)\nHow to interpret results critically",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Texts, Algorithms, and Black-Boxes</span>"
    ]
  },
  {
    "objectID": "tutorials/sentiment-and-syuzhet.html#practical-applications",
    "href": "tutorials/sentiment-and-syuzhet.html#practical-applications",
    "title": "1Â  Texts, Algorithms, and Black-Boxes",
    "section": "1.9 Practical Applications",
    "text": "1.9 Practical Applications\nDespite the controversy, sentiment trajectory analysis can be genuinely useful when applied to interesting questions:\nGood uses:\n\nComparing narrative structures across genres (do mysteries have different arcs than romances?)\nTracking emotional tone in historical documents over time\nAnalyzing sentiment patterns in social media during events\nExploring how different authors structure emotional progression\n\nLess useful:\n\nClaiming to have discovered â€œthe seven basic plotsâ€ from curves\nUsing a single novelâ€™s trajectory as evidence for anything\nIgnoring literary context and close reading\nTreating sentiment scores as psychological reality",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Texts, Algorithms, and Black-Boxes</span>"
    ]
  },
  {
    "objectID": "tutorials/sentiment-and-syuzhet.html#discussion-questions",
    "href": "tutorials/sentiment-and-syuzhet.html#discussion-questions",
    "title": "1Â  Texts, Algorithms, and Black-Boxes",
    "section": "1.10 Discussion Questions",
    "text": "1.10 Discussion Questions\n\nMethodological skepticism: If four different sentiment lexicons produce four different trajectory shapes for the same novel, what does that tell us about the validity of plot shape analysis? Can any of them be â€œrightâ€?\nSignal vs.Â noise: Looking at the raw vs.Â transformed plot, how do you decide whether DCT is revealing a meaningful pattern or creating an artificial one? What evidence would convince you either way?\nResearch design: Imagine you want to compare emotional arcs across 1,000 Victorian novels. What would make this a compelling research question? What would you need to do beyond generating trajectory plots?\nInterpretive authority: Traditional literary criticism involves close reading and interpretation. When we let an algorithm smooth sentiment scores into a plot shape, who or what is doing the interpreting? How does this change the nature of literary analysis?\nThe â€œcool visualizationâ€ problem: The trajectory plots look impressive and scientific. How do you guard against being seduced by the aesthetic appeal of computational output? When does a pretty graph become misleading?\nVocabulary and culture: Sentiment lexicons assign fixed emotional values to words. But word meanings change across time, cultures, and contexts. How might this affect analysis of historical texts? What about texts from different English-speaking countries?",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Texts, Algorithms, and Black-Boxes</span>"
    ]
  },
  {
    "objectID": "tutorials/sentiment-and-syuzhet.html#common-pitfalls",
    "href": "tutorials/sentiment-and-syuzhet.html#common-pitfalls",
    "title": "1Â  Texts, Algorithms, and Black-Boxes",
    "section": "1.11 Common Pitfalls",
    "text": "1.11 Common Pitfalls\n\n\n\n\n\n\nWhat Can Go Wrong\n\n\n\nOver-smoothing: Too much smoothing (low low_pass_size) can make everything look like a simple downward or upward arc, erasing meaningful variation.\nSingle-text findings: One novelâ€™s trajectory tells you very little. You need comparative analysis across multiple texts to make claims.\nIgnoring genre conventions: Romance novels might systematically differ from tragedies not because of authorial choice but because of genre requirements.\nDecontextualization: A word like â€œawfulâ€ meant â€œawe-inspiringâ€ in older texts, not â€œterrible.â€ Sentiment dictionaries donâ€™t account for historical semantic change.\nThe replication trap: Just because you can reproduce Jockersâ€™ results doesnâ€™t mean the method is soundâ€”it might just mean youâ€™re making the same assumptions.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Texts, Algorithms, and Black-Boxes</span>"
    ]
  },
  {
    "objectID": "tutorials/sentiment-and-syuzhet.html#experimentation-ideas",
    "href": "tutorials/sentiment-and-syuzhet.html#experimentation-ideas",
    "title": "1Â  Texts, Algorithms, and Black-Boxes",
    "section": "1.12 Experimentation Ideas",
    "text": "1.12 Experimentation Ideas\nFor hands-on practice, try:\n\nCompare novels: Load different novels from the sample data and compare their trajectories. Do different authors show different patterns?\nAdjust smoothing: Experiment with low_pass_size values from 2 to 20. How does this change the shape? At what point does it become meaningless?\nGenre analysis: If you had access to 100 mystery novels and 100 romances, how would you test whether they have systematically different emotional arcs?\nCritical replication: Try to reproduce one of Jockersâ€™ original plots. Does your version match his? What might account for differences?\nLexicon archaeology: Look up the actual Syuzhet dictionary. What words are included? What values do they have? Do you agree with the assignments?\n\n\n\n\n\n\n\nFor the Mini Lab\n\n\n\nThe corresponding Mini Lab 01 provides a Colab environment where you can experiment with this code hands-on, trying different texts and parameters without installing anything locally.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Texts, Algorithms, and Black-Boxes</span>"
    ]
  },
  {
    "objectID": "tutorials/sentiment-and-syuzhet.html#reflection-exploratory-vs.-confirmatory-research",
    "href": "tutorials/sentiment-and-syuzhet.html#reflection-exploratory-vs.-confirmatory-research",
    "title": "1Â  Texts, Algorithms, and Black-Boxes",
    "section": "1.13 Reflection: Exploratory vs.Â Confirmatory Research",
    "text": "1.13 Reflection: Exploratory vs.Â Confirmatory Research\nThis tutorial has focused on critical evaluation of a specific method, but the broader lesson is about how we use computational tools in humanities research.\n\n\n\n\n\n\nTwo Modes of Research\n\n\n\nExploratory analysis: Using computational methods to discover patterns, generate hypotheses, identify texts for closer reading. This is valuable! But findings are tentative and need validation.\nConfirmatory research: Testing specific hypotheses with appropriate methods and transparent assumptions. This is where computational analysis can make strong claimsâ€”but only with careful design.\nThe syuzhet controversy arose partly because exploratory findings (â€œlook at these interesting plot shapes!â€) were presented as if they were confirmatory results (â€œnovels have six basic emotional arcsâ€).\n\n\nThe takeaway: Use sentiment trajectory analysis as an exploratory tool to identify interesting patterns or texts. Then do the interpretive workâ€”close reading, historical contextualization, comparative analysisâ€”to understand what those patterns mean.\nComputational methods donâ€™t replace humanistic interpretation; they give us new ways to formulate questions and identify whatâ€™s worth interpreting.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Texts, Algorithms, and Black-Boxes</span>"
    ]
  },
  {
    "objectID": "tutorials/sentiment-and-syuzhet.html#further-reading",
    "href": "tutorials/sentiment-and-syuzhet.html#further-reading",
    "title": "1Â  Texts, Algorithms, and Black-Boxes",
    "section": "1.14 Further Reading",
    "text": "1.14 Further Reading\n\nSwafford, A. (2015). Problems with the Syuzhet Package\nJockers, M. (2015). Revealing Sentiment and Plot Arcs with the Syuzhet Package\nPiper, A. (2018). Enumerations: Data and Literary Study. University of Chicago Press. (Chapter on sentiment)\nReagan, A.J., et al.Â (2016). â€œThe emotional arcs of stories are dominated by six basic shapes.â€ EPJ Data Science, 5(1), 31.\nUnderwood, T. (2019). Distant Horizons: Digital Evidence and Literary Change. University of Chicago Press. (On appropriate use of computational methods)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Texts, Algorithms, and Black-Boxes</span>"
    ]
  },
  {
    "objectID": "tutorials/sentiment-and-syuzhet.html#key-takeaways",
    "href": "tutorials/sentiment-and-syuzhet.html#key-takeaways",
    "title": "1Â  Texts, Algorithms, and Black-Boxes",
    "section": "1.15 Key Takeaways",
    "text": "1.15 Key Takeaways\n\nComputational methods are not neutral - They make assumptions and can impose structure\nUnderstanding matters - You need to know what methods do, even if not the full math\nBe critical - Cool visualizations arenâ€™t the same as meaningful findings\nContext is essential - Numbers need interpretation informed by domain knowledge\nTools enable questions - Use methods to explore interesting problems, not just to make pretty graphs\n\nThis controversy from 2015 is still relevant because these issues apply to all computational humanities work: text classification, topic modeling, word embeddings, and more. The questions donâ€™t go awayâ€”how much do we know, how much should we know, and what counts as a meaningful finding?",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Texts, Algorithms, and Black-Boxes</span>"
    ]
  },
  {
    "objectID": "tutorials/corpus-basics.html",
    "href": "tutorials/corpus-basics.html",
    "title": "2Â  Corpus Basics: Tokenization and Processing Pipelines",
    "section": "",
    "text": "2.1 A Simple Processing Pipeline\nIn order to carry out any sort of computational analysis, we need to convert text into numbers. Although this is now fairly easy to do with computers, it nonetheless constitutes a radical reorganization of text.\nA processing pipeline typically looks something like this:\nTo begin seeing what this looks like in practice, letâ€™s start with a toy example.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Corpus Basics: Tokenization and Processing Pipelines</span>"
    ]
  },
  {
    "objectID": "tutorials/corpus-basics.html#a-simple-processing-pipeline",
    "href": "tutorials/corpus-basics.html#a-simple-processing-pipeline",
    "title": "2Â  Corpus Basics: Tokenization and Processing Pipelines",
    "section": "",
    "text": "A processing pipeline\n\n\n\n\n2.1.1 A Toy Example\nFirst, weâ€™ll create an object consisting of a character string. In this case, the first sentence from A Tale of Two Cities:\n\nIt was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair.\n\n\ntotc_txt = \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair.\"\n\n\n\n2.1.2 Tokenization\nğŸ¤” How would you turn this text into something you can count?\nWe need to convert text into numbers in order to carry out any kind of computational or statistical analysis. One obvious way would be to simply split the text at spaces.\n\nsplit_totc = totc_txt.split(\" \")\nprint(split_totc)\n\n['It', 'was', 'the', 'best', 'of', 'times,', 'it', 'was', 'the', 'worst', 'of', 'times,', 'it', 'was', 'the', 'age', 'of', 'wisdom,', 'it', 'was', 'the', 'age', 'of', 'foolishness,', 'it', 'was', 'the', 'epoch', 'of', 'belief,', 'it', 'was', 'the', 'epoch', 'of', 'incredulity,', 'it', 'was', 'the', 'season', 'of', 'Light,', 'it', 'was', 'the', 'season', 'of', 'Darkness,', 'it', 'was', 'the', 'spring', 'of', 'hope,', 'it', 'was', 'the', 'winter', 'of', 'despair.']\n\n\n\n\n2.1.3 Counting Tokens\nTo create a table of counts, weâ€™ll use Polars for data manipulation and Pythonâ€™s built-in Counter for counting.\n\nimport polars as pl\nfrom collections import Counter\n\n# Count tokens\ntotc_counts = Counter(split_totc)\n\n# Convert to DataFrame\ncounts_df = pl.DataFrame(totc_counts).transpose(\n    include_header=True, \n    header_name=\"token\"\n).rename({\"column_0\": \"count\"})\n\nprint(counts_df.head())\n\nshape: (5, 2)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ token â”† count â”‚\nâ”‚ ---   â”† ---   â”‚\nâ”‚ str   â”† i64   â”‚\nâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\nâ”‚ It    â”† 1     â”‚\nâ”‚ was   â”† 10    â”‚\nâ”‚ the   â”† 10    â”‚\nâ”‚ best  â”† 1     â”‚\nâ”‚ of    â”† 10    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nThe process of splitting the string vector into constituent parts is called tokenization or word segmentation. Think of this as telling the computer how to define a word (or a â€œtokenâ€, which is a more precise, technical term).\nIn this case, weâ€™ve done it in an extremely simple wayâ€”by defining a token as any string that is bounded by spaces. As a result, we have different tokens for the third-person pronoun it:\n\n# Filter for variations of \"it\"\ncounts_df.filter(pl.col(\"token\").str.contains(r\"(?i)^it$\"))\n\n\nshape: (2, 2)\n\n\n\ntoken\ncount\n\n\nstr\ni64\n\n\n\n\n\"It\"\n1\n\n\n\"it\"\n9\n\n\n\n\n\n\n\n\n\n\n\n\nRegex Patterns\n\n\n\nThe pattern (?i)^it$ is a regular expression where: - (?i) makes the search case-insensitive - ^ marks the beginning of the string - $ marks the end of the string\nThis ensures we match exactly â€œitâ€ or â€œItâ€, not words like â€œwithâ€ or â€œitâ€™sâ€.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Corpus Basics: Tokenization and Processing Pipelines</span>"
    ]
  },
  {
    "objectID": "tutorials/corpus-basics.html#using-models-to-tokenize-at-scale",
    "href": "tutorials/corpus-basics.html#using-models-to-tokenize-at-scale",
    "title": "2Â  Corpus Basics: Tokenization and Processing Pipelines",
    "section": "2.2 Using Models to Tokenize at Scale",
    "text": "2.2 Using Models to Tokenize at Scale\nIn order to execute this process at scale, we have a couple of options:\n\nPre-processing: Manipulate text by converting to lowercase, removing punctuation, deleting non-letter sequencesâ€”then split on spaces. This is called text cleaning.\nModel-based parsing: Pass data to an algorithm or model with complex rules or probabilities encoded into it.\n\nThe second option is more computationally intensive, but allows us to extract additional information: part-of-speech tags, named entities, sentiment scores, or syntactic relations.\nFor most of these tutorials, we use spaCy models to tokenize and parse data. These models are efficient, well-documented, and widely used in industry.\n\n2.2.1 Load Libraries and Model\nWeâ€™ll use docuscospacy, which provides convenient wrappers around spaCy functionality:\n\nimport docuscospacy as ds\nimport spacy\n\n# Load the custom spaCy model\nnlp = spacy.load(\"en_docusco_spacy\")\n\n\n\n2.2.2 Parsing Text with spaCy\nParsing text with spaCy requires three steps:\n\nInitialize an instance of the model\nLoad some text\n\nPass the text to the model\n\n\n# Process the text\ndoc = nlp(totc_txt)\n\n# Display token information\nfor token in doc:\n    print(f\"{token.text:15} {token.pos_:10} {token.tag_:10}\")\n\nIt                         PPH1      \nwas                        VBDZ      \nthe                        AT        \nbest                       JJT       \nof                         IO        \ntimes                      NNT2      \n,                          Y         \nit                         PPH1      \nwas                        VBDZ      \nthe                        AT        \nworst                      JJT       \nof                         IO        \ntimes                      NNT2      \n,                          Y         \nit                         PPH1      \nwas                        VBDZ      \nthe                        AT        \nage                        NN1       \nof                         IO        \nwisdom                     NN1       \n,                          Y         \nit                         PPH1      \nwas                        VBDZ      \nthe                        AT        \nage                        NN1       \nof                         IO        \nfoolishness                NN1       \n,                          Y         \nit                         PPH1      \nwas                        VBDZ      \nthe                        AT        \nepoch                      NN1       \nof                         IO        \nbelief                     NN1       \n,                          Y         \nit                         PPH1      \nwas                        VBDZ      \nthe                        AT        \nepoch                      NN1       \nof                         IO        \nincredulity                NN1       \n,                          Y         \nit                         PPH1      \nwas                        VBDZ      \nthe                        AT        \nseason                     NNT1      \nof                         IO        \nLight                      NN1       \n,                          Y         \nit                         PPH1      \nwas                        VBDZ      \nthe                        AT        \nseason                     NNT1      \nof                         IO        \nDarkness                   NN1       \n,                          Y         \nit                         PPH1      \nwas                        VBDZ      \nthe                        AT        \nspring                     NN1       \nof                         IO        \nhope                       NN1       \n,                          Y         \nit                         PPH1      \nwas                        VBDZ      \nthe                        AT        \nwinter                     NNT1      \nof                         IO        \ndespair                    NN1       \n.                          Y         \n\n\nNotice how spaCy automatically: - Separates punctuation from words - Assigns part-of-speech tags (PRON, AUX, DET, etc.) - Provides fine-grained tags (PRP, VBD, DT, etc.)\n\n\n2.2.3 Using docuscospacy to Automate Processing\nTo use docuscospacy, we need a DataFrame with document IDs and text:\n\ntotc_corpus = pl.DataFrame({\n    \"doc_id\": \"totc\", \n    \"text\": [totc_txt]\n})\n\n# Parse the corpus\ntotc_tokens = ds.docuscope_parse(totc_corpus, nlp_model=nlp, n_process=4)\n\n# Create frequency table\nfrequency_table = ds.frequency_table(totc_tokens)\nprint(frequency_table.head())\n\nshape: (5, 5)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ Token â”† Tag  â”† AF  â”† RF            â”† Range â”‚\nâ”‚ ---   â”† ---  â”† --- â”† ---           â”† ---   â”‚\nâ”‚ str   â”† str  â”† u32 â”† f64           â”† f64   â”‚\nâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\nâ”‚ it    â”† PPH1 â”† 10  â”† 166666.666667 â”† 100.0 â”‚\nâ”‚ of    â”† IO   â”† 10  â”† 166666.666667 â”† 100.0 â”‚\nâ”‚ the   â”† AT   â”† 10  â”† 166666.666667 â”† 100.0 â”‚\nâ”‚ was   â”† VBDZ â”† 10  â”† 166666.666667 â”† 100.0 â”‚\nâ”‚ age   â”† NN1  â”† 2   â”† 33333.333333  â”† 100.0 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n2.2.4 Processing a Larger Dataset\nLetâ€™s read in a larger corpus from the course GitHub repository:\n\n# Load sample corpus\ndf = pl.read_parquet(\"https://github.com/browndw/humanities_analytics/raw/refs/heads/main/data/data_tables/sample_corpus.parquet\")\n\nprint(f\"Loaded {df.height} documents\")\n\nLoaded 400 documents\n\n\nNow parse the entire corpus (this takes about 2 minutes):\n\n# Parse corpus (set eval: false to avoid re-running during rendering)\nds_tokens = ds.docuscope_parse(df, nlp_model=nlp, n_process=4)\n\n# Create frequency table\nwc = ds.frequency_table(ds_tokens)\n\nprint(wc.head())\nprint(wc.tail())\n\nFrom the frequency table, we can extract important information like total word count:\n\n# Sum the Absolute Frequency column\ntotal_words = wc.select(\"AF\").sum()\nprint(f\"Total corpus size: {total_words:,} words\")",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Corpus Basics: Tokenization and Processing Pipelines</span>"
    ]
  },
  {
    "objectID": "tutorials/corpus-basics.html#common-pitfalls-and-considerations",
    "href": "tutorials/corpus-basics.html#common-pitfalls-and-considerations",
    "title": "2Â  Corpus Basics: Tokenization and Processing Pipelines",
    "section": "2.3 Common Pitfalls and Considerations",
    "text": "2.3 Common Pitfalls and Considerations\nWhen working with tokenization and parsing, watch out for these issues:\n\n2.3.1 Punctuation and Special Characters\nSimple space-splitting treats \"times,\" and \"times\" as different tokens. Model-based tokenizers handle this better, but you still need to think about: - Contractions (don't, isn't) - Hyphenated words (twenty-first, self-aware) - URLs and email addresses - Emoticons and emoji\n\n\n2.3.2 Language and Domain Assumptions\nspaCy models are trained on specific corpora. The en_docusco_spacy model works well for academic and formal writing, but may struggle with: - Historical texts (archaic spelling, grammar) - Social media (hashtags, (mentions?), slang) - Technical jargon or specialized vocabularies - Non-standard dialects\n\n\n2.3.3 Computational vs.Â Interpretive Work\nRemember: tokenization is interpretation. Every choice you makeâ€”how to split, what counts as a word, whether to lowercaseâ€”shapes your analysis. Thereâ€™s no â€œneutralâ€ way to process text.\n\n\n\n\n\n\nCritical Question\n\n\n\nIf the model assigns part-of-speech tags with 95% accuracy, what does that mean for a corpus of 100,000 words? Thatâ€™s 5,000 potential errors. How do you account for this in your analysis?",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Corpus Basics: Tokenization and Processing Pipelines</span>"
    ]
  },
  {
    "objectID": "tutorials/corpus-basics.html#why-this-matters-for-humanities-research",
    "href": "tutorials/corpus-basics.html#why-this-matters-for-humanities-research",
    "title": "2Â  Corpus Basics: Tokenization and Processing Pipelines",
    "section": "2.4 Why This Matters for Humanities Research",
    "text": "2.4 Why This Matters for Humanities Research\nComputational text analysis isnâ€™t just about efficiencyâ€”it enables entirely new kinds of questions:\n\nScale: Read thousands of novels, not dozens\nPatterns: Identify linguistic trends across time, geography, or genre\nComparison: Systematically compare how different authors use language\nDiscovery: Find unexpected patterns that close reading might miss\n\nBut remember: computational methods complement close reading, they donâ€™t replace it. The goal is to move between distant reading (patterns across large corpora) and close reading (careful interpretation of individual texts).",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Corpus Basics: Tokenization and Processing Pipelines</span>"
    ]
  },
  {
    "objectID": "tutorials/corpus-basics.html#discussion-questions",
    "href": "tutorials/corpus-basics.html#discussion-questions",
    "title": "2Â  Corpus Basics: Tokenization and Processing Pipelines",
    "section": "2.5 Discussion Questions",
    "text": "2.5 Discussion Questions\n\nTokenization choices: Why might simple space-splitting not work well for all languages or text types? What problems arise with languages that donâ€™t use spaces between words (like Chinese or Japanese)?\nModel reliability: What additional information does the spaCy model provide compared to simple splitting? How confident should we be in these assignments? What kinds of errors might the model make?\nComputational trade-offs: The spaCy approach is more computationally expensive than simple splitting. When might the extra computational cost be worth it? When might it not be?\nScale and infrastructure: We processed a small corpus quickly. What challenges might arise when processing millions of documents? How would you approach that? What about accessibilityâ€”not everyone has access to powerful servers.\nCritical methodology: The model treats every text the same wayâ€”no attention to historical context, genre conventions, or authorial style in the parsing stage. How do you maintain critical, contextual awareness when using computational methods?",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Corpus Basics: Tokenization and Processing Pipelines</span>"
    ]
  },
  {
    "objectID": "tutorials/corpus-basics.html#further-exploration",
    "href": "tutorials/corpus-basics.html#further-exploration",
    "title": "2Â  Corpus Basics: Tokenization and Processing Pipelines",
    "section": "2.6 Further Exploration",
    "text": "2.6 Further Exploration\n\n\n\n\n\n\nFor the Mini Lab\n\n\n\nIn Mini Lab 02, youâ€™ll get hands-on practice with these concepts. Try:\n\nTokenizing text from different domains (tweets, legal documents, poetry)\nComparing outputs from different spaCy models\nAnalyzing frequency tables to understand patterns in the corpus\nExperimenting with edge cases that break simple tokenization\n\n\n\n\n\n\n\n\n\nKey Takeaways\n\n\n\n\nSimple tokenization (splitting on spaces) is fast but loses linguistic information\nModel-based tokenization (spaCy) is slower but provides rich linguistic features\nTrade-offs matter: Choose your approach based on your research questions\nScale changes everything: What works for one sentence may not work for millions of documents\nTokenization is interpretation: Every processing choice shapes your analysis\nErrors compound: Model accuracy matters more at scale\nContext is critical: Maintain awareness of what the model canâ€™t capture",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>Corpus Basics: Tokenization and Processing Pipelines</span>"
    ]
  },
  {
    "objectID": "tutorials/spacy-basics.html",
    "href": "tutorials/spacy-basics.html",
    "title": "3Â  NLP Processing with spaCy",
    "section": "",
    "text": "3.1 Introduction\nMost text analysis workflows follow a similar pattern: read text â†’ process text â†’ extract features â†’ analyze features â†’ interpret results. The middle stepsâ€”processing and feature extractionâ€”often involve the same computational operations: tokenization, part-of-speech tagging, parsing, and entity recognition. Instead of reinventing these steps for every project, NLP processing pipelines automate them.\nspaCy is an industrial-strength NLP library that provides a complete pipeline architecture. Unlike tools designed for teaching (NLTK) or experimental research (Stanford CoreNLP), spaCy prioritizes speed, accuracy, and production readiness. Itâ€™s the engine behind many commercial NLP applications and can process millions of words per second on modern hardware.\nResearch questions spaCy can help answer:\nThis tutorial covers:",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>NLP Processing with spaCy</span>"
    ]
  },
  {
    "objectID": "tutorials/spacy-basics.html#introduction",
    "href": "tutorials/spacy-basics.html#introduction",
    "title": "3Â  NLP Processing with spaCy",
    "section": "",
    "text": "Why Processing Pipelines Matter\n\n\n\nA good pipeline:\n\nStandardizes preprocessing: Everyone on a team gets the same tokenization, the same POS tags, the same parses\nEnables reproducibility: Save the pipeline configuration and get identical results later\nSupports complex workflows: Chain operations (tokenize â†’ tag â†’ parse â†’ extract) without manual coordination\nFacilitates reuse: Train custom components (NER, text classification) and plug them into existing pipelines\nImproves efficiency: Process once, extract many features (tokens, lemmas, dependencies, entities) in one pass\n\n\n\n\n\nHow do sentence structures differ between genres or time periods?\nWhich characters are described with which adjectives across a novel?\nWhere do key thematic words cluster in a narrative?\nHow do authors distribute place names or temporal references?\nWhich documents in a corpus share similar vocabulary profiles?\n\n\n\nPipeline architecture: What spaCy does automatically when you process text\nToken-level analysis: Exploring word attributes, frequencies, and distributions\nSentence-level analysis: Segmentation and structural patterns\nNamed entity recognition: Extracting people, places, organizations, and events\nPart-of-speech tagging: Identifying grammatical categories and stylistic patterns\nDependency parsing: Understanding grammatical relationships\nWord embeddings: Semantic similarity from pre-trained vectors\nDocument vectorization: Comparing texts numerically",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>NLP Processing with spaCy</span>"
    ]
  },
  {
    "objectID": "tutorials/spacy-basics.html#understanding-the-spacy-pipeline",
    "href": "tutorials/spacy-basics.html#understanding-the-spacy-pipeline",
    "title": "3Â  NLP Processing with spaCy",
    "section": "3.2 Understanding the spaCy Pipeline",
    "text": "3.2 Understanding the spaCy Pipeline\n\n3.2.1 What Happens When You Process Text?\nWhen you run doc = nlp(text), spaCy executes a multi-stage pipeline:\n1. Tokenization: Splits text into words, punctuation, numbers\n2. Part-of-speech tagging: Assigns grammatical categories (noun, verb, adjective, etc.)\n3. Lemmatization: Reduces words to dictionary forms (â€œrunningâ€ â†’ â€œrunâ€)\n4. Dependency parsing: Identifies grammatical relationships between words\n5. Named entity recognition (NER): Labels spans as people, organizations, locations, etc.\n6. (Optional) Additional components: Sentiment analysis, text classification, custom extractors\nAll of these happen in one pass over the text. The result is a Doc object containing the original text plus all linguistic annotations.\n\n\n3.2.2 Models and Sizes\nspaCyâ€™s English models come in three sizes:\n\nen_core_web_sm (small, ~12 MB): No word vectors, faster, less accurate\nen_core_web_md (medium, ~40 MB): 300-dimensional word vectors, balanced speed/accuracy\nen_core_web_lg (large, ~560 MB): More vectors, highest accuracy, slower\n\nFor this tutorial, we use en_core_web_mdâ€”large enough for meaningful semantic analysis, small enough for quick processing.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>NLP Processing with spaCy</span>"
    ]
  },
  {
    "objectID": "tutorials/spacy-basics.html#loading-and-pre-processing-texts",
    "href": "tutorials/spacy-basics.html#loading-and-pre-processing-texts",
    "title": "3Â  NLP Processing with spaCy",
    "section": "3.3 Loading and Pre-processing Texts",
    "text": "3.3 Loading and Pre-processing Texts\nWeâ€™ll analyze two texts: Monty Python and the Holy Grail (1975 screenplay) and Jane Austenâ€™s Pride and Prejudice (1813 novel).\n\n# Load texts from GitHub\ngrail_url = 'https://github.com/browndw/humanities_analytics/raw/refs/heads/main/data/data_text/grail.txt'\npride_url = 'https://github.com/browndw/humanities_analytics/raw/refs/heads/main/data/data_text/pride.txt'\n\nwith urllib.request.urlopen(grail_url) as response:\n    grail_raw = response.read().decode('utf-8')\n\nwith urllib.request.urlopen(pride_url) as response:\n    pride_raw = response.read().decode('utf-8')\n\n\n3.3.1 The Importance of Pre-processing\nThe Monty Python script includes character names, stage directions, and formatting. Letâ€™s peek:\n\ngrail_raw[:500]\n\n'SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\nSOLDIER #1: Pull the other one!\\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and mast'\n\n\nQuestion: Should we keep this metadata?\nAnswer depends on your research question:\n\nStudying dialogue style â†’ remove character names and stage directions\nAnalyzing narrative structure â†’ keep stage directions (they describe action)\nExamining character-specific language â†’ keep character labels, analyze per-character\n\nFor this tutorial, weâ€™ll extract just the dialogue:\n\n# Remove character names (anything before a colon at line start)\ngrail_raw = re.sub(r'\\n.*?: ', ' ', grail_raw)\ngrail_raw = re.sub(r'^.*?: ', ' ', grail_raw)\n# Remove stage directions in brackets\ngrail_raw = re.sub(r'\\[.*?\\]', ' ', grail_raw)\n# Normalize whitespace\ngrail_raw = \" \".join(grail_raw.split()).strip()\n\n\n# Pride & Prejudice needs only whitespace normalization\npride_raw = \" \".join(pride_raw.split()).strip()\n\n\n\n3.3.2 Parsing the Texts\nNow we pass the cleaned texts through the pipeline:\n\ngrail = nlp(grail_raw)\npride = nlp(pride_raw)\n\nThis takes ~20-30 seconds for Pride and Prejudice (~120K words). Each word is now annotated with POS tags, lemmas, dependency labels, and entity types.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>NLP Processing with spaCy</span>"
    ]
  },
  {
    "objectID": "tutorials/spacy-basics.html#token-level-analysis",
    "href": "tutorials/spacy-basics.html#token-level-analysis",
    "title": "3Â  NLP Processing with spaCy",
    "section": "3.4 Token-Level Analysis",
    "text": "3.4 Token-Level Analysis\n\n3.4.1 Basic Token Operations\nA Doc object is an iterable sequence of Token objects:\n\n# First 10 tokens\npride[:10]\n\nIt is a truth universally acknowledged, that a single\n\n\n\n# Total token count (includes punctuation)\nlen(pride)\n\n143771\n\n\nTokens have rich attributes:\n\n# Examine a single token\ntoken = pride[4]  # \"universally\"\nprint(f\"Text: {token.text}\")\nprint(f\"Lemma: {token.lemma_}\")\nprint(f\"POS: {token.pos_}\")\nprint(f\"Tag: {token.tag_}\")\nprint(f\"Index: {token.i}\")\nprint(f\"Character index: {token.idx}\")\nprint(f\"Is punctuation?: {token.is_punct}\")\nprint(f\"Is stop word?: {token.is_stop}\")\n\nText: universally\nLemma: universally\nPOS: ADV\nTag: RB\nIndex: 4\nCharacter index: 14\nIs punctuation?: False\nIs stop word?: False\n\n\nKey attributes:\n\n.text: Original word form\n.lemma_: Dictionary form (â€œrunningâ€ â†’ â€œrunâ€)\n.pos_: Coarse part-of-speech (NOUN, VERB, ADJ, etc.)\n.tag_: Fine-grained tag (NN, VBG, JJ, etc. from Penn Treebank)\n.i: Token index in document\n.is_punct, .is_stop, .is_alpha: Boolean flags\n.ent_type_: Named entity label (if applicable)\n\n\n\n3.4.2 Frequency Distributions\nCount word frequencies with Counter:\n\npride_counts = Counter([tok.text for tok in pride])\npride_counts.most_common(10)\n\n[(',', 9125),\n ('.', 5021),\n ('to', 4110),\n ('the', 4058),\n ('of', 3597),\n ('\"', 3521),\n ('and', 3426),\n ('her', 2137),\n ('I', 2067),\n ('a', 1905)]\n\n\nUnsurprisingly, function words dominate. Filter for content words:\n\n# Count only nouns, verbs, adjectives\ncontent_words = [tok.lemma_.lower() for tok in pride \n                 if tok.pos_ in ['NOUN', 'VERB', 'ADJ'] and not tok.is_stop]\nCounter(content_words).most_common(10)\n\n[('say', 450),\n ('know', 389),\n ('think', 332),\n ('sister', 294),\n ('come', 254),\n ('see', 245),\n ('good', 226),\n ('time', 223),\n ('go', 187),\n ('great', 186)]\n\n\n\n\n3.4.3 Visualizing Token Distributions\nUse token indices (.i) to plot word locations across narrative time:\n\n# Find all instances of \"love\" and plot their positions\nlove_indices = [word.i for word in pride if word.text.lower() == 'love']\nplt.figure(figsize=(12, 3))\nplt.hist(love_indices, bins=50, color='skyblue', edgecolor='black')\nplt.xlabel('Token Position')\nplt.ylabel('Frequency')\nplt.title('Distribution of \"love\" in Pride and Prejudice')\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation: Where does â€œloveâ€ cluster? Early, middle, or late in the novel? Clusters might correspond to romantic turning points.\nCompare multiple words:\n\ndef get_histogram(word, doc, bins=50):\n    indices = [tok.i for tok in doc if tok.text.lower() == word.lower()]\n    hist, bin_edges = np.histogram(indices, bins=bins, range=(0, len(doc)))\n    return hist\n\nwords = ['love', 'hate', 'pride', 'prejudice']\nword_hists = [get_histogram(w, pride) for w in words]\nwords_df = pd.DataFrame(word_hists, index=words).T\n\nwords_df.plot(subplots=True, figsize=(14, 8), legend=False, color=['steelblue', 'darkred', 'green', 'purple'])\nplt.suptitle('Narrative Arcs of Key Words in Pride and Prejudice')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComputational Reasoning: From Patterns to Interpretation\n\n\n\nFinding that â€œprideâ€ appears more in early chapters and â€œloveâ€ more in later chapters is description. Interpreting this as reflecting the novelâ€™s thematic arc from social judgment to romantic resolution is interpretation.\nComputational methods provide patterns; literary knowledge explains them. Always ask:\n\nWhat does this pattern show? (Description)\nWhy does it matter? (Interpretation)\nWhat alternative explanations exist? (Critique)\nHow would I validate this with close reading? (Triangulation)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>NLP Processing with spaCy</span>"
    ]
  },
  {
    "objectID": "tutorials/spacy-basics.html#sentence-level-analysis",
    "href": "tutorials/spacy-basics.html#sentence-level-analysis",
    "title": "3Â  NLP Processing with spaCy",
    "section": "3.5 Sentence-Level Analysis",
    "text": "3.5 Sentence-Level Analysis\n\n3.5.1 Sentence Segmentation\nspaCy automatically segments text into sentences:\n\n# Count sentences\npride_sents = list(pride.sents)\nlen(pride_sents)\n\n5844\n\n\n\n# First sentence\nnext(pride.sents)\n\nIt is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife.\n\n\n\n# Last sentence\npride_sents[-1]\n\nDarcy, as well as Elizabeth, really loved them; and they were both ever sensible of the warmest gratitude towards the persons who, by bringing her into Derbyshire, had been the means of uniting them.\n\n\n\n\n3.5.2 Sentence Length Analysis\nAverage sentence length is a basic stylometric measure:\n\npride_sent_lengths = [len(sent) for sent in pride_sents]\navg_length = np.mean(pride_sent_lengths)\nprint(f\"Average sentence length: {avg_length:.2f} tokens\")\nprint(f\"Longest sentence: {max(pride_sent_lengths)} tokens\")\nprint(f\"Shortest sentence: {min(pride_sent_lengths)} tokens\")\n\nAverage sentence length: 24.60 tokens\nLongest sentence: 200 tokens\nShortest sentence: 2 tokens\n\n\nPlot the distribution:\n\nplt.figure(figsize=(10, 5))\nplt.hist(pride_sent_lengths, bins=50, color='skyblue', edgecolor='black')\nplt.xlabel('Sentence Length (tokens)')\nplt.ylabel('Frequency')\nplt.title('Sentence Length Distribution in Pride and Prejudice')\nplt.axvline(avg_length, color='red', linestyle='--', label=f'Mean: {avg_length:.1f}')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nCompare across texts or time periods: Are 19th-century novels more syntactically complex than 20th-century screenplays? Do sentence lengths correlate with narrative tension?\n\n\n3.5.3 Finding Extreme Sentences\nLongest sentence:\n\nlongest_sent = max(pride_sents, key=len)\nprint(f\"Length: {len(longest_sent)} tokens\")\nprint(longest_sent)\n\nLength: 200 tokens\nEvery lingering struggle in his favour grew fainter and fainter; and in farther justification of Mr. Darcy, she could not but allow that Mr. Bingley, when questioned by Jane, had long ago asserted his blamelessness in the affair; that proud and repulsive as were his manners, she had never, in the whole course of their acquaintance--an acquaintance which had latterly brought them much together, and given her a sort of intimacy with his ways--seen anything that betrayed him to be unprincipled or unjust--anything that spoke him of irreligious or immoral habits; that among his own connections he was esteemed and valued--that even Wickham had allowed him merit as a brother, and that she had often heard him speak so affectionately of his sister as to prove him capable of some amiable feeling; that had his actions been what Mr. Wickham represented them, so gross a violation of everything right could hardly have been concealed from the world; and that friendship between a person capable of it, and such an amiable man as Mr. Bingley, was incomprehensible.\n\n\nShortest non-trivial sentence:\n\nshort_sents = [sent for sent in pride_sents if len(sent) &gt; 2]\nshortest_sent = min(short_sents, key=len)\nprint(f\"Length: {len(shortest_sent)} tokens\")\nprint(shortest_sent)\n\nLength: 3 tokens\n\"Oh!",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>NLP Processing with spaCy</span>"
    ]
  },
  {
    "objectID": "tutorials/spacy-basics.html#named-entity-recognition-ner",
    "href": "tutorials/spacy-basics.html#named-entity-recognition-ner",
    "title": "3Â  NLP Processing with spaCy",
    "section": "3.6 Named Entity Recognition (NER)",
    "text": "3.6 Named Entity Recognition (NER)\n\n3.6.1 Extracting Entity Types\nNamed entities are accessible via doc.ents:\n\n# What entity types appear in Monty Python?\ngrail_entity_types = set([ent.label_ for ent in grail.ents])\ngrail_entity_types\n\n{'CARDINAL',\n 'DATE',\n 'FAC',\n 'GPE',\n 'LANGUAGE',\n 'LAW',\n 'LOC',\n 'NORP',\n 'ORDINAL',\n 'ORG',\n 'PERSON',\n 'PRODUCT',\n 'QUANTITY',\n 'TIME',\n 'WORK_OF_ART'}\n\n\nCommon entity labels:\n\nPERSON: People, including fictional characters\nGPE: Geopolitical entities (countries, cities, states)\nORG: Organizations, companies, institutions\nDATE: Absolute or relative dates\nWORK_OF_ART: Titles of books, songs, etc.\nNORP: Nationalities, religious or political groups\nEVENT: Named events (wars, hurricanes, etc.)\n\n\n\n3.6.2 Counting Specific Entities\nExtract place names:\n\npride_places = Counter([ent.text for ent in pride.ents if ent.label_ == 'GPE'])\npride_places.most_common(10)\n\n[('Charlotte', 73),\n ('London', 54),\n ('Hertfordshire', 41),\n ('Brighton', 23),\n ('Lucases', 12),\n ('Georgiana', 12),\n ('Scotland', 8),\n ('Lambton', 7),\n ('Wickham', 5),\n ('England', 4)]\n\n\nNote limitations: NER works best on standard modern English. Historical texts, dialect, and creative spelling cause errors. Always inspect results critically.\nExtract people:\n\npride_people = Counter([ent.text for ent in pride.ents if ent.label_ == 'PERSON'])\npride_people.most_common(10)\n\n[('Elizabeth', 625),\n ('Darcy', 407),\n ('Jane', 289),\n ('Bennet', 268),\n ('Bingley', 207),\n ('Wickham', 180),\n ('Collins', 179),\n ('Lydia', 154),\n ('Lizzy', 95),\n ('Gardiner', 94)]\n\n\n\n\n3.6.3 Validating NER with Context\nNER isnâ€™t perfect. Check context when uncertain:\n\n# Find sentences mentioning \"Bennet\"\nbennet_sents = [ent.sent for ent in pride.ents \n                if ent.label_ == 'PERSON' and 'Bennet' in ent.text]\nbennet_sents[:3]\n\n[\"My dear Mr. Bennet,\" said his lady to him one day, \"have you heard that Netherfield Park is let at last?\",\n Mr. Bennet replied that he had not.,\n Mr. Bennet made no answer.]",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>NLP Processing with spaCy</span>"
    ]
  },
  {
    "objectID": "tutorials/spacy-basics.html#part-of-speech-tagging",
    "href": "tutorials/spacy-basics.html#part-of-speech-tagging",
    "title": "3Â  NLP Processing with spaCy",
    "section": "3.7 Part-of-Speech Tagging",
    "text": "3.7 Part-of-Speech Tagging\n\n3.7.1 Understanding POS Categories\nPart-of-speech (POS) tagging assigns grammatical categories to each word. spaCy uses two tag sets:\n\nCoarse tags (.pos_): Universal Dependencies tagset (NOUN, VERB, ADJ, ADV, etc.)\nFine-grained tags (.tag_): Penn Treebank tagset (NN, VBZ, JJ, RB, etc.)\n\n\n# Examine POS tags in a sentence\nsent = list(pride.sents)[0]\nfor token in sent:\n    if not token.is_punct:\n        print(f\"{token.text:15} {token.pos_:8} {token.tag_:6} ({spacy.explain(token.tag_)})\")\n\nIt              PRON     PRP    (pronoun, personal)\nis              AUX      VBZ    (verb, 3rd person singular present)\na               DET      DT     (determiner)\ntruth           NOUN     NN     (noun, singular or mass)\nuniversally     ADV      RB     (adverb)\nacknowledged    VERB     VBD    (verb, past tense)\nthat            SCONJ    IN     (conjunction, subordinating or preposition)\na               DET      DT     (determiner)\nsingle          ADJ      JJ     (adjective (English), other noun-modifier (Chinese))\nman             NOUN     NN     (noun, singular or mass)\nin              ADP      IN     (conjunction, subordinating or preposition)\npossession      NOUN     NN     (noun, singular or mass)\nof              ADP      IN     (conjunction, subordinating or preposition)\na               DET      DT     (determiner)\ngood            ADJ      JJ     (adjective (English), other noun-modifier (Chinese))\nfortune         NOUN     NN     (noun, singular or mass)\nmust            AUX      MD     (verb, modal auxiliary)\nbe              AUX      VB     (verb, base form)\nin              ADP      IN     (conjunction, subordinating or preposition)\nwant            NOUN     NN     (noun, singular or mass)\nof              ADP      IN     (conjunction, subordinating or preposition)\na               DET      DT     (determiner)\nwife            NOUN     NN     (noun, singular or mass)\n\n\n\n\n3.7.2 POS Distribution Analysis\nCompare POS distributions across texts to detect stylistic differences:\n\ndef get_pos_distribution(doc):\n    \"\"\"Calculate POS tag frequencies as proportions.\"\"\"\n    pos_counts = Counter([tok.pos_ for tok in doc if not tok.is_punct and not tok.is_space])\n    total = sum(pos_counts.values())\n    return {pos: count/total for pos, count in pos_counts.items()}\n\npride_pos = get_pos_distribution(pride)\ngrail_pos = get_pos_distribution(grail)\n\n# Compare distributions\npos_df = pd.DataFrame({'Pride & Prejudice': pride_pos, 'Monty Python': grail_pos}).fillna(0)\npos_df = pos_df.sort_values('Pride & Prejudice', ascending=False)\n\n# Plot\npos_df.plot(kind='bar', figsize=(12, 5))\nplt.title('POS Distribution Comparison')\nplt.xlabel('Part of Speech')\nplt.ylabel('Proportion')\nplt.xticks(rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation: Novels typically have higher proportions of nouns (scene-setting, character description), while dialogue-heavy screenplays may show more verbs and pronouns (action, direct address).\n\n\n3.7.3 Filtering by POS\nExtract all adjectives to analyze descriptive language:\n\npride_adjectives = [tok.lemma_.lower() for tok in pride \n                    if tok.pos_ == 'ADJ' and not tok.is_stop]\nCounter(pride_adjectives).most_common(20)\n\n[('good', 219),\n ('great', 186),\n ('little', 176),\n ('young', 172),\n ('dear', 141),\n ('sure', 102),\n ('happy', 96),\n ('well', 56),\n ('possible', 56),\n ('certain', 55),\n ('able', 54),\n ('ill', 51),\n ('general', 49),\n ('present', 49),\n ('agreeable', 45),\n ('handsome', 44),\n ('impossible', 44),\n ('short', 44),\n ('fine', 38),\n ('glad', 37)]\n\n\nStylometric question: Do certain authors favor particular adjective types (evaluative vs.Â physical, subjective vs.Â objective)?",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>NLP Processing with spaCy</span>"
    ]
  },
  {
    "objectID": "tutorials/spacy-basics.html#dependency-parsing",
    "href": "tutorials/spacy-basics.html#dependency-parsing",
    "title": "3Â  NLP Processing with spaCy",
    "section": "3.8 Dependency Parsing",
    "text": "3.8 Dependency Parsing\n\n3.8.1 Understanding Syntax Trees\nDependency parsing reveals grammatical structureâ€”which words modify which, what role each word plays (subject, object, modifier).\nEach token has:\n\n.dep_: Dependency relation (e.g., â€œnsubjâ€ = nominal subject, â€œdobjâ€ = direct object)\n.head: The word this token modifies or depends on\n.children: Words that modify this token\n\n\n# Analyze a sentence\nsent = list(pride.sents)[10]\nprint(sent)\nprint()\n\nfor token in sent:\n    print(f\"{token.text:15} {token.dep_:10} {token.head.text}\")\n\n\"What is his name?\"\n\n\"               punct      is\nWhat            attr       is\nis              ROOT       is\nhis             poss       name\nname            nsubj      is\n?               punct      is\n\"               punct      is\n\n\n\n\n3.8.2 Visualizing Parse Trees\nspaCyâ€™s displacy renders dependency trees:\n\nfrom IPython.display import SVG, display\n\n# Render a short sentence\nshort_sent = list(pride.sents)[5]\nsvg = spacy.displacy.render(short_sent, style='dep', jupyter=False)\n# Display inline\ndisplay(SVG(svg))\n\n\n\n\n\n\n\n\nReading the tree:\n\nArrows point from head to dependent\nLabels describe the relationship (nsubj, dobj, amod, etc.)\nThe root is the main verb\n\n\n\n3.8.3 Extracting Syntactic Patterns\nFind adjectives modifying character names:\n\ndef adjectives_for_character(doc, character):\n    \"\"\"Find adjectives that modify a character name.\"\"\"\n    adjectives = []\n    for sent in doc.sents:\n        for word in sent:\n            if character in word.text:\n                for child in word.children:\n                    if child.pos_ == 'ADJ':\n                        adjectives.append(child.text)\n    return Counter(adjectives).most_common(10)\n\nadjectives_for_character(pride, 'Darcy')\n\n[('late', 7),\n ('superior', 1),\n ('abominable', 1),\n ('poor', 1),\n ('disagreeable', 1)]\n\n\nInterpretation: â€œFineâ€, â€œhandsomeâ€, â€œgreatâ€ describe Darcyâ€”but whatâ€™s the context? Are these narrator descriptions, Elizabethâ€™s perceptions, or other charactersâ€™ opinions? Always verify with close reading.\nFind verbs associated with characters:\n\ndef verbs_for_character(doc, character):\n    \"\"\"Find verbs where the character is the subject.\"\"\"\n    verbs = []\n    char_tokens = [tok for sent in doc.sents for tok in sent \n                   if character in tok.text]\n    \n    for token in char_tokens:\n        # Walk up the dependency tree to find governing verbs\n        for ancestor in token.ancestors:\n            if ancestor.pos_ == 'VERB':\n                verbs.append(ancestor.lemma_)\n                break  # Take only the first verb ancestor\n    \n    return Counter(verbs).most_common(20)\n\nelizabeth_verbs = verbs_for_character(pride, 'Elizabeth')\nelizabeth_verbs\n\n[('say', 55),\n ('cry', 24),\n ('reply', 21),\n ('have', 20),\n ('see', 18),\n ('look', 13),\n ('feel', 13),\n ('make', 10),\n ('find', 9),\n ('think', 8),\n ('receive', 8),\n ('go', 8),\n ('listen', 7),\n ('take', 7),\n ('help', 7),\n ('give', 7),\n ('hear', 6),\n ('speak', 6),\n ('begin', 5),\n ('ask', 5)]\n\n\nComputational reasoning question: Do character-verb associations reveal agency? If Elizabeth is the subject of active verbs (â€œsay,â€ â€œask,â€ â€œfeelâ€) more than passive constructions, does that signal narrative focalization or character autonomy?",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>NLP Processing with spaCy</span>"
    ]
  },
  {
    "objectID": "tutorials/spacy-basics.html#word-embeddings-and-semantic-similarity",
    "href": "tutorials/spacy-basics.html#word-embeddings-and-semantic-similarity",
    "title": "3Â  NLP Processing with spaCy",
    "section": "3.9 Word Embeddings and Semantic Similarity",
    "text": "3.9 Word Embeddings and Semantic Similarity\n\n3.9.1 Pre-trained Word Vectors\nThe en_core_web_md model includes 300-dimensional word vectors trained on web text using the GloVe algorithm. These vectors capture semantic relationships: words with similar meanings have similar vectors.\n\n# Compare word similarities\npeanut = nlp('peanut')\nhorse = nlp('horse')\nmockingbird = nlp('mockingbird')\n\nprint(f\"peanut â†” horse: {peanut.similarity(horse):.3f}\")\nprint(f\"peanut â†” mockingbird: {peanut.similarity(mockingbird):.3f}\")\nprint(f\"horse â†” mockingbird: {horse.similarity(mockingbird):.3f}\")\n\npeanut â†” horse: 0.640\npeanut â†” mockingbird: 0.036\nhorse â†” mockingbird: 0.096\n\n\nInterpretation: Similarity scores range from -1 (opposite) to 1 (identical). Higher scores = more semantically related.\n\n\n\n\n\n\nCritical Limitation: Generic vs.Â Contextual Embeddings\n\n\n\nspaCyâ€™s default word vectors are pre-trained and fixed. They capture general semantic relationships from a large web corpus, NOT context-specific meanings in your texts.\nExample: â€œBankâ€ in â€œriver bankâ€ vs.Â â€œbank accountâ€ gets the same vectorâ€”a blend of both meanings. The model doesnâ€™t distinguish word senses.\nImplication: Word similarities reflect broad semantic fields, not specific usage in Pride and Prejudice or Monty Python. To analyze local collocational patterns, use frequency-based methods (Mini Lab 6) or train custom embeddings.\n\n\n\n\n3.9.2 Visualizing Word Embeddings\nExtract nouns from Pride and Prejudice and reduce dimensions for 2D plotting:\n\n# Get first 150 nouns\npride_nouns = [tok for tok in pride if tok.pos_ == 'NOUN'][:150]\npride_noun_vecs = [tok.vector for tok in pride_nouns]\npride_noun_labels = [tok.text for tok in pride_nouns]\n\n# Reduce from 300 to 2 dimensions using SVD\nfrom sklearn.decomposition import TruncatedSVD\nlsa = TruncatedSVD(n_components=2)\nnoun_2d = lsa.fit_transform(pride_noun_vecs)\n\n# Plot\nplt.figure(figsize=(12, 8))\nfor i, label in enumerate(pride_noun_labels):\n    x, y = noun_2d[i]\n    plt.scatter(x, y, alpha=0.6)\n    plt.annotate(label, (x, y), fontsize=8, alpha=0.7)\n\nplt.title('Semantic Space of Nouns in Pride and Prejudice (Pre-trained Embeddings)')\nplt.xlabel('Component 1')\nplt.ylabel('Component 2')\nplt.show()\n\n\n\n\n\n\n\n\nWhat this shows: Semantic neighborhoods from GloVe, not Austen-specific usage. Character names, family terms, and social concepts may cluster based on general semantic similarity, not narrative relationships.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>NLP Processing with spaCy</span>"
    ]
  },
  {
    "objectID": "tutorials/spacy-basics.html#document-level-analysis",
    "href": "tutorials/spacy-basics.html#document-level-analysis",
    "title": "3Â  NLP Processing with spaCy",
    "section": "3.10 Document-Level Analysis",
    "text": "3.10 Document-Level Analysis\n\n3.10.1 Document Similarity\nspaCy can compute similarity between entire documents using averaged word vectors:\n\n# Load a few inaugural speeches\nimport nltk\nnltk.download('inaugural', quiet=True)\nfrom nltk.corpus import inaugural\n\n# Get two speeches\nobama_2009 = nlp(inaugural.raw('2009-Obama.txt'))\ntrump_2017 = nlp(inaugural.raw('2017-Trump.txt'))\nreagan_1981 = nlp(inaugural.raw('1981-Reagan.txt'))\n\nprint(f\"Obama 2009 â†” Trump 2017: {obama_2009.similarity(trump_2017):.3f}\")\nprint(f\"Obama 2009 â†” Reagan 1981: {obama_2009.similarity(reagan_1981):.3f}\")\nprint(f\"Trump 2017 â†” Reagan 1981: {trump_2017.similarity(reagan_1981):.3f}\")\n\nObama 2009 â†” Trump 2017: 0.996\nObama 2009 â†” Reagan 1981: 0.999\nTrump 2017 â†” Reagan 1981: 0.997\n\n\nInterpretation: Higher scores = more semantically similar vocabulary and themes. But similarity doesnâ€™t capture tone, political ideology, or rhetorical strategyâ€”only word choice.\n\n\n3.10.2 TF-IDF Vectorization\nFor more nuanced document comparison, use TF-IDF (term frequency-inverse document frequency) to weight words by distinctiveness:\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Prepare inaugural speeches from 20th century onward\ninaugural_files = [f for f in inaugural.fileids() if int(f[:4]) &gt;= 1900]\ninaugural_texts = [inaugural.raw(f) for f in inaugural_files]\ninaugural_labels = [f[:-4] for f in inaugural_files]\n\n# Vectorize\ntfidf = TfidfVectorizer(stop_words='english', max_features=1000)\ntfidf_matrix = tfidf.fit_transform(inaugural_texts)\n\n# Reduce dimensions for visualization\nlsa = TruncatedSVD(n_components=2)\ndoc_2d = lsa.fit_transform(tfidf_matrix.toarray())\n\n# Plot\nplt.figure(figsize=(12, 8))\nfor i, label in enumerate(inaugural_labels):\n    x, y = doc_2d[i]\n    plt.scatter(x, y, s=100, alpha=0.6)\n    plt.annotate(label, (x, y), fontsize=8)\n\nplt.title('Inaugural Addresses Semantic Space (TF-IDF + LSA)')\nplt.xlabel('Component 1')\nplt.ylabel('Component 2')\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation: Documents that cluster together use similar distinctive vocabulary. Time periods, political parties, or thematic concerns might drive clustering.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>NLP Processing with spaCy</span>"
    ]
  },
  {
    "objectID": "tutorials/spacy-basics.html#choosing-the-right-tool-for-your-question",
    "href": "tutorials/spacy-basics.html#choosing-the-right-tool-for-your-question",
    "title": "3Â  NLP Processing with spaCy",
    "section": "3.11 Choosing the Right Tool for Your Question",
    "text": "3.11 Choosing the Right Tool for Your Question\nNot every research question requires the full spaCy pipeline. Hereâ€™s how to decide which components you need:\n\n\n\n\n\n\n\n\nResearch Question\nspaCy Features to Use\nWhy These Features?\n\n\n\n\nTrack character mentions across chapters\nNER (entities)\nAutomatically identifies person names\n\n\nCompare sentence complexity over time\nTokenization, sentence segmentation\nMeasures syntactic structure\n\n\nFind verb-object patterns (e.g., â€œseek justiceâ€)\nDependency parsing\nReveals grammatical relationships\n\n\nIdentify which texts discuss similar themes\nWord embeddings, TF-IDF\nCaptures semantic similarity\n\n\nAnalyze pronoun usage (â€œIâ€ vs.Â â€œweâ€ vs.Â â€œyouâ€)\nPOS tagging\nDistinguishes grammatical categories\n\n\nMap place names in travel narratives\nNER (GPE entities)\nExtracts location references\n\n\nMeasure lexical diversity\nTokenization, lemmatization\nCounts unique word forms\n\n\nCompare dialogue vs.Â narration\nCustom sentence filtering\nRequires preprocessing choices\n\n\n\nGeneral principle: Start simple. Use basic tokenization and frequencies before moving to complex parsing. Add pipeline components only when they answer questions your current tools canâ€™t.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>NLP Processing with spaCy</span>"
    ]
  },
  {
    "objectID": "tutorials/spacy-basics.html#when-to-use-nlp-pipelines",
    "href": "tutorials/spacy-basics.html#when-to-use-nlp-pipelines",
    "title": "3Â  NLP Processing with spaCy",
    "section": "3.12 When to Use NLP Pipelines",
    "text": "3.12 When to Use NLP Pipelines\n\n\n\n\n\n\nWell-Suited Research Questions\n\n\n\n\nSyntactic analysis: Comparing sentence structures, finding grammatical patterns\nEntity extraction: Tracking characters, places, organizations across texts\nDependency-based collocations: Finding verb-object pairs, adjective-noun pairs\nStylometric comparison: Sentence length, POS distributions, lexical diversity\nLarge-scale processing: Annotating thousands of texts consistently\n\n\n\n\n\n\n\n\n\nLimitations and Alternatives\n\n\n\nWhen pipelines struggle:\n\nHistorical texts: Pre-1900 English differs enough to cause POS and NER errors\nCreative language: Poetry, experimental fiction, and non-standard syntax confuse parsers\nDomain-specific terminology: Medical, legal, technical texts need specialized models\nWord sense disambiguation: Fixed embeddings canâ€™t distinguish polysemy\n\nAlternative approaches:\n\nManual annotation: For small corpora, human coding may be more accurate\nRule-based extraction: Regular expressions for highly structured patterns\nCustom model training: Train spaCy on domain-specific data\nContextual embeddings: Use transformer models (BERT, GPT) for sense-aware vectors",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>NLP Processing with spaCy</span>"
    ]
  },
  {
    "objectID": "tutorials/spacy-basics.html#common-pitfalls",
    "href": "tutorials/spacy-basics.html#common-pitfalls",
    "title": "3Â  NLP Processing with spaCy",
    "section": "3.13 Common Pitfalls",
    "text": "3.13 Common Pitfalls\n1. Trusting NER blindly\nAlways inspect entity extractions. NER models make systematic errors on non-standard text. A character named â€œEnglandâ€ in a fantasy novel might be tagged as GPE (place).\n2. Ignoring preprocessing choices\nRemoving stage directions, character names, or chapter headings changes results. Document what you remove and why.\n3. Conflating description with interpretation\nFinding that â€œDarcyâ€ co-occurs with â€œproudâ€ is a fact. Claiming this reflects his character arc requires close reading of context.\n4. Overinterpreting embeddings\nPre-trained vectors show general semantic neighborhoods, not text-specific meanings. Donâ€™t claim â€œloveâ€ and â€œmarriageâ€ are thematically linked in Pride and Prejudice based on GloVe similarityâ€”check their actual collocational patterns.\n5. Scalability assumptions\nProcessing one novel takes 30 seconds. Processing 10,000 novels takes hours or days. Plan compute resources accordingly.\n6. Ignoring model limitations\nspaCy is trained on modern web text. Apply to historical corpora with caution. Check accuracy on a sample before processing thousands of documents.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>NLP Processing with spaCy</span>"
    ]
  },
  {
    "objectID": "tutorials/spacy-basics.html#what-to-do-after-processing",
    "href": "tutorials/spacy-basics.html#what-to-do-after-processing",
    "title": "3Â  NLP Processing with spaCy",
    "section": "3.14 What to Do After Processing",
    "text": "3.14 What to Do After Processing\nspaCy extracts featuresâ€”what you do with them depends on your discipline and research questions.\n\n3.14.1 Connect to Literary Analysis\nCharacter networks: Use dependency parsing to extract character-verb relationships, then analyze who performs which actions. Does agency correlate with narrative arc?\nNarrative structure: Plot word distributions across text positions (Mini Lab 8â€™s narrative time analysis). Do clusters reveal plot structure?\nFocalization: Track whose perspective dominates via subject-verb patterns. Whose verbs of perception (â€œsaw,â€ â€œheard,â€ â€œfeltâ€) appear most?\n\n\n3.14.2 Connect to Linguistic Analysis\nRegister variation: Compare POS distributions between dialogue and narration. Do characters speak differently than the narrator describes?\nDiachronic change: Process texts across decades. How do sentence lengths, pronoun ratios, or entity types shift?\nGenre markers: Use TF-IDF to identify distinctive vocabulary. What words characterize detective fiction vs.Â romance?\n\n\n3.14.3 Connect to Distant Reading\nClustering: Group documents by similarity (embeddings or TF-IDF). Do clusters align with known genres, time periods, or authors?\nOutlier detection: Which texts have unusual POS distributions, sentence lengths, or entity densities? What makes them anomalous?\nHypothesis generation: Let computational patterns suggest close reading targets. Why does â€œprideâ€ cluster early while â€œloveâ€ clusters late?\n\n\n3.14.4 Methodological Triangulation\n\nStart computational: Use spaCy to find patterns across large corpora\nZoom to close reading: Examine actual sentences where patterns appear\nContextualize historically: Check if patterns align with known historical events, genre conventions, or authorial biography\nIterate: Let close reading refine computational categories; let computation test interpretive hunches\n\nRemember: spaCy is infrastructure. The humanities questionsâ€”why does this pattern matter? what does it reveal about culture, power, or aesthetics?â€”require domain expertise, not just code.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>NLP Processing with spaCy</span>"
    ]
  },
  {
    "objectID": "tutorials/spacy-basics.html#conclusion",
    "href": "tutorials/spacy-basics.html#conclusion",
    "title": "3Â  NLP Processing with spaCy",
    "section": "3.15 Conclusion",
    "text": "3.15 Conclusion\nNLP pipelines transform raw text into structured, analyzable data. spaCy automates the boring parts (tokenization, POS tagging, parsing) so you can focus on the interesting parts (patterns, interpretations, arguments).\nThe workflow:\n\nPreprocess thoughtfully (clean text, but document what you remove)\nProcess with pipelines (let spaCy handle linguistic annotation)\nExtract features (tokens, entities, dependencies, embeddings)\nAnalyze patterns (frequencies, distributions, relationships)\nInterpret critically (connect computational patterns to domain knowledge)\nValidate qualitatively (read actual examples, check errors)\n\nRemember: pipelines provide infrastructure, not answers. They standardize preprocessing and feature extraction. What you build on that foundationâ€”whether stylometric comparison, character network analysis, or historical linguistic changeâ€”depends on your research questions and disciplinary expertise.\n\n\n\n\n\n\nConnecting to Mini Lab 8\n\n\n\nMini Lab 8: All Things spaCy provides extensive hands-on practice with all the techniques covered here, plus advanced examples like character-verb extraction, narrative time correlation matrices, and document clustering. The mini lab uses Pride and Prejudice, Monty Python, and the inaugural corpus to demonstrate practical workflows.\nAttribution: Mini Lab 8 is adapted from materials by Jonathan Reeve (Group for Experimental Methods in Humanities, Columbia University), shared under the MIT License.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>NLP Processing with spaCy</span>"
    ]
  },
  {
    "objectID": "tutorials/spacy-basics.html#see-also",
    "href": "tutorials/spacy-basics.html#see-also",
    "title": "3Â  NLP Processing with spaCy",
    "section": "3.16 See Also",
    "text": "3.16 See Also\nspaCy Documentation: spacy.io â€” Official guides, API reference, model documentation\nTextacy: A higher-level library built on spaCy for text analysis workflows, adding corpus management and feature extraction (textacy.readthedocs.io)\nTutorial: NLP for Literary Text Analysis: Jonathan Reeveâ€™s materials at github.com/JonathanReeve/advanced-text-analysis-workshop-2017\nContextual Embeddings: For sense-aware word vectors, explore Hugging Faceâ€™s transformer models (huggingface.co/models)\nRelated Mini Labs: - Mini Lab 6: Collocations â€” Frequency-based word associations - Mini Lab 9: Topic Modeling â€” Unsupervised theme discovery - Mini Lab 10: Multidimensional Analysis â€” Register variation",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>NLP Processing with spaCy</span>"
    ]
  },
  {
    "objectID": "tutorials/spacy-basics.html#works-cited",
    "href": "tutorials/spacy-basics.html#works-cited",
    "title": "3Â  NLP Processing with spaCy",
    "section": "3.17 Works Cited",
    "text": "3.17 Works Cited",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>NLP Processing with spaCy</span>"
    ]
  },
  {
    "objectID": "tutorials/frequency-and-distributions.html",
    "href": "tutorials/frequency-and-distributions.html",
    "title": "4Â  Distributions",
    "section": "",
    "text": "4.1 Prepare a corpus",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "tutorials/frequency-and-distributions.html#prepare-a-corpus",
    "href": "tutorials/frequency-and-distributions.html#prepare-a-corpus",
    "title": "4Â  Distributions",
    "section": "",
    "text": "4.1.1 Load the needed packages\n\nimport polars as pl\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom great_tables import GT\nimport docuscospacy.corpus_analysis as ds\nimport spacy\n\n\n\n4.1.2 Load a corpus\nThe repository comes with data sets for practice. The conventional way to format text data prior to processing is as a table with a column of document ids (which correspond to file names) and a column of texts.\nWeâ€™ll load a sample corpus from a URL:\n\nurl = \"https://github.com/browndw/humanities_analytics/raw/refs/heads/main/data/data_tables/sample_corpus.parquet\"\nsample_corpus = pl.read_parquet(url)\n\nTo peek at the data, weâ€™ll look at the first 100 characters in the â€œtextâ€ column:\n\n\nCode\nsample_corpus.with_columns(\n    pl.col(\"text\").str.slice(0, 100)\n).head(10)\n\n\n\nshape: (10, 2)\n\n\n\ndoc_id\ntext\n\n\nstr\nstr\n\n\n\n\n\"acad_01\"\n\"Teachers and other school persâ€¦\n\n\n\"acad_02\"\n\"Abstract Does the conflict in â€¦\n\n\n\"acad_03\"\n\"January 17, 1993, will mark thâ€¦\n\n\n\"acad_04\"\n\"Thirty years have passed sinceâ€¦\n\n\n\"acad_05\"\n\"ABSTRACT -- A common property â€¦\n\n\n\"acad_06\"\n\"Despite some encouraging signsâ€¦\n\n\n\"acad_07\"\n\"evaluation component. Similarlâ€¦\n\n\n\"acad_08\"\n\"Section: Education \"A lab is wâ€¦\n\n\n\"acad_09\"\n\"In 1968, the thirtieth anniverâ€¦\n\n\n\"acad_10\"\n\"monologue -- and of Novas Calvâ€¦\n\n\n\n\n\n\n\n\n4.1.3 Load the spaCy model\nWeâ€™ll use the DocuScope-tagged spaCy model for tokenization and tagging:\n\nnlp = spacy.load(\"en_docusco_spacy\")\n\nAnd process the corpus:\n\nds_tokens = ds.docuscope_parse(sample_corpus, nlp)\n\nPerformance: Corpus processing completed in 68.78s\n\n\nCheck the result:\n\n\nCode\nds_tokens.head()\n\n\n\nshape: (5, 6)\n\n\n\ndoc_id\ntoken\npos_tag\nds_tag\npos_id\nds_id\n\n\nstr\nstr\nstr\nstr\nu32\nu32\n\n\n\n\n\"acad_01\"\n\"Teachers \"\n\"NN2\"\n\"Character\"\n1\n1\n\n\n\"acad_01\"\n\"and \"\n\"CC\"\n\"Untagged\"\n2\n2\n\n\n\"acad_01\"\n\"other \"\n\"JJ\"\n\"InformationTopics\"\n3\n3\n\n\n\"acad_01\"\n\"school \"\n\"NN1\"\n\"InformationTopics\"\n4\n3\n\n\n\"acad_01\"\n\"personnel \"\n\"NN2\"\n\"Character\"\n5\n4",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "tutorials/frequency-and-distributions.html#document-variables-name-your-files-systematically",
    "href": "tutorials/frequency-and-distributions.html#document-variables-name-your-files-systematically",
    "title": "4Â  Distributions",
    "section": "4.2 Document variables (Name your files systematically!)",
    "text": "4.2 Document variables (Name your files systematically!)\n\n\n\n\n\n\nImportant\n\n\n\nFile names can encode important meta-data. In this case, the names include text-types, much like the Corpus of Contemporary American English.\nThis is extremely important. When you build your own corpora, you want to purposefully and systematically name your files and organize your directories. This will save you time and effort later in your analysis.\n\n\nWe can extract the meta-data from the file names using polars string operations. Notice how the doc_id values start with prefixes like â€œacadâ€ or â€œficâ€. We can extract those:\n\nsample_corpus.select(\n    pl.col(\"doc_id\").str.extract(r\"^([a-z]+)\", 1).alias(\"text_type\")\n).unique()\n\n\nshape: (8, 1)\n\n\n\ntext_type\n\n\nstr\n\n\n\n\n\"spok\"\n\n\n\"fic\"\n\n\n\"acad\"\n\n\n\"web\"\n\n\n\"news\"\n\n\n\"tvm\"\n\n\n\"mag\"\n\n\n\"blog\"\n\n\n\n\n\n\nThis meta-data is already embedded in our tokens table via the doc_id column, so we can use it for grouping and filtering in our analyses.\n\n\n\n\n\n\nWhy This Matters\n\n\n\nFrequency analysis is foundational to corpus linguistics and computational text analysis. Understanding word distributions helps us:\n\nIdentify patterns: What words characterize a text type or author?\nMake comparisons: How does academic writing differ from fiction?\nDetect anomalies: Are certain words unusually common or rare?\nInform decisions: Should we remove stopwords? Weight by frequency?\n\nBut frequency alone can be misleadingâ€”a word might be common because it appears many times in one document, or because it appears once in every document. This is why we need dispersion measures alongside frequency counts.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "tutorials/frequency-and-distributions.html#create-a-frequency-table",
    "href": "tutorials/frequency-and-distributions.html#create-a-frequency-table",
    "title": "4Â  Distributions",
    "section": "4.3 Create a frequency table",
    "text": "4.3 Create a frequency table\nThe frequency_table() function creates a comprehensive summary of token frequencies across our corpus:\n\nwc = ds.frequency_table(ds_tokens)\n\nCheck the result:\n\n\nCode\nwc.head(10)\n\n\n\nshape: (10, 5)\n\n\n\nToken\nTag\nAF\nRF\nRange\n\n\nstr\nstr\nu32\nf64\nf64\n\n\n\n\n\"the\"\n\"AT\"\n51032\n49493.494717\n100.0\n\n\n\"and\"\n\"CC\"\n25285\n24522.711513\n100.0\n\n\n\"of\"\n\"IO\"\n22524\n21844.949737\n100.0\n\n\n\"a\"\n\"AT1\"\n21998\n21334.807509\n100.0\n\n\n\"to\"\n\"TO\"\n16514\n16016.13834\n100.0\n\n\n\"in\"\n\"II\"\n15694\n15220.85958\n100.0\n\n\n\"i\"\n\"PPIS1\"\n15431\n14965.788466\n100.0\n\n\n\"it\"\n\"PPH1\"\n12860\n12472.298598\n100.0\n\n\n\"you\"\n\"PPY\"\n12448\n12072.719514\n100.0\n\n\n\"is\"\n\"VBZ\"\n10565\n10246.487923\n100.0\n\n\n\n\n\n\nThe frequency table contains columns for:\n\nToken: The word or token\nAF: Absolute frequency (total count across all documents)\nRF: Relative frequency (per million tokens)\nRange: Number of documents containing the token\nDP: Deviation of Proportions (dispersion measure)",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "tutorials/frequency-and-distributions.html#some-properties-of-token-frequencies",
    "href": "tutorials/frequency-and-distributions.html#some-properties-of-token-frequencies",
    "title": "4Â  Distributions",
    "section": "4.4 Some properties of token frequencies",
    "text": "4.4 Some properties of token frequencies\nJust glancing at the top of the frequency table, a couple of things are obvious. The first is that on their own, frequencies arenâ€™t obviously interpretable. The most frequent words in most English language corpora will look much like this oneâ€”dominated by function words like the, of, and and.\n\n4.4.1 Zipfâ€™s Law\nAnother fundamental property of word (or token) frequencies is called Zipfâ€™s Law. To get a sense of what it is and its implications, letâ€™s create a simple plot.\nWeâ€™ll add a row index (which weâ€™ll call rank) and take the first 250 rows:\n\nplot_df = wc.with_row_index(\"rank\", offset=1).head(250)\n\nAnd we can use matplotlib to create a simple scatter plot of rank vs.Â absolute frequency:\n\n\nCode\nx = plot_df.select(\"rank\").to_numpy()\ny = plot_df.select(\"AF\").to_numpy()\n\nplt.figure(figsize=(8, 5))\nplt.scatter(x, y, alpha=0.6)\nplt.xlabel('Token rank')\nplt.ylabel('Token frequency')\nplt.title(\"Zipf's Law\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe steep power-law curve youâ€™re seeingâ€”where a small number of tokens account for the vast majority of occurrencesâ€”holds true for almost any corpus. This is Zipfâ€™s Law: the frequency of any word is inversely proportional to its rank in the frequency table. The most common word appears roughly twice as often as the second most common word, three times as often as the third most common word, and so on.\n\n\n\n\n\n\nWhy Zipfâ€™s Law Matters\n\n\n\nThis distribution has profound implications for text analysis:\n\nCommon tokens dominate: The top 100 words often account for 50%+ of all tokens\nLong tail of rare words: Most unique words appear only a few times\nStatistical challenges: Many standard statistical methods assume normal distributions, not power laws\nSampling concerns: Even large corpora may have insufficient data for rare words\n\nUnderstanding this distribution helps us choose appropriate analysis methods and interpret results correctly.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "tutorials/frequency-and-distributions.html#dispersions",
    "href": "tutorials/frequency-and-distributions.html#dispersions",
    "title": "4Â  Distributions",
    "section": "4.5 Dispersions",
    "text": "4.5 Dispersions\nOne fundamental question we might have when looking at token frequencies is: what is driving the frequency of a given token?\nFor example, letâ€™s say we find that raccoon is particularly frequent in our data. Does that mean we have a particularly raccoon-heavy set of texts? Or is it that 1 or 2 texts are all about raccoons while the rest never mention them?\nFor this reason, we might want to know how dispersed tokens are. One simple way to report this is to see what percentage of our texts a given token appears in.\nThe limitation to this approach is that our corpus may not contain texts of equal length. If some are shorter than others, the opportunity for a token to appear will not be equal.\nThus there are a variety of dispersion measures available. Some of these can be calculated using the dispersions_table() function. For this, we simply pass our original tokens object to the function:\n\ndt = ds.dispersions_table(ds_tokens)\n\nNote that for some dispersion measures, the higher value means that the token is more dispersed:\n\n\nCode\ndt.head(10)\n\n\n\nshape: (10, 11)\n\n\n\nToken\nTag\nAF\nRF\nCarrolls_D2\nâ€¦\nLynes_D3\nDC\nJuillands_D\nDP\nDP_norm\n\n\nstr\nstr\nu64\nf64\nf64\nâ€¦\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"the\"\n\"AT\"\n51032\n49493.494717\n0.988606\nâ€¦\n0.967496\n0.962878\n0.981737\n0.143405\n0.143702\n\n\n\"and\"\n\"CC\"\n25285\n24522.711513\n0.990274\nâ€¦\n0.973234\n0.967056\n0.983422\n0.125533\n0.125794\n\n\n\"of\"\n\"IO\"\n22524\n21844.949737\n0.979403\nâ€¦\n0.938009\n0.9353\n0.974667\n0.197503\n0.197913\n\n\n\"a\"\n\"AT1\"\n21998\n21334.807509\n0.992072\nâ€¦\n0.978803\n0.97165\n0.985503\n0.111006\n0.111236\n\n\n\"to\"\n\"TO\"\n16514\n16016.13834\n0.989978\nâ€¦\n0.973467\n0.963931\n0.983837\n0.122953\n0.123208\n\n\n\"in\"\n\"II\"\n15694\n15220.85958\n0.986626\nâ€¦\n0.960228\n0.957547\n0.979665\n0.155209\n0.155531\n\n\n\"i\"\n\"PPIS1\"\n15431\n14965.788466\n0.908625\nâ€¦\n0.731843\n0.677248\n0.94832\n0.428251\n0.42914\n\n\n\"it\"\n\"PPH1\"\n12860\n12472.298598\n0.976747\nâ€¦\n0.936577\n0.918503\n0.974995\n0.196682\n0.19709\n\n\n\"you\"\n\"PPY\"\n12448\n12072.719514\n0.897063\nâ€¦\n0.659539\n0.654258\n0.941817\n0.450321\n0.451255\n\n\n\"is\"\n\"VBZ\"\n10565\n10246.487923\n0.969252\nâ€¦\n0.914525\n0.891143\n0.97051\n0.229789\n0.230266\n\n\n\n\n\n\nThe most commonly used dispersion measure is Deviation of Proportions (DP), which ranges from 0 to 1:\n\nDP = 0: Token appears in perfectly equal proportions across all documents\nDP = 1: Token appears in only one document\nHigher DP = more uneven distribution\n\nLetâ€™s compare the DP values for a couple of tokens:\n\ndt.filter(pl.col(\"Token\").is_in([\"the\", \"data\", \"school\"])).select([\"Token\", \"DP\"]).head(3)\n\n\nshape: (3, 2)\n\n\n\nToken\nDP\n\n\nstr\nf64\n\n\n\n\n\"the\"\n0.143405\n\n\n\"school\"\n0.670521\n\n\n\"data\"\n0.844023\n\n\n\n\n\n\nNotice how the has very low DP (close to 0), meaning itâ€™s evenly distributed across documentsâ€”itâ€™s a true high-frequency word. Compare this to data or school, which may have higher DP values if they cluster in academic texts.\n\n\n\n\n\n\nWhen Dispersion Matters\n\n\n\nImagine comparing two corpora: one about climate science and one about cooking. The word heat might appear frequently in both. But in the climate corpus, it might appear in 90% of documents (high dispersion, DP â‰ˆ 0.2), while in the cooking corpus it might appear in only 20% of documents (low dispersion, DP â‰ˆ 0.7)â€”all the recipes for baking.\nThis tells us something important: heat is a core term in climate discourse but a specialized term in cooking discourse. Frequency alone wouldnâ€™t reveal this distinction.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "tutorials/frequency-and-distributions.html#create-a-document-feature-matrix",
    "href": "tutorials/frequency-and-distributions.html#create-a-document-feature-matrix",
    "title": "4Â  Distributions",
    "section": "4.6 Create a document-feature matrix",
    "text": "4.6 Create a document-feature matrix\nSo far weâ€™ve looked at individual token frequencies and dispersions. But many text analysis tasks require us to represent entire documents numericallyâ€”for clustering, classification, or statistical comparison. This is where document-feature matrices (DFMs) come in.\nA DFM converts each document into a vector of numbers, where each column represents a feature (like a word or tag) and each cell contains a count. This representation enables mathematical operations on texts.\nWith our tokens object we can create a DFM using the tags_dtm() function. In this case, our features are POS (part-of-speech) tags rather than individual tokens. Each cell contains a count of how many times that tag appears in that document:\n\ndfm = ds.tags_dtm(ds_tokens)\n\nCheck the result:\n\n\nCode\ndfm.head()\n\n\n\nshape: (5, 178)\n\n\n\ndoc_id\nNN1\nJJ\nII\nAT\nâ€¦\nRT21\nNN121\nNP\nRGQV32\nRR41\n\n\nstr\nu32\nu32\nu32\nu32\nâ€¦\nu32\nu32\nu32\nu32\nu32\n\n\n\n\n\"acad_01\"\n451\n279\n218\n182\nâ€¦\n0\n0\n0\n0\n0\n\n\n\"acad_02\"\n432\n276\n208\n279\nâ€¦\n0\n0\n0\n0\n0\n\n\n\"acad_03\"\n422\n187\n167\n234\nâ€¦\n0\n0\n0\n0\n0\n\n\n\"acad_04\"\n469\n270\n161\n206\nâ€¦\n0\n1\n0\n0\n0\n\n\n\"acad_05\"\n445\n201\n169\n180\nâ€¦\n0\n0\n0\n0\n0\n\n\n\n\n\n\nThe DFM has a column for doc_id and then columns for every POS tag (NN1 = singular noun, JJ = adjective, etc.).\n\n4.6.1 Using document-feature matrices\nWe can use the DFM to calculate document-level statistics. For example, we can get the total word counts for each document by summing across all tag columns:\n\n\nCode\n# Get total word counts per document\ndoc_totals = dfm.with_columns(\n    pl.sum_horizontal(pl.exclude(\"doc_id\")).alias(\"total\")\n).select(\"doc_id\", \"total\")\n\ndoc_totals.head(10)\n\n\n\nshape: (10, 2)\n\n\n\ndoc_id\ntotal\n\n\nstr\nu32\n\n\n\n\n\"acad_01\"\n2533\n\n\n\"acad_02\"\n2558\n\n\n\"acad_03\"\n2531\n\n\n\"acad_04\"\n2542\n\n\n\"acad_05\"\n2505\n\n\n\"acad_06\"\n2580\n\n\n\"acad_07\"\n2444\n\n\n\"acad_08\"\n2538\n\n\n\"acad_09\"\n2554\n\n\n\"acad_10\"\n2538\n\n\n\n\n\n\n\n\n4.6.2 Aggregating by text type\nWe can use the text-type information embedded in our doc_id values to compare totals across different kinds of texts:\n\n# Extract text type and calculate totals\ntotals = (\n    doc_totals\n    .with_columns(\n        pl.col(\"doc_id\").str.extract(r\"^([a-z]+)\", 1).alias(\"text_type\")\n    )\n    .group_by(\"text_type\")\n    .agg(pl.col(\"total\").sum())\n    .sort(\"text_type\")\n)\n\nWe can format this as a table using great_tables:\n\n\nCode\nGT(totals)\n\n\n\n\n\n\n\n\ntext_type\ntotal\n\n\n\n\nacad\n126315\n\n\nblog\n128556\n\n\nfic\n130292\n\n\nmag\n129905\n\n\nnews\n128058\n\n\nspok\n129866\n\n\ntvm\n130132\n\n\nweb\n127961",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "tutorials/frequency-and-distributions.html#common-pitfalls",
    "href": "tutorials/frequency-and-distributions.html#common-pitfalls",
    "title": "4Â  Distributions",
    "section": "4.7 Common Pitfalls",
    "text": "4.7 Common Pitfalls\n\n\n\n\n\n\nWatch Out For\n\n\n\n\nRare Word Illusion: High-frequency rare words may appear in only one or two documents. Always check dispersion alongside frequency.\nNormalization Matters: Comparing raw frequencies across corpora of different sizes is misleading. Always normalize (e.g., per million words).\nFunction Word Dominance: The most frequent tokens are rarely the most interesting. Consider filtering or focusing on content words.\nText Length Variation: If your corpus has documents of very different lengths, some analyses may be biased toward longer documents.\nCase Sensitivity: Remember that tokenization decisions (lowercase vs.Â mixed case) affect frequency counts. Be consistent and document your choices.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "tutorials/frequency-and-distributions.html#discussion-questions",
    "href": "tutorials/frequency-and-distributions.html#discussion-questions",
    "title": "4Â  Distributions",
    "section": "4.8 Discussion Questions",
    "text": "4.8 Discussion Questions\n\nInterpreting Zipfâ€™s Law: What does the power-law distribution of token frequencies suggest about language use? How might this pattern differ in specialized vs.Â general corpora?\nFrequency vs.Â Importance: The most frequent words are function words (articles, prepositions). What does this tell us about using frequency as a proxy for â€œimportanceâ€? When might frequency be misleading?\nDispersion and Interpretation: Why might a moderately frequent word with high dispersion be more interesting than a high-frequency word with low dispersion? Think of examples from your research area.\nDocument Length Effects: How might varying document lengths in your corpus affect frequency and dispersion measures? What steps could you take to account for this?\nBeyond Single Tokens: This tutorial focuses on individual word frequencies. What might we miss by not considering multi-word expressions, collocations, or n-grams?\nCritical Reflection: Frequency tables reduce texts to counts, discarding context, order, and meaning. What are the trade-offs of this reductionist approach? When is it appropriate, and when might it be problematic?",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "tutorials/frequency-and-distributions.html#key-takeaways",
    "href": "tutorials/frequency-and-distributions.html#key-takeaways",
    "title": "4Â  Distributions",
    "section": "4.9 Key Takeaways",
    "text": "4.9 Key Takeaways\n\nFrequency tables provide a foundation for corpus analysis, showing how often tokens appear\nZipfâ€™s Law describes the power-law distribution where a few tokens dominate and most are rare\nDispersion measures like DP help distinguish between tokens that are evenly distributed vs.Â clustered in a few documents\nDocument-feature matrices convert texts into numerical representations for further analysis\nNormalization (e.g., to per-million-word rates) is essential for comparing corpora of different sizes\nContext matters: Raw frequency counts should always be interpreted in light of corpus composition, text length variation, and research questions",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "tutorials/frequency-and-distributions.html#further-reading",
    "href": "tutorials/frequency-and-distributions.html#further-reading",
    "title": "4Â  Distributions",
    "section": "4.10 Further Reading",
    "text": "4.10 Further Reading\n\nBrezina, V. (2018). Statistics in Corpus Linguistics: A Practical Guide. Chapter 3: Frequency.\nGries, S. Th. (2008). â€œDispersions and adjusted frequencies in corpora.â€ International Journal of Corpus Linguistics 13(4): 403-437.\nZipf, G. K. (1935). The Psycho-Biology of Language. Houghton, Mifflin.\n\n\n\n\n\n\n\n\nReady to Practice?\n\n\n\nHead to Mini Lab 03: Frequencies to apply these concepts hands-on. Youâ€™ll work with the same corpus and functions, but with opportunities to explore different tokens, create visualizations, and experiment with filtering and aggregation techniques.\nThe mini lab includes experimentation prompts to help you develop intuitions about when frequency and dispersion measures are most useful in text analysis.",
    "crumbs": [
      "Foundations",
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Distributions</span>"
    ]
  },
  {
    "objectID": "tutorials/keyness.html",
    "href": "tutorials/keyness.html",
    "title": "5Â  Keyness",
    "section": "",
    "text": "5.1 What is keyness?\nKeyness is a generic term for various statistical tests that compare observed vs.Â expected frequencies in text corpora.\nThe most commonly used measure (though not the only option) is called log-likelihood in corpus linguistics, but you will see it elsewhere called a G-test of goodness-of-fit.\nThe calculation is based on a 2 Ã— 2 contingency table. It is similar to a chi-square test, but performs better when corpora are unequally sizedâ€”which is almost always the case in real research.",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Keyness</span>"
    ]
  },
  {
    "objectID": "tutorials/keyness.html#what-is-keyness",
    "href": "tutorials/keyness.html#what-is-keyness",
    "title": "5Â  Keyness",
    "section": "",
    "text": "Why Keyness Matters\n\n\n\nKeyness helps us answer a fundamental question in corpus linguistics: What makes this corpus distinctive?\n\nComparing genres: What words characterize academic writing vs.Â fiction?\nAuthor attribution: What lexical choices distinguish Author A from Author B?\nTemporal change: How has language use shifted over time?\nRegister analysis: What features mark formal vs.Â informal language?\n\nKeyness provides statistical evidence for claims like â€œacademic writing uses more passive voiceâ€ or â€œthis author prefers certain vocabulary.â€\n\n\n\n5.1.1 The mathematics\nExpected frequencies are based on the relative size of each corpus (in total number of words Ni) and the total number of observed frequencies:\n\\[\nE_i = \\sum_i O_i \\times \\frac{N_i}{\\sum_i N_i}\n\\]\nAnd log-likelihood is calculated according to the formula:\n\\[\nLL = 2 \\times \\sum_i O_i \\ln \\frac{O_i}{E_i}\n\\]\nA good explanation of its implementation in linguistics can be found here: http://ucrel.lancs.ac.uk/llwizard.html\nYou donâ€™t need to worry about the math, but you should understand what is happening conceptually and what the results show:\n\nKeyness measures the frequency we observe in a target corpus vs.Â what we would expect if our target corpus and our reference corpus were part of the same distribution. In other words: pool both corpora together and calculate expected frequencies based on overall proportions.\n\nImportantly, keyness measures how much evidence we have for an effect. It doesnâ€™t make much sense to claim, for example, that one token is â€œmore keyâ€ than another based solely on LL values.",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Keyness</span>"
    ]
  },
  {
    "objectID": "tutorials/keyness.html#prepare-a-corpus",
    "href": "tutorials/keyness.html#prepare-a-corpus",
    "title": "5Â  Keyness",
    "section": "5.2 Prepare a corpus",
    "text": "5.2 Prepare a corpus\n\n5.2.1 Load the needed packages\n\nimport docuscospacy.corpus_analysis as ds\nimport polars as pl\nimport spacy\nfrom great_tables import GT\nimport matplotlib.pyplot as plt\n\n\n\n5.2.2 Load the corpus\n\nurl = \"https://github.com/browndw/humanities_analytics/raw/refs/heads/main/data/data_tables/sample_corpus.parquet\"\ndf = pl.read_parquet(url)\n\nLetâ€™s peek at the data:\n\n\nCode\ndf.head()\n\n\n\nshape: (5, 2)\n\n\n\ndoc_id\ntext\n\n\nstr\nstr\n\n\n\n\n\"acad_01\"\n\"Teachers and other school persâ€¦\n\n\n\"acad_02\"\n\"Abstract Does the conflict in â€¦\n\n\n\"acad_03\"\n\"January 17, 1993, will mark thâ€¦\n\n\n\"acad_04\"\n\"Thirty years have passed sinceâ€¦\n\n\n\"acad_05\"\n\"ABSTRACT -- A common property â€¦\n\n\n\n\n\n\nFor this tutorial, itâ€™s useful to note what text categories are encoded in the doc_id:\n\ndf.get_column(\"doc_id\").str.extract(r\"^([a-z]+)\", 1).unique().sort().to_list()\n\n['acad', 'blog', 'fic', 'mag', 'news', 'spok', 'tvm', 'web']\n\n\n\n\n5.2.3 Process the corpus\nLoad the spaCy model and process the corpus:\n\nnlp = spacy.load(\"en_docusco_spacy\")\nds_tokens = ds.docuscope_parse(df, nlp_model=nlp)\n\nPerformance: Corpus processing completed in 68.40s\n\n\nCheck the result:\n\n\nCode\nds_tokens.head()\n\n\n\nshape: (5, 6)\n\n\n\ndoc_id\ntoken\npos_tag\nds_tag\npos_id\nds_id\n\n\nstr\nstr\nstr\nstr\nu32\nu32\n\n\n\n\n\"acad_01\"\n\"Teachers \"\n\"NN2\"\n\"Character\"\n1\n1\n\n\n\"acad_01\"\n\"and \"\n\"CC\"\n\"Untagged\"\n2\n2\n\n\n\"acad_01\"\n\"other \"\n\"JJ\"\n\"InformationTopics\"\n3\n3\n\n\n\"acad_01\"\n\"school \"\n\"NN1\"\n\"InformationTopics\"\n4\n3\n\n\n\"acad_01\"\n\"personnel \"\n\"NN2\"\n\"Character\"\n5\n4",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Keyness</span>"
    ]
  },
  {
    "objectID": "tutorials/keyness.html#corpus-composition-table",
    "href": "tutorials/keyness.html#corpus-composition-table",
    "title": "5Â  Keyness",
    "section": "5.3 Corpus composition table",
    "text": "5.3 Corpus composition table\nIt is conventional to report the composition of the corpus or corpora you are using for your study. Letâ€™s create a table showing texts and tokens by text type:\n\ncorpus_comp = (\n    df\n    .with_columns(\n        pl.col(\"doc_id\").str.extract(r\"^([a-z]+)\", 1).alias(\"text_type\")\n    )\n    .group_by(\"text_type\")\n    .agg([\n        pl.count().alias(\"Texts\"),\n        pl.col(\"text\").str.len_chars().sum().alias(\"Characters\")\n    ])\n    .sort(\"text_type\")\n)\n\n/tmp/ipykernel_2829/541498524.py:8: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n(Deprecated in version 0.20.5)\n  pl.count().alias(\"Texts\"),\n\n\nDisplay as a formatted table:\n\n\nCode\nGT(corpus_comp)\n\n\n\n\n\n\n\n\ntext_type\nTexts\nCharacters\n\n\n\n\nacad\n50\n800127\n\n\nblog\n50\n713929\n\n\nfic\n50\n702355\n\n\nmag\n50\n746641\n\n\nnews\n50\n753216\n\n\nspok\n50\n691963\n\n\ntvm\n50\n661891\n\n\nweb\n50\n724367\n\n\n\n\n\n\n        \n\n\nThis gives us context for interpreting keyness resultsâ€”we know the relative sizes and composition of each text type.",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Keyness</span>"
    ]
  },
  {
    "objectID": "tutorials/keyness.html#research-workflow-from-question-to-keyness",
    "href": "tutorials/keyness.html#research-workflow-from-question-to-keyness",
    "title": "5Â  Keyness",
    "section": "5.4 Research workflow: From question to keyness",
    "text": "5.4 Research workflow: From question to keyness\nKeyness analysis is not a fishing expeditionâ€”it should be guided by research questions. Hereâ€™s a practical workflow:\n\n\n\n\n\n\nComputational Reasoning: The Keyness Workflow\n\n\n\n1. Start with a research question\n\nâŒ Bad: â€œWhat are the keywords in my corpus?â€\nâœ… Good: â€œHow does academic writing differ lexically from other registers?â€\nâœ… Good: â€œWhat linguistic features distinguish Author A from Author B?â€\n\n2. Choose your target and reference thoughtfully\n\nTarget: The corpus you want to characterize (e.g., academic texts)\nReference: The baseline for comparison (e.g., all other text types)\nCritical question: What claims can you make based on this comparison?\n\n3. Decide on your unit of analysis\n\nIndividual tokens (most common)\nPOS tags (grammatical patterns)\nDocuScope categories (rhetorical patterns)\nN-grams (multi-word expressions)\n\n4. Set appropriate thresholds\n\nLarger corpora â†’ stricter p-value thresholds (p &lt; 0.001 instead of 0.05)\nConsider both LL and LR when filtering\nCheck dispersion (Range) to avoid document-specific artifacts\n\n5. Interpret results in context\n\nDonâ€™t just list keywordsâ€”explain what they reveal\nConsider negative keywords (whatâ€™s absent)\nConnect to your research question\nLook for patterns, not just individual words\n\n6. Validate and iterate\n\nDo the keywords align with your domain knowledge?\nCheck concordances to see words in context\nRefine your corpus boundaries if needed\nConsider multiple comparisons to triangulate findings",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Keyness</span>"
    ]
  },
  {
    "objectID": "tutorials/keyness.html#choosing-target-and-reference-corpora",
    "href": "tutorials/keyness.html#choosing-target-and-reference-corpora",
    "title": "5Â  Keyness",
    "section": "5.5 Choosing target and reference corpora",
    "text": "5.5 Choosing target and reference corpora\nThe first step in carrying out a keyness calculation is to decide what youâ€™re comparing to what. This choice has clear implications for what you can claim based on your results.\nWhat would it show, for example, to compare television and movie scripts to blogs? Whatâ€™s the research question? What do you hope to learn?\nLetâ€™s start by comparing the academic texts to the other text types. Weâ€™ll use polarsâ€™ filter() function. Note that the tilde (~) is a negator (so â€œdoes not containâ€):\n\ntarget = ds_tokens.filter(pl.col(\"doc_id\").str.contains(\"acad\"))\nreference = ds_tokens.filter(~pl.col(\"doc_id\").str.contains(\"acad\"))\n\nCheck the results:\n\n\nCode\ntarget.head()\n\n\n\nshape: (5, 6)\n\n\n\ndoc_id\ntoken\npos_tag\nds_tag\npos_id\nds_id\n\n\nstr\nstr\nstr\nstr\nu32\nu32\n\n\n\n\n\"acad_01\"\n\"Teachers \"\n\"NN2\"\n\"Character\"\n1\n1\n\n\n\"acad_01\"\n\"and \"\n\"CC\"\n\"Untagged\"\n2\n2\n\n\n\"acad_01\"\n\"other \"\n\"JJ\"\n\"InformationTopics\"\n3\n3\n\n\n\"acad_01\"\n\"school \"\n\"NN1\"\n\"InformationTopics\"\n4\n3\n\n\n\"acad_01\"\n\"personnel \"\n\"NN2\"\n\"Character\"\n5\n4\n\n\n\n\n\n\n\n\nCode\nreference.head()\n\n\n\nshape: (5, 6)\n\n\n\ndoc_id\ntoken\npos_tag\nds_tag\npos_id\nds_id\n\n\nstr\nstr\nstr\nstr\nu32\nu32\n\n\n\n\n\"blog_01\"\n\"unpleasant\"\n\"JJ\"\n\"Negative\"\n146154\n122236\n\n\n\"blog_01\"\n\", \"\n\"Y\"\n\"Untagged\"\n146155\n122237\n\n\n\"blog_01\"\n\"and \"\n\"CC\"\n\"Narrative\"\n146156\n122238\n\n\n\"blog_01\"\n\"then \"\n\"RT\"\n\"Narrative\"\n146157\n122238\n\n\n\"blog_01\"\n\"you \"\n\"PPY\"\n\"Untagged\"\n146158\n122239",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Keyness</span>"
    ]
  },
  {
    "objectID": "tutorials/keyness.html#creating-frequency-tables",
    "href": "tutorials/keyness.html#creating-frequency-tables",
    "title": "5Â  Keyness",
    "section": "5.6 Creating frequency tables",
    "text": "5.6 Creating frequency tables\nThe docuscospacy package has a function called keyness_table() that will calculate all of the statistical information we need.\nSee here: https://docuscospacy.readthedocs.io/en/latest/corpus_analysis.html#Keyword-tables\nThe function requires us to first create frequency tables for our target and our reference:\n\nwc_target = ds.frequency_table(target)\nwc_ref = ds.frequency_table(reference)",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Keyness</span>"
    ]
  },
  {
    "objectID": "tutorials/keyness.html#creating-a-keyness-table",
    "href": "tutorials/keyness.html#creating-a-keyness-table",
    "title": "5Â  Keyness",
    "section": "5.7 Creating a keyness table",
    "text": "5.7 Creating a keyness table\nNow we can generate our keyness table:\n\nkw = ds.keyness_table(wc_target, wc_ref)\n\nCheck the table:\n\n\nCode\nkw.head(20)\n\n\n\nshape: (20, 11)\n\n\n\nToken\nTag\nLL\nLR\nPV\nâ€¦\nRF_Ref\nAF\nAF_Ref\nRange\nRange_Ref\n\n\nstr\nstr\nf64\nf64\nf64\nâ€¦\nf64\nu32\nu32\nf64\nf64\n\n\n\n\n\"of\"\n\"IO\"\n1548.592131\n0.98328\n0.0\nâ€¦\n19509.930701\n4872\n17652\n100.0\n100.0\n\n\n\"the\"\n\"AT\"\n691.134509\n0.472652\n2.5327e-152\nâ€¦\n47249.577241\n8282\n42750\n100.0\n100.0\n\n\n\"social\"\n\"JJ\"\n458.52448\n3.499488\n1.0069e-101\nâ€¦\n146.998685\n210\n133\n100.0\n100.0\n\n\n\"studies\"\n\"NN2\"\n391.557242\n3.97618\n3.7920e-87\nâ€¦\n78.472982\n156\n71\n100.0\n100.0\n\n\n\"in\"\n\"II\"\n340.508149\n0.587222\n4.9454e-76\nâ€¦\n14338.450656\n2721\n12973\n100.0\n100.0\n\n\n\"study\"\n\"NN1\"\n335.543027\n3.51295\n5.9640e-75\nâ€¦\n106.104314\n153\n96\n100.0\n100.0\n\n\n\"perfectionism\"\n\"NN1\"\n300.378833\n9.049978\n2.7242e-67\nâ€¦\n1.105253\n74\n1\n100.0\n100.0\n\n\n\"by\"\n\"II\"\n296.287372\n1.082657\n2.1215e-66\nâ€¦\n2945.499961\n788\n2665\n100.0\n100.0\n\n\n\"practice\"\n\"NN1\"\n265.747035\n3.623427\n9.5907e-60\nâ€¦\n75.157222\n117\n68\n100.0\n100.0\n\n\n\"science\"\n\"NN1\"\n257.2121\n3.494349\n6.9538e-58\nâ€¦\n82.893995\n118\n75\n100.0\n100.0\n\n\n\"students\"\n\"NN2\"\n241.996508\n2.801531\n1.4434e-54\nâ€¦\n165.78799\n146\n150\n100.0\n100.0\n\n\n\"changes\"\n\"NN2\"\n240.217168\n3.296903\n3.5266e-54\nâ€¦\n95.051781\n118\n86\n100.0\n100.0\n\n\n\"learning\"\n\"NN1\"\n238.915772\n5.019849\n6.7783e-54\nâ€¦\n18.789306\n77\n17\n100.0\n100.0\n\n\n\"patients\"\n\"NN2\"\n238.835566\n4.203095\n7.0568e-54\nâ€¦\n38.683864\n90\n35\n100.0\n100.0\n\n\n\"results\"\n\"NN2\"\n236.995138\n3.634074\n1.7779e-53\nâ€¦\n66.315196\n104\n60\n100.0\n100.0\n\n\n\"research\"\n\"NN1\"\n226.906408\n2.656493\n2.8185e-51\nâ€¦\n184.577296\n147\n167\n100.0\n100.0\n\n\n\"et\"\n\"RA21\"\n220.606438\n5.840525\n6.6697e-50\nâ€¦\n8.842026\n64\n8\n100.0\n100.0\n\n\n\"al\"\n\"RA22\"\n220.517562\n5.692968\n6.9741e-50\nâ€¦\n9.947279\n65\n9\n100.0\n100.0\n\n\n\"methylation\"\n\"NN1\"\n196.217098\n8.455235\n1.3976e-44\nâ€¦\n1.105253\n49\n1\n100.0\n100.0\n\n\n\"education\"\n\"NN1\"\n196.077076\n3.027403\n1.4995e-44\nâ€¦\n103.893807\n107\n94\n100.0\n100.0\n\n\n\n\n\n\nThe columns are as follows:\n\nToken: the token\nTag: the POS tag\nLL: the keyness value or log-likelihood, also known as a GÂ² or goodness-of-fit test\nLR: the effect size, which here is the log ratio\nPV: the p-value associated with the log-likelihood\nRF_Tar: the relative frequency in the target corpus (per million words)\nRF_Ref: the relative frequency in the reference corpus (per million words)\nAF_Tar: the absolute frequency in the target corpus\nAF_Ref: the absolute frequency in the reference corpus\nRange_Tar: the percentage of texts in which the token appears in the target corpus\nRange_Ref: the percentage of texts in which the token appears in the reference corpus\n\n\n5.7.1 Interpreting the results\nLooking at the top keywords for academic texts:\n\nHigh LL values indicate strong statistical evidence that the word is more frequent than expected\nPositive LR values show the word is overrepresented in the target (academic)\nThe Range columns show whether a word appears widely or is concentrated in a few texts",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Keyness</span>"
    ]
  },
  {
    "objectID": "tutorials/keyness.html#comparing-tags-instead-of-tokens",
    "href": "tutorials/keyness.html#comparing-tags-instead-of-tokens",
    "title": "5Â  Keyness",
    "section": "5.8 Comparing tags instead of tokens",
    "text": "5.8 Comparing tags instead of tokens\nIt can be useful sometimes to compare tags (parts-of-speech or DocuScope categories) instead of individual tokens. For that, the process is the same, but we create tables of tag frequencies:\n\ntag_tar = ds.tags_table(target, count_by='pos')  # by part-of-speech\ntag_ref = ds.tags_table(reference, count_by='pos')\n\nAnd generate a keyness table by setting tags_only=True:\n\nkt_pos = ds.keyness_table(tag_tar, tag_ref, tags_only=True)\n\nCheck the result:\n\n\nCode\nkt_pos.head(10)\n\n\n\nshape: (10, 10)\n\n\n\nTag\nLL\nLR\nPV\nRF\nRF_Ref\nAF\nAF_Ref\nRange\nRange_Ref\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nu32\nu32\nf64\nf64\n\n\n\n\n\"JJ\"\n2222.682823\n0.733261\n0.0\n9.354392\n5.627065\n11816\n50912\n100.0\n100.0\n\n\n\"NN2\"\n2028.250863\n0.789941\n0.0\n7.459922\n4.314577\n9423\n39037\n100.0\n100.0\n\n\n\"IO\"\n1548.592131\n0.98328\n0.0\n3.857024\n1.950993\n4872\n17652\n100.0\n100.0\n\n\n\"NN1\"\n1492.093018\n0.415132\n0.0\n18.084155\n13.562231\n22843\n122707\n100.0\n100.0\n\n\n\"AT\"\n655.713132\n0.455552\n1.2783e-144\n6.667458\n4.86212\n8422\n43991\n100.0\n100.0\n\n\n\"VVN\"\n514.599191\n0.663204\n6.3324e-114\n2.601433\n1.642738\n3286\n14863\n100.0\n100.0\n\n\n\"MC\"\n425.963724\n0.578643\n1.2285e-94\n2.769267\n1.854283\n3498\n16777\n100.0\n100.0\n\n\n\"FO\"\n317.537151\n1.44874\n4.9816e-71\n0.408503\n0.149651\n516\n1354\n100.0\n100.0\n\n\n\"II\"\n234.458124\n0.267098\n6.3554e-53\n6.610458\n5.493219\n8350\n49701\n100.0\n100.0\n\n\n\"RA21\"\n220.606438\n5.840525\n6.6697e-50\n0.050667\n0.000884\n64\n8\n100.0\n100.0\n\n\n\n\n\n\nThis shows which grammatical categories are key to academic writing (e.g., more nouns, fewer pronouns).\nWe can do the same for DocuScope rhetorical categories:\n\nds_tar = ds.tags_table(target, count_by='ds')\nds_ref = ds.tags_table(reference, count_by='ds')\nkt_ds = ds.keyness_table(ds_tar, ds_ref, tags_only=True)\n\n\n\nCode\nkt_ds.head(10)\n\n\n\nshape: (10, 10)\n\n\n\nTag\nLL\nLR\nPV\nRF\nRF_Ref\nAF\nAF_Ref\nRange\nRange_Ref\n\n\nstr\nf64\nf64\nf64\nf64\nf64\nu32\nu32\nf64\nf64\n\n\n\n\n\"AcademicTerms\"\n3926.988896\n1.206877\n0.0\n8.53599\n3.697838\n8747\n25636\n100.0\n100.0\n\n\n\"Untagged\"\n590.347255\n0.209737\n2.1056e-130\n33.036342\n28.566359\n33853\n198042\n100.0\n100.0\n\n\n\"InformationTopics\"\n588.038704\n0.640758\n6.6914e-130\n3.935709\n2.524269\n4033\n17500\n100.0\n100.0\n\n\n\"AcademicWritingMoves\"\n545.07555\n2.118669\n1.4834e-120\n0.481107\n0.110779\n493\n768\n100.0\n100.0\n\n\n\"PublicTerms\"\n497.706143\n0.783065\n2.9996e-110\n2.311851\n1.343488\n2369\n9314\n100.0\n100.0\n\n\n\"Citation\"\n465.831508\n1.303515\n2.5874e-103\n0.889023\n0.360177\n911\n2497\n100.0\n100.0\n\n\n\"InformationChange\"\n390.543094\n1.036224\n6.3044e-87\n1.103716\n0.538174\n1131\n3731\n100.0\n100.0\n\n\n\"InformationExposition\"\n171.44672\n0.277324\n3.5743e-39\n5.583964\n4.60744\n5722\n31942\n100.0\n100.0\n\n\n\"Reasoning\"\n147.032946\n0.44636\n7.7184e-34\n1.930283\n1.41662\n1978\n9821\n100.0\n100.0\n\n\n\"InformationChangePositive\"\n119.194291\n1.143781\n9.4958e-28\n0.28398\n0.128521\n291\n891\n100.0\n100.0",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Keyness</span>"
    ]
  },
  {
    "objectID": "tutorials/keyness.html#effect-size",
    "href": "tutorials/keyness.html#effect-size",
    "title": "5Â  Keyness",
    "section": "5.9 Effect size",
    "text": "5.9 Effect size\nWhile the LL value produces one important piece of information (the amount of evidence we have for an effect), it neglects another (the magnitude of the effect). Whenever we report on significance it is critical to report effect size.\nSome common effect size measures include:\n\n%DIFF - see Gabrielatos and Marchi (2011)\nBayes Factor (BIC) - see Wilson (2013)\nEffect Size for Log Likelihood (ELL) - see Johnston et al.Â (2006)\nRelative Risk\nOdds Ratio\nLog Ratio - see Andrew Hardieâ€™s CASS blog\n\n\n5.9.1 Log Ratio (LR)\nThe keyness_table() function returns Hardieâ€™s Log Ratio, which is easy and intuitive. It is simply the (base 2) logarithm of the ratio of relative frequencies:\n\nLR = 1: Token is 2Ã— more frequent in target than reference\nLR = 2: Token is 4Ã— more frequent in target\nLR = 3: Token is 8Ã— more frequent in target\nLR = -1: Token is 2Ã— more frequent in reference than target\n\nThis makes it straightforward to interpret the practical significance of keyness results.\n\n\n\n\n\n\nExample Interpretation\n\n\n\nIf a word has: - LL = 150 (very high statistical significance) - LR = 0.5 (only 1.4Ã— more frequent)\nWe have strong evidence for a difference, but the magnitude is modest. Both pieces of information matter.",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Keyness</span>"
    ]
  },
  {
    "objectID": "tutorials/keyness.html#p-values-and-statistical-significance",
    "href": "tutorials/keyness.html#p-values-and-statistical-significance",
    "title": "5Â  Keyness",
    "section": "5.10 P-values and statistical significance",
    "text": "5.10 P-values and statistical significance\nIt is important to interpret and report p-values correctly. A p-value represents a threshold (conventionally 0.05, 0.01, or 0.001) at which we can claim a difference is statistically significant.\nThe threshold we choose is largely dependent on the size of our corpora. With a corpus of many millions of words, at a p-value &lt; 0.05, an enormous quantity of tokens would reach significance.\n\n\n\n\n\n\nInterpreting P-values\n\n\n\n\np &lt; 0.05: Significant (but with large corpora, may be trivial differences)\np &lt; 0.01: More stringent threshold (recommended for larger corpora)\np &lt; 0.001: Very stringent (for very large corpora)\n\nNote that the default threshold in the keyness_table() function is threshold=0.01. In other words, it returns only values below that threshold.\nAvoid phrases like â€œmarginally significantâ€ or â€œapproaching significanceâ€â€”statistical significance is a threshold, not a continuum.",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Keyness</span>"
    ]
  },
  {
    "objectID": "tutorials/keyness.html#visualizing-keyness",
    "href": "tutorials/keyness.html#visualizing-keyness",
    "title": "5Â  Keyness",
    "section": "5.11 Visualizing keyness",
    "text": "5.11 Visualizing keyness\nTo understand the relationship between statistical significance and effect size, letâ€™s examine specific examples that illustrate different scenarios.\n\n5.11.1 Finding representative examples\nWeâ€™ll select tokens that represent different combinations of LL (statistical significance) and LR (effect size):\n\n# Find examples of different keyness patterns\n# High LL, High LR: truly distinctive keywords\nhigh_high = kw.filter((pl.col(\"LL\") &gt; 50) & (pl.col(\"LR\") &gt; 2)).head(3)\n\n# High LL, Low LR: statistically significant but small effect\nhigh_low = kw.filter((pl.col(\"LL\") &gt; 20) & (pl.col(\"LR\") &lt; 1) & (pl.col(\"LR\") &gt; 0)).head(3)\n\n# Moderate LL, High LR: large effect with moderate evidence\nmod_high = kw.filter((pl.col(\"LL\") &gt; 10) & (pl.col(\"LL\") &lt; 30) & (pl.col(\"LR\") &gt; 3)).head(3)\n\n# Combine for visualization\nexamples = pl.concat([high_high, high_low, mod_high])\n\nLetâ€™s look at these examples:\n\n\nCode\nexamples.select([\"Token\", \"LL\", \"LR\", \"PV\"]).head(10)\n\n\n\nshape: (9, 4)\n\n\n\nToken\nLL\nLR\nPV\n\n\nstr\nf64\nf64\nf64\n\n\n\n\n\"social\"\n458.52448\n3.499488\n1.0069e-101\n\n\n\"studies\"\n391.557242\n3.97618\n3.7920e-87\n\n\n\"study\"\n335.543027\n3.51295\n5.9640e-75\n\n\n\"of\"\n1548.592131\n0.98328\n0.0\n\n\n\"the\"\n691.134509\n0.472652\n2.5327e-152\n\n\n\"in\"\n340.508149\n0.587222\n4.9454e-76\n\n\n\"a.\"\n29.838973\n4.299957\n4.6946e-8\n\n\n\"microhardness\"\n29.394234\n6.64788\n5.9053e-8\n\n\n\"transcendental\"\n29.394234\n6.64788\n5.9053e-8\n\n\n\n\n\n\n\n\n5.11.2 Visualizing the patterns\nNow weâ€™ll create side-by-side bar plots to show both LL and LR for these examples:\n\n\nCode\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# Get tokens and values\ntokens = examples[\"Token\"].to_list()\nll_values = examples[\"LL\"].to_list()\nlr_values = examples[\"LR\"].to_list()\n\n# Plot 1: Log-Likelihood (Statistical Significance)\ncolors_ll = ['darkgreen' if ll &gt; 50 else 'orange' if ll &gt; 20 else 'steelblue' for ll in ll_values]\nax1.barh(tokens, ll_values, color=colors_ll, alpha=0.7)\nax1.axvline(x=10.83, color='red', linestyle='--', linewidth=1, alpha=0.5, label='p &lt; 0.001')\nax1.axvline(x=6.63, color='orange', linestyle='--', linewidth=1, alpha=0.5, label='p &lt; 0.01')\nax1.set_xlabel('Log-Likelihood (LL)', fontsize=11)\nax1.set_title('Statistical Significance', fontsize=12, fontweight='bold')\nax1.legend(fontsize=9)\nax1.grid(axis='x', alpha=0.3)\n\n# Plot 2: Log Ratio (Effect Size)\ncolors_lr = ['darkgreen' if lr &gt; 2 else 'orange' if lr &gt; 1 else 'steelblue' for lr in lr_values]\nax2.barh(tokens, lr_values, color=colors_lr, alpha=0.7)\nax2.axvline(x=1, color='gray', linestyle='--', linewidth=1, alpha=0.5, label='2Ã— more frequent')\nax2.axvline(x=2, color='orange', linestyle='--', linewidth=1, alpha=0.5, label='4Ã— more frequent')\nax2.axvline(x=3, color='red', linestyle='--', linewidth=1, alpha=0.5, label='8Ã— more frequent')\nax2.set_xlabel('Log Ratio (LR)', fontsize=11)\nax2.set_title('Effect Size', fontsize=12, fontweight='bold')\nax2.legend(fontsize=9)\nax2.grid(axis='x', alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting the Patterns\n\n\n\nLooking at these examples side-by-side reveals important patterns:\n\nDark green bars (both plots): These tokens have both high statistical significance and large effect sizes. They are truly distinctive keywordsâ€”highly reliable evidence of a substantial difference.\nOrange bars (LL high, LR moderate): These tokens are statistically significant but with more modest effect sizes. Theyâ€™re real differences, but not as dramatic as they might first appear from the LL value alone.\nBlue bars (LL moderate, LR high): These tokens show large effect sizes but with less statistical certainty. This often happens with relatively rare words that nonetheless show strong preferences when they do appear.\n\nThe key insight: You need both panels to tell the full story. Statistical significance alone (LL) doesnâ€™t tell you how big the effect is, and effect size alone (LR) doesnâ€™t tell you how certain we can be about it.\n\n\n\n\n5.11.3 A practical example\nLetâ€™s examine one token in detail to see what these numbers actually mean:\n\n# Pick the first high LL, high LR example\nexample_token = examples.head(1)\nexample_token.select([\"Token\", \"LL\", \"LR\", \"RF\", \"RF_Ref\", \"PV\"])\n\n\nshape: (1, 6)\n\n\n\nToken\nLL\nLR\nRF\nRF_Ref\nPV\n\n\nstr\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"social\"\n458.52448\n3.499488\n1662.510391\n146.998685\n1.0069e-101\n\n\n\n\n\n\nFor the token â€œsocialâ€:\n\nLL = 458.52: Extremely strong statistical evidence (p &lt; 0.001) that this is not due to chance\nLR = 3.5: The word appears approximately 11.3Ã— more frequently in academic texts than in other text types\nFrequencies: 1662.51 per million in academic vs.Â 147.0 per million in reference\n\nThis is a truly distinctive keywordâ€”both statistically reliable and practically meaningful.",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Keyness</span>"
    ]
  },
  {
    "objectID": "tutorials/keyness.html#what-to-do-after-keyness-analysis",
    "href": "tutorials/keyness.html#what-to-do-after-keyness-analysis",
    "title": "5Â  Keyness",
    "section": "5.12 What to do after keyness analysis",
    "text": "5.12 What to do after keyness analysis\nIdentifying keywords is just the beginning. Hereâ€™s how keyness connects to downstream analyses:\n\n5.12.1 1. Examine keywords in context\nHigh keyness doesnâ€™t tell you how a word is used. Next steps:\n\n# Example: Get top 5 keywords\ntop_keywords = kw.head(5).select(\"Token\").to_series().to_list()\nprint(\"Top keywords to investigate:\", top_keywords)\n\nTop keywords to investigate: ['of', 'the', 'social', 'studies', 'in']\n\n\nNext: Use concordancing to see these words in their original contexts. Do they cluster with certain topics? Are they used literally or metaphorically?\n\n\n5.12.2 2. Investigate collocations\nKeywords often reveal their significance through their company:\n\nWhat words co-occur with your top keywords?\nAre there distinctive multi-word expressions?\nDo certain keywords cluster together thematically?\n\nExample research question: If â€œdataâ€ is a keyword in academic writing, what verbs commonly precede it? (â€œanalyze dataâ€, â€œcollect dataâ€, â€œinterpret dataâ€)\n\n\n5.12.3 3. Build a feature set for classification\nKeywords can become features for machine learning:\n\n# Get keywords as a feature list\nkeyword_features = kw.filter(pl.col(\"LR\") &gt; 1).select(\"Token\").to_series().to_list()\nprint(f\"Found {len(keyword_features)} potential features for classification\")\n\nFound 3285 potential features for classification\n\n\nNext: Use these tokens as features to train a classifier that can automatically categorize new texts.\n\n\n5.12.4 4. Compare multiple subcorpora\nMove beyond binary comparisons:\n\nCalculate keyness for each text type vs.Â all others\nCreate a â€œkeyword profileâ€ for each category\nIdentify words that are key to multiple categories (avoid these as distinguishing features)\n\n\n\n5.12.5 5. Validate with qualitative analysis\nKeyness is quantitative, but interpretation requires qualitative judgment:\n\nDo the keywords align with your domain expertise?\nRead sample texts from high-keyword documents\nConsider whatâ€™s absent (negative keywords)\nDiscuss findings with domain experts\n\n\n\n\n\n\n\nFrom Keywords to Insights\n\n\n\nKeyness analysis is most powerful when combined with other methods:\n\nKeyness â†’ identifies distinctive tokens\nConcordancing â†’ shows how those tokens are used\nCollocation â†’ reveals meaningful phrases\nClose reading â†’ interprets significance\nIteration â†’ refines corpus boundaries and research questions\n\nThis iterative, multi-method approach is at the heart of corpus-assisted discourse analysis.",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Keyness</span>"
    ]
  },
  {
    "objectID": "tutorials/keyness.html#when-to-use-keyness-and-when-not-to",
    "href": "tutorials/keyness.html#when-to-use-keyness-and-when-not-to",
    "title": "5Â  Keyness",
    "section": "5.13 When to use keyness (and when not to)",
    "text": "5.13 When to use keyness (and when not to)\nKeyness is powerful but not universally applicable. Hereâ€™s a decision framework:\n\n\n\n\n\n\nDecision Tree: Is Keyness Right for Your Question?\n\n\n\nKeyness is WELL-SUITED for:\n\nâœ… Comparing defined categories: Academic vs.Â fiction, Author A vs.Â Author B\nâœ… Identifying distinctive features: What makes this corpus unique?\nâœ… Exploratory analysis: Getting a quick overview of differences\nâœ… Hypothesis generation: Finding patterns to investigate further\nâœ… Register/genre analysis: Characterizing text types\n\nKeyness is POORLY-SUITED for:\n\nâŒ Continuous variables: Keyness requires categorical groupings (canâ€™t compare â€œdegree of formalityâ€ directly)\nâŒ Temporal trends: Use time series analysis instead, or bin time into discrete periods\nâŒ Nuanced semantic differences: Keyness shows frequency, not meaning shifts\nâŒ Small corpora: Insufficient statistical power (aim for 10,000+ words per subcorpus)\nâŒ Single-text analysis: Need multiple texts for meaningful comparison\n\nConsider ALTERNATIVES when:\n\nYou want to understand how words are used (not just frequency) â†’ Use concordancing and collocation analysis\nYou want to find topics or semantic clusters â†’ Use topic modeling\nYou want to measure similarity between texts â†’ Use distance metrics or cluster analysis\nYou want to track change over time â†’ Use time series or diachronic analysis\nYou want to predict categories â†’ Use classification algorithms",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Keyness</span>"
    ]
  },
  {
    "objectID": "tutorials/keyness.html#common-pitfalls",
    "href": "tutorials/keyness.html#common-pitfalls",
    "title": "5Â  Keyness",
    "section": "5.14 Common Pitfalls",
    "text": "5.14 Common Pitfalls\n\n\n\n\n\n\nWatch Out For\n\n\n\n\nIgnoring Effect Size: High LL doesnâ€™t mean high importance. Always check LR to see the magnitude of difference.\nInappropriate Reference Corpus: Comparing academic writing to fiction will give different results than comparing to all other text types pooled. Your reference corpus shapes your claims.\nMultiple Comparisons: If you run 100 keyness tests, youâ€™ll get ~5 â€œsignificantâ€ results by chance at p &lt; 0.05. Consider adjusting thresholds.\nCorpus Size Imbalance: While log-likelihood handles this better than chi-square, extreme size differences can still be problematic.\nInterpreting Range: A high-frequency, low-range token might be driven by just a few texts. Always check dispersion.\nConflating Keyness with Importance: Keyness tells us whatâ€™s statistically distinctive, not necessarily whatâ€™s rhetorically, culturally, or theoretically important.\n\n\n\n\n5.14.1 A cautionary example\nConsider this scenario:\n\nA researcher compares 18th-century texts to 21st-century texts and finds that â€œhathâ€ is a highly significant keyword (LL = 500, LR = 8). They conclude: â€œThis proves that 18th-century authors were more concerned with possession and ownership.â€\n\nWhatâ€™s wrong here?\n\nâŒ Confusing spelling/grammar changes with semantic concerns\nâŒ Ignoring historical language change\nâŒ Not checking concordances to see how â€œhathâ€ is used\nâŒ Assuming frequency = thematic importance\n\nBetter interpretation:\nâ€œHathâ€ is key because itâ€™s an archaic verb form, not because of semantic content. To study concerns with possession, weâ€™d need to:\n\nNormalize for historical spelling differences\nLook at semantic fields related to ownership (property, possess, own, etc.)\nExamine syntactic patterns (who owns what?)\nRead texts closely to understand context\n\nThis example illustrates why computational methods inform interpretation but donâ€™t replace it.",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Keyness</span>"
    ]
  },
  {
    "objectID": "tutorials/keyness.html#discussion-questions",
    "href": "tutorials/keyness.html#discussion-questions",
    "title": "5Â  Keyness",
    "section": "5.15 Discussion Questions",
    "text": "5.15 Discussion Questions\n\nTarget vs.Â Reference: How does the choice of reference corpus shape the results and claims we can make? What would change if we compared academic texts to fiction vs.Â academic texts to news?\nStatistical vs.Â Practical Significance: Can you think of a scenario where a word might be statistically significant (high LL) but not practically important (low LR)? What about the reverse?\nNegative Keywords: Weâ€™ve focused on positive keywords (overrepresented in target). What might negative keywords (underrepresented in target) tell us about a corpus?\nPOS Tags vs.Â Tokens: When would keyness analysis of POS tags be more useful than analysis of individual tokens? What different research questions might each approach answer?\nCorpus Composition: How might the internal composition of your target or reference corpus affect keyness results? What if your â€œacademicâ€ corpus includes both hard sciences and humanities?\nCritical Reflection: Keyness assumes that statistical distinctiveness equals meaningful difference. When might this assumption be problematic? What contextual factors might keyness measures miss?",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Keyness</span>"
    ]
  },
  {
    "objectID": "tutorials/keyness.html#key-takeaways",
    "href": "tutorials/keyness.html#key-takeaways",
    "title": "5Â  Keyness",
    "section": "5.16 Key Takeaways",
    "text": "5.16 Key Takeaways\n\nKeyness measures the statistical significance of frequency differences between a target and reference corpus\nLog-likelihood (LL) quantifies the strength of evidence for a difference\nLog ratio (LR) quantifies the magnitude or effect size of a difference\nBoth metrics are essential: High LL without high LR indicates a statistically significant but small effect\nP-values must be interpreted in context of corpus size; larger corpora require more stringent thresholds\nChoice of reference corpus fundamentally shapes what claims you can make from keyness analysis\nDispersion matters: Check Range values to distinguish widespread keywords from concentrated ones",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Keyness</span>"
    ]
  },
  {
    "objectID": "tutorials/keyness.html#further-reading",
    "href": "tutorials/keyness.html#further-reading",
    "title": "5Â  Keyness",
    "section": "5.17 Further Reading",
    "text": "5.17 Further Reading\n\nGabrielatos, C., & Marchi, A. (2011). â€œKeyness: Matching metrics to definitions.â€ Proceedings of Corpus Linguistics.\nHardie, A. (2014). â€œLog Ratio: An informal introduction.â€ CASS Technical Paper.\nJohnston, J. E., Berry, K. J., & Mielke, P. W. (2006). â€œMeasures of effect size for chi-squared and likelihood-ratio goodness-of-fit tests.â€ Perceptual and Motor Skills 103(2): 412-414.\nRayson, P., & Garside, R. (2000). â€œComparing corpora using frequency profiling.â€ Proceedings of the Workshop on Comparing Corpora (pp.Â 1-6).\n\n\n\n\n\n\n\n\nReady to Practice?\n\n\n\nHead to Mini Lab 04: Keywords to apply these concepts hands-on. Youâ€™ll compare different text types, filter keyness tables, experiment with effect size thresholds, and explore both token and tag-level keyness.\nThe mini lab includes experimentation prompts to help you develop intuitions about when and how to use keyness analysis effectively.",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Keyness</span>"
    ]
  },
  {
    "objectID": "tutorials/collocations.html",
    "href": "tutorials/collocations.html",
    "title": "6Â  Collocations and N-grams",
    "section": "",
    "text": "6.1 Introduction\nWhile frequency analysis tells us which words appear often, collocations reveal which words appear together. Collocation analysis identifies words that attract each otherâ€”lexical partnerships that shape meaning, construct genre conventions, and reveal stylistic patterns invisible to single-word analysis.\nThis tutorial covers:",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Collocations and N-grams</span>"
    ]
  },
  {
    "objectID": "tutorials/collocations.html#introduction",
    "href": "tutorials/collocations.html#introduction",
    "title": "6Â  Collocations and N-grams",
    "section": "",
    "text": "Why Collocations Matter\n\n\n\nConsider â€œstrong coffeeâ€ vs.Â â€œpowerful coffeeâ€â€”both adjectives mean roughly the same thing, but only one sounds natural. This is collocation: some word combinations occur far more often than chance would predict. Studying these patterns reveals:\n\nSemantic prosody: Words carry emotional colorings from their frequent partners (â€œrancidâ€ almost always modifies negative things)\nRegister and genre: Academic writing has different collocational patterns than fiction\nCharacter and narrative: How a character is described through their collocates\nFormulaic language: Phraseological patterns that structure discourse\n\n\n\n\n\nCollocational analysis: Finding words that co-occur within a span\nAssociation measures: Different ways to quantify word attraction (PMI, NPMI, t-score)\nN-gram clusters: Fixed multi-word sequences anchored by a node word\nComputational reasoning: Moving from collocational patterns to linguistic interpretation",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Collocations and N-grams</span>"
    ]
  },
  {
    "objectID": "tutorials/collocations.html#understanding-collocation",
    "href": "tutorials/collocations.html#understanding-collocation",
    "title": "6Â  Collocations and N-grams",
    "section": "6.2 Understanding Collocation",
    "text": "6.2 Understanding Collocation\n\n6.2.1 What Makes Words Collocate?\nTwo words collocate when they appear together more often than weâ€™d expect by chance. The key question is: given the frequency of word A and word B in a corpus, how often would they co-occur if placement were random? When the observed frequency exceeds this expected frequency, we have a collocation.\nThe fundamental insight: Collocation is not about frequency aloneâ€”itâ€™s about attraction. â€œTheâ€ and â€œofâ€ appear together frequently simply because theyâ€™re both extremely common words. But â€œstrong coffeeâ€ appears together more often than the individual frequencies of â€œstrongâ€ and â€œcoffeeâ€ would predict. Thatâ€™s collocation.\n\n\n6.2.2 Core Concepts\n1. The Node Word\nThe node word is your targetâ€”the word you want to investigate. In the phrase â€œstrong coffee,â€ if weâ€™re studying â€œcoffee,â€ thatâ€™s our node. Everything else is analyzed in relation to this anchor point.\n\nNode selection drives your research question: â€œHow is this concept described?â€ â€œWhat verbs does this character perform?â€\nNodes can be single words (â€œloveâ€) or lemmas (grouping â€œlove,â€ â€œloves,â€ â€œloved,â€ â€œlovingâ€)\nChoose nodes strategically: content words (nouns, verbs, adjectives) reveal more than function words\n\n2. The Span or Window\nThe span (or window) defines how far from the node to look for collocates. A span of 4-4 means 4 words to the left and 4 to the right of the node.\n\n\n\n\n\n\nWhy Does Window Size Matter?\n\n\n\nDifferent linguistic relationships require different windows:\n\nNarrow windows (1-2 words): Capture grammatical collocations\n\nPrepositions after verbs: â€œdepend on,â€ â€œarrive atâ€\nAdjective-noun pairs: â€œstrong coffee,â€ â€œheavy rainâ€\n\nMedium windows (4-5 words): Capture semantic associations\n\nTypical academic default\nBalances precision with context\n\nWide windows (8+ words): Capture thematic co-occurrence\n\nWords appearing in the same general context\nRisk including unrelated words\n\n\n\n\nThe span creates a virtual document around each instance of your node word. If â€œcoffeeâ€ appears 100 times in your corpus with a 4-4 span, youâ€™re analyzing 100 nine-word snippets (4 before + node + 4 after).\nNote on directionality: Most collocation tools (including coll_table()) treat left and right context symmetricallyâ€”a word 2 positions to the left counts the same as 2 positions to the right. This is usually appropriate for semantic associations, but grammatical relationships often show strong directional preferences. When direction matters (like verb-preposition pairs), use clusters_by_token() to specify exact positions.\n3. Observed vs.Â Expected Frequency\nThis is the heart of collocation analysis. Consider:\n\nCorpus size: 1 million words\nâ€œTheâ€ appears 70,000 times (7% of corpus)\nâ€œCoffeeâ€ appears 500 times (0.05% of corpus)\nSpan: 4-4 window = 9 words total\n\nExpected co-occurrence: If words were placed randomly, how often would â€œtheâ€ appear in coffeeâ€™s window?\n\n500 instances of â€œcoffeeâ€ Ã— 9-word window = 4,500 word positions\n7% of words are â€œtheâ€ â†’ we expect ~315 instances of â€œtheâ€ near â€œcoffeeâ€\n\nObserved co-occurrence: Count how many times â€œtheâ€ actually appears near â€œcoffeeâ€ in your corpus.\nCollocation occurs when: Observed &gt;&gt; Expected (or Observed &lt;&lt; Expected for negative collocation)\n\n\n\n\n\n\nThe Frequency Trap\n\n\n\nA common mistake: assuming frequent co-occurrence means collocation.\nScenario: In a novel, â€œheâ€ appears 3,000 times and â€œtheâ€ appears 5,000 times. You find â€œhe theâ€ appearing 200 times together.\n\nNaive interpretation: 200 is a lot! They must collocate.\nCorrect interpretation: Given their individual frequencies, weâ€™d expect ~1,500 co-occurrences by chance. Actually appearing 200 times means they avoid each otherâ€”negative collocation!\n\nThis is why we need association measures like MI or NPMIâ€”they account for baseline frequency.\n\n\n4. Association Strength vs.Â Frequency\nAssociation measures (MI, NPMI, t-score, log-likelihood) quantify the strength of attraction between words, correcting for their individual frequencies.\nTwo dimensions to consider:\n\nStrength of association: How reliably do these words co-occur? (MI/NPMI)\nFrequency of co-occurrence: How many times do we observe this pattern? (raw count)\n\nBoth matter:\n\nHigh MI + low frequency = strong but rare pairing (possibly idiosyncratic)\nHigh MI + high frequency = robust, substantive collocation âœ“\nLow MI + high frequency = words appearing together by chance (both just common)\n\nComponents of collocation analysis:\n\nNode word: The target word youâ€™re interested in (your anchor point)\nSpan/Window: How many words to the left and right to search (commonly 4-4, 5-5, or 2-2)\nCollocate: A word that appears within the span more often than expected by chance\nAssociation measure: The statistic quantifying strength of attraction (MI, NPMI, etc.)\nObserved frequency: Actual count of co-occurrences in your data\nExpected frequency: Predicted count if words appeared independently\n\n\n\n6.2.3 The Mathematics: Mutual Information\nThe most common association measure is Pointwise Mutual Information (PMI):\n\\[\n\\text{PMI} = \\log_2 \\frac{P(w_1, w_2)}{P(w_1) \\times P(w_2)}\n\\]\nWhere:\n\n\\(P(w_1, w_2)\\) = observed probability of words appearing together in the span\n\\(P(w_1)\\) = probability of word 1 in the corpus\n\n\\(P(w_2)\\) = probability of word 2 in the corpus\n\nIntuition: If two words appear together purely by chance, the fraction equals 1 and \\(\\log_2(1) = 0\\). Higher PMI means stronger attraction.\nWorked Example:\nImagine a 10,000-word corpus with a 2-2 window (5 words total per span):\n\nâ€œcoffeeâ€ appears 50 times (node word)\nâ€œstrongâ€ appears 40 times total in the corpus\nâ€œstrongâ€ appears 15 times in coffeeâ€™s window (within 2 words before/after â€œcoffeeâ€)\n\nStep 1: Calculate probabilities\n\n\\(P(\\text{strong}) = 40/10000 = 0.004\\)\n50 instances of â€œcoffeeâ€ Ã— 5-word window = 250 word positions in coffeeâ€™s spans\n\\(P(\\text{strong, coffee}) = 15/250 = 0.06\\)\n\nStep 2: Expected co-occurrence\nIf placement were random: \\(P(\\text{strong}) \\times P(\\text{coffee's spans}) = 0.004 \\times 250 = 1\\) instance\nStep 3: PMI calculation\n\\[\\text{PMI} = \\log_2 \\frac{0.06}{0.004} = \\log_2(15) = 3.9\\]\nInterpretation: We observed â€œstrongâ€ near â€œcoffeeâ€ 15 times more often than chance would predict. Thatâ€™s a collocation!\nCompare this to a non-collocation:\n\nâ€œtheâ€ appears 700 times in the corpus\n\nâ€œtheâ€ appears 8 times in coffeeâ€™s window\nExpected: \\(700/10000 \\times 250 = 17.5\\) times\nObserved: 8 times\nPMI = \\(\\log_2(8/17.5) = -1.1\\) (negative = avoidance!)\n\nThe problem: PMI is biased toward rare words. Two words that appear together once each will have high PMI even though one co-occurrence isnâ€™t meaningful.\nThe solution: Use Normalized PMI (NPMI):\n\\[\n\\text{NPMI} = \\frac{\\text{PMI}}{-\\log_2 P(w_1, w_2)}\n\\]\nNPMI ranges from -1 (words never co-occur) to +1 (words always co-occur), making scores comparable across different frequency ranges. This normalization prevents rare words from dominating your results.\n\n\n\n\n\n\nInterpreting PMI: What You Need to Know\n\n\n\nPMI is extremely sensitive to rare occurrences. Even with NPMI normalization, two words that appear together just once or twice can show high association scores. This is mathematically correct (they do co-occur at a high rate relative to their individual frequencies), but may not be linguistically meaningful.\nTwo common approaches:\n\nFilter by minimum frequency thresholds: Only analyze collocates that appear at least N times (e.g., 5 or 10). Some researchers find this practical and necessary; others argue itâ€™s arbitrary and may hide interesting rare phenomena.\nReport both association strength AND raw frequency: Present NPMI scores alongside co-occurrence counts so readers can judge for themselves.\n\nImportant limitations:\n\nPMI has no p-value: Itâ€™s not a statistical hypothesis test. Thereâ€™s no significance level, no confidence interval. PMI quantifies association strength, not statistical reliability.\nConventional threshold: Many researchers use PMI &gt; 3 as a rule of thumb for â€œstrongâ€ collocations (meaning words co-occur 8Ã— more than expected, since \\(2^3 = 8\\)). For NPMI, values above 0.2-0.3 often indicate meaningful association, but these are conventions, not statistical cutoffs.\nAlternative methods exist: PMI treats words as discrete symbols. For semantic similarity and paradigmatic relations (words that could substitute for each other), consider vector embeddings with cosine similarity (see Vector Models). Different questions require different tools.",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Collocations and N-grams</span>"
    ]
  },
  {
    "objectID": "tutorials/collocations.html#collocation-analysis-with-docuscospacy",
    "href": "tutorials/collocations.html#collocation-analysis-with-docuscospacy",
    "title": "6Â  Collocations and N-grams",
    "section": "6.3 Collocation Analysis with docuscospacy",
    "text": "6.3 Collocation Analysis with docuscospacy\n\n6.3.1 Basic Workflow\nWeâ€™ll analyze Twilight by Stephenie Meyer, exploring how different characters and concepts are described through their collocates.\n\n\n\n\n\n\nConnecting to Mini Lab 6\n\n\n\nMini Lab 6: Collocations provides hands-on practice with these methods using Google Colab. The mini lab walks through uploading your own data, generating collocation tables, and exporting results for further analysis.\n\n\nLoad and prepare data:\n\n# Already loaded at top\ntwilight_tokens.head()\n\n\nshape: (5, 6)\n\n\n\ndoc_id\ntoken\npos_tag\nds_tag\npos_id\nds_id\n\n\nstr\nstr\nstr\nstr\nu32\nu32\n\n\n\n\n\"Meyer_Twilight.txt\"\n\"Preface \"\n\"NN1\"\n\"InformationPlace\"\n595242\n477529\n\n\n\"Meyer_Twilight.txt\"\n\"I\"\n\"PPIS1\"\n\"FirstPerson\"\n595243\n477530\n\n\n\"Meyer_Twilight.txt\"\n\"'d \"\n\"VM\"\n\"FirstPerson\"\n595244\n477530\n\n\n\"Meyer_Twilight.txt\"\n\"never \"\n\"RR\"\n\"ForceStressed\"\n595245\n477531\n\n\n\"Meyer_Twilight.txt\"\n\"given \"\n\"VVN\"\n\"ForceStressed\"\n595246\n477531\n\n\n\n\n\n\n\n\n6.3.2 Generating a Collocation Table\nThe coll_table() function finds words that collocate with a node word:\n\n# Find collocates of \"me\" (narrator Bella)\nme_collocates = ds.coll_table(twilight_tokens, node_word='me')\nme_collocates.head(10)\n\n\nshape: (10, 5)\n\n\n\nToken\nTag\nFreq Span\nFreq Total\nMI\n\n\nstr\nstr\nu32\nu32\nf64\n\n\n\n\n\"trust\"\n\"VV0\"\n10\n8\n0.487659\n\n\n\"he\"\n\"PPHS1\"\n482\n2784\n0.471936\n\n\n\"tell\"\n\"VV0\"\n23\n34\n0.463423\n\n\n\"astonished\"\n\"VVN\"\n2\n1\n0.459082\n\n\n\"coherent\"\n\"JJ\"\n2\n1\n0.459082\n\n\n\"frightening\"\n\"VVG\"\n2\n1\n0.459082\n\n\n\"insistent\"\n\"JJ\"\n2\n1\n0.459082\n\n\n\"oath\"\n\"NN1\"\n2\n1\n0.459082\n\n\n\"offending\"\n\"VVG\"\n2\n1\n0.459082\n\n\n\"paranoia\"\n\"NP1\"\n2\n1\n0.459082\n\n\n\n\n\n\nUnderstanding the output:\n\nToken: The collocating word\nTag: Part-of-speech tag\nFreq Span: Times this word appears within the span around â€œmeâ€\nFreq Total: Total times this word appears in the corpus\nMI: Normalized PMI score (default)\n\n\n\n6.3.3 Filtering and Exploration\nRaw collocation tables are overwhelming. Strategic filtering reveals patterns:\nFilter by part of speechâ€”nouns near â€œmeâ€:\n\nme_collocates.filter(pl.col(\"Tag\").str.starts_with(\"NN\")).head(10)\n\n\nshape: (10, 5)\n\n\n\nToken\nTag\nFreq Span\nFreq Total\nMI\n\n\nstr\nstr\nu32\nu32\nf64\n\n\n\n\n\"oath\"\n\"NN1\"\n2\n1\n0.459082\n\n\n\"jump\"\n\"NN1\"\n5\n4\n0.454196\n\n\n\"think\"\n\"NN1\"\n3\n2\n0.449514\n\n\n\"english\"\n\"NN1\"\n7\n9\n0.421259\n\n\n\"route\"\n\"NN1\"\n2\n2\n0.39617\n\n\n\"arms\"\n\"NN2\"\n21\n55\n0.392562\n\n\n\"absurdities\"\n\"NN2\"\n1\n1\n0.372722\n\n\n\"addiction\"\n\"NN1\"\n1\n1\n0.372722\n\n\n\"ambivalent\"\n\"NN1\"\n1\n1\n0.372722\n\n\n\"aquarium\"\n\"NN1\"\n1\n1\n0.372722\n\n\n\n\n\n\nAdd frequency threshold to focus on substantive patterns:\n\nme_collocates.filter(\n    pl.col(\"Tag\").str.starts_with(\"NN\"),\n    pl.col(\"Freq Total\") &gt; 10\n).head(10)\n\n\nshape: (10, 5)\n\n\n\nToken\nTag\nFreq Span\nFreq Total\nMI\n\n\nstr\nstr\nu32\nu32\nf64\n\n\n\n\n\"arms\"\n\"NN2\"\n21\n55\n0.392562\n\n\n\"door\"\n\"NN1\"\n34\n166\n0.339577\n\n\n\"mom\"\n\"NN1\"\n16\n62\n0.336793\n\n\n\"chest\"\n\"NN1\"\n8\n25\n0.334889\n\n\n\"surprise\"\n\"NN1\"\n7\n23\n0.325174\n\n\n\"step\"\n\"NN1\"\n6\n19\n0.323841\n\n\n\"eyes\"\n\"NN2\"\n70\n497\n0.322257\n\n\n\"look\"\n\"NN1\"\n11\n44\n0.319834\n\n\n\"minute\"\n\"NNT1\"\n8\n36\n0.29703\n\n\n\"back\"\n\"NN1\"\n15\n82\n0.296155\n\n\n\n\n\n\nLook at verbs (especially present participles -ing forms):\n\nme_collocates.filter(\n    pl.col(\"Tag\") == \"VVG\",\n    pl.col(\"Freq Total\") &gt; 5\n).head(10)\n\n\nshape: (10, 5)\n\n\n\nToken\nTag\nFreq Span\nFreq Total\nMI\n\n\nstr\nstr\nu32\nu32\nf64\n\n\n\n\n\"staring\"\n\"VVG\"\n24\n62\n0.400315\n\n\n\"telling\"\n\"VVG\"\n7\n13\n0.383602\n\n\n\"speaking\"\n\"VVG\"\n7\n14\n0.376012\n\n\n\"holding\"\n\"VVG\"\n10\n23\n0.375412\n\n\n\"waiting\"\n\"VVG\"\n18\n64\n0.351044\n\n\n\"pulling\"\n\"VVG\"\n7\n18\n0.350276\n\n\n\"touching\"\n\"VVG\"\n5\n13\n0.337515\n\n\n\"glancing\"\n\"VVG\"\n4\n10\n0.334019\n\n\n\"opening\"\n\"VVG\"\n3\n7\n0.331466\n\n\n\"making\"\n\"VVG\"\n8\n26\n0.330817\n\n\n\n\n\n\n\n\n6.3.4 Comparative Analysis\nCollocations become more revealing when compared across different node words:\n\n# Edward's collocates (love interest)\nedward_collocates = ds.coll_table(twilight_tokens, node_word='edward')\n\n# Nouns around Edward\nedward_collocates.filter(\n    pl.col(\"Tag\").str.starts_with(\"NN\"),\n    pl.col(\"Freq Total\") &gt; 10\n).head(10)\n\n\nshape: (10, 5)\n\n\n\nToken\nTag\nFreq Span\nFreq Total\nMI\n\n\nstr\nstr\nu32\nu32\nf64\n\n\n\n\n\"look\"\n\"NN1\"\n7\n44\n0.388868\n\n\n\"step\"\n\"NN1\"\n3\n19\n0.357109\n\n\n\"shadows\"\n\"NN2\"\n2\n11\n0.356772\n\n\n\"biology\"\n\"NN1\"\n3\n21\n0.347678\n\n\n\"matter\"\n\"NN1\"\n2\n13\n0.341609\n\n\n\"gaze\"\n\"NN1\"\n3\n24\n0.335095\n\n\n\"engine\"\n\"NN1\"\n3\n25\n0.331249\n\n\n\"boyfriend\"\n\"NN1\"\n2\n15\n0.328621\n\n\n\"horror\"\n\"NN1\"\n2\n16\n0.322763\n\n\n\"plan\"\n\"NN1\"\n2\n16\n0.322763\n\n\n\n\n\n\nWhat patterns emerge?\nCompare the nouns around â€œmeâ€ vs.Â â€œedwardâ€â€”do they reveal different narrative roles or descriptive patterns?",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Collocations and N-grams</span>"
    ]
  },
  {
    "objectID": "tutorials/collocations.html#n-gram-clusters-fixed-sequences",
    "href": "tutorials/collocations.html#n-gram-clusters-fixed-sequences",
    "title": "6Â  Collocations and N-grams",
    "section": "6.4 N-gram Clusters: Fixed Sequences",
    "text": "6.4 N-gram Clusters: Fixed Sequences\nWhile coll_table() finds any words within a span, clusters_by_token() identifies fixed multi-word sequences (n-grams) anchored by your node word in a specific position.\n\n6.4.1 2-grams with â€œmeâ€ in Final Position\n\n# Find patterns like \"told me\", \"asked me\"\nme_clusters_2gram = ds.clusters_by_token(twilight_tokens, \"me\", node_position=2, span=2)\nme_clusters_2gram.head(10)\n\n\nshape: (10, 7)\n\n\n\nToken_1\nToken_2\nTag_1\nTag_2\nAF\nRF\nRange\n\n\nstr\nstr\nstr\nstr\nu32\nf64\nf64\n\n\n\n\n\"at\"\n\"me\"\n\"II\"\n\"PPIO1\"\n145\n1189.538623\n100.0\n\n\n\"to\"\n\"me\"\n\"II\"\n\"PPIO1\"\n107\n877.797467\n100.0\n\n\n\"for\"\n\"me\"\n\"IF\"\n\"PPIO1\"\n84\n689.11203\n100.0\n\n\n\"with\"\n\"me\"\n\"IW\"\n\"PPIO1\"\n62\n508.630308\n100.0\n\n\n\"behind\"\n\"me\"\n\"II\"\n\"PPIO1\"\n39\n319.944871\n100.0\n\n\n\"made\"\n\"me\"\n\"VVD\"\n\"PPIO1\"\n32\n262.518869\n100.0\n\n\n\"toward\"\n\"me\"\n\"II\"\n\"PPIO1\"\n29\n237.907725\n100.0\n\n\n\"on\"\n\"me\"\n\"II\"\n\"PPIO1\"\n26\n213.296581\n100.0\n\n\n\"away from\"\n\"me\"\n\"II\"\n\"PPIO1\"\n21\n172.278007\n100.0\n\n\n\"tell\"\n\"me\"\n\"VV0\"\n\"PPIO1\"\n20\n164.074293\n100.0\n\n\n\n\n\n\nInterpreting cluster tables:\n\nToken_1, Token_2: The words in the n-gram\nTag_1, Tag_2: Part-of-speech tags for each position\nAF: Absolute frequency (raw count)\nRF: Relative frequency (per 100 tokens)\nRange: Percentage of documents containing this cluster\n\n\n\n6.4.2 Filtering Clusters by Pattern\nFind verbs preceding â€œmeâ€:\n\nme_clusters_2gram.filter(pl.col(\"Tag_1\") == \"VVG\").head(10)\n\n\nshape: (10, 7)\n\n\n\nToken_1\nToken_2\nTag_1\nTag_2\nAF\nRF\nRange\n\n\nstr\nstr\nstr\nstr\nu32\nf64\nf64\n\n\n\n\n\"watching\"\n\"me\"\n\"VVG\"\n\"PPIO1\"\n9\n73.833432\n100.0\n\n\n\"telling\"\n\"me\"\n\"VVG\"\n\"PPIO1\"\n7\n57.426002\n100.0\n\n\n\"pulling\"\n\"me\"\n\"VVG\"\n\"PPIO1\"\n6\n49.222288\n100.0\n\n\n\"holding\"\n\"me\"\n\"VVG\"\n\"PPIO1\"\n6\n49.222288\n100.0\n\n\n\"making\"\n\"me\"\n\"VVG\"\n\"PPIO1\"\n5\n41.018573\n100.0\n\n\n\"taking\"\n\"me\"\n\"VVG\"\n\"PPIO1\"\n4\n32.814859\n100.0\n\n\n\"leaving\"\n\"me\"\n\"VVG\"\n\"PPIO1\"\n4\n32.814859\n100.0\n\n\n\"touching\"\n\"me\"\n\"VVG\"\n\"PPIO1\"\n3\n24.611144\n100.0\n\n\n\"dragging\"\n\"me\"\n\"VVG\"\n\"PPIO1\"\n2\n16.407429\n100.0\n\n\n\"forcing\"\n\"me\"\n\"VVG\"\n\"PPIO1\"\n2\n16.407429\n100.0\n\n\n\n\n\n\n\n\n6.4.3 3-grams: Complex Patterns\nLonger n-grams capture more complex grammatical structures:\n\n# Find 3-word sequences with \"he\" in first position\nhe_me_clusters = ds.clusters_by_token(twilight_tokens, \"he\", node_position=1, span=3)\n\n# Filter for those with \"me\" in third position\nhe_me_clusters = he_me_clusters.filter(pl.col(\"Token_3\") == \"me\")\nhe_me_clusters.head(10)\n\n\nshape: (10, 9)\n\n\n\nToken_1\nToken_2\nToken_3\nTag_1\nTag_2\nTag_3\nAF\nRF\nRange\n\n\nstr\nstr\nstr\nstr\nstr\nstr\nu32\nf64\nf64\n\n\n\n\n\"he\"\n\"reminded\"\n\"me\"\n\"PPHS1\"\n\"VVD\"\n\"PPIO1\"\n9\n73.833432\n100.0\n\n\n\"he\"\n\"ignored\"\n\"me\"\n\"PPHS1\"\n\"VVD\"\n\"PPIO1\"\n7\n57.426002\n100.0\n\n\n\"he\"\n\"pulled\"\n\"me\"\n\"PPHS1\"\n\"VVD\"\n\"PPIO1\"\n6\n49.222288\n100.0\n\n\n\"he\"\n\"told\"\n\"me\"\n\"PPHS1\"\n\"VVD\"\n\"PPIO1\"\n6\n49.222288\n100.0\n\n\n\"he\"\n\"held\"\n\"me\"\n\"PPHS1\"\n\"VVD\"\n\"PPIO1\"\n5\n41.018573\n100.0\n\n\n\"he\"\n\"walked\"\n\"me\"\n\"PPHS1\"\n\"VVD\"\n\"PPIO1\"\n4\n32.814859\n100.0\n\n\n\"he\"\n\"caught\"\n\"me\"\n\"PPHS1\"\n\"VVD\"\n\"PPIO1\"\n4\n32.814859\n100.0\n\n\n\"he\"\n\"asked\"\n\"me\"\n\"PPHS1\"\n\"VVD\"\n\"PPIO1\"\n4\n32.814859\n100.0\n\n\n\"he\"\n\"led\"\n\"me\"\n\"PPHS1\"\n\"VVD\"\n\"PPIO1\"\n3\n24.611144\n100.0\n\n\n\"he\"\n\"gave\"\n\"me\"\n\"PPHS1\"\n\"VVD\"\n\"PPIO1\"\n3\n24.611144\n100.0\n\n\n\n\n\n\nComparing directional patterns:\n\n# Now reverse: \"I _ him\" patterns\ni_him_clusters = ds.clusters_by_token(twilight_tokens, \"I\", node_position=1, span=3)\ni_him_clusters = i_him_clusters.filter(pl.col(\"Token_3\") == \"him\")\ni_him_clusters.head(10)\n\n\nshape: (10, 9)\n\n\n\nToken_1\nToken_2\nToken_3\nTag_1\nTag_2\nTag_3\nAF\nRF\nRange\n\n\nstr\nstr\nstr\nstr\nstr\nstr\nu32\nf64\nf64\n\n\n\n\n\"i\"\n\"reminded\"\n\"him\"\n\"PPIS1\"\n\"VVD\"\n\"PPHO1\"\n12\n98.444576\n100.0\n\n\n\"i\"\n\"assured\"\n\"him\"\n\"PPIS1\"\n\"VVD\"\n\"PPHO1\"\n7\n57.426002\n100.0\n\n\n\"i\"\n\"watched\"\n\"him\"\n\"PPIS1\"\n\"VVD\"\n\"PPHO1\"\n6\n49.222288\n100.0\n\n\n\"i\"\n\"told\"\n\"him\"\n\"PPIS1\"\n\"VVD\"\n\"PPHO1\"\n6\n49.222288\n100.0\n\n\n\"i\"\n\"warned\"\n\"him\"\n\"PPIS1\"\n\"VVD\"\n\"PPHO1\"\n6\n49.222288\n100.0\n\n\n\"i\"\n\"saw\"\n\"him\"\n\"PPIS1\"\n\"VVD\"\n\"PPHO1\"\n5\n41.018573\n100.0\n\n\n\"i\"\n\"ignored\"\n\"him\"\n\"PPIS1\"\n\"VVD\"\n\"PPHO1\"\n4\n32.814859\n100.0\n\n\n\"i\"\n\"felt\"\n\"him\"\n\"PPIS1\"\n\"VVD\"\n\"PPHO1\"\n3\n24.611144\n100.0\n\n\n\"i\"\n\"imagined\"\n\"him\"\n\"PPIS1\"\n\"VVD\"\n\"PPHO1\"\n2\n16.407429\n100.0\n\n\n\"i\"\n\"asked\"\n\"him\"\n\"PPIS1\"\n\"VVD\"\n\"PPHO1\"\n2\n16.407429\n100.0",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Collocations and N-grams</span>"
    ]
  },
  {
    "objectID": "tutorials/collocations.html#computational-reasoning-from-patterns-to-insights",
    "href": "tutorials/collocations.html#computational-reasoning-from-patterns-to-insights",
    "title": "6Â  Collocations and N-grams",
    "section": "6.5 Computational Reasoning: From Patterns to Insights",
    "text": "6.5 Computational Reasoning: From Patterns to Insights\n\n6.5.1 Research Workflow\nCollocation analysis supports multiple research questions:\n1. Semantic prosody and connotation\nWhat emotional or evaluative tone do words carry through their collocates?\n\nSearch for a node word (e.g., â€œcommitâ€)\nExamine noun collocates: what does it commit? (crimes, suicide, resources, time)\nConclusion: Different senses have different prosodies\n\n2. Character construction\nHow are characters described through language patterns around their names?\n\nGenerate collocates for character names\nCompare verb patterns: Who does what? Who has things done to them?\nConclusion: Collocational profiles reveal agency, power, relationships\n\n3. Genre conventions\nWhat phraseological patterns define a genre?\n\nFind collocates of genre-specific terms\nCompare across corpora (romance vs.Â thriller, academic vs.Â news)\nConclusion: Genres have distinctive collocational fingerprints\n\n4. Diachronic change\nHow do word meanings shift through changing collocational patterns?\n\nTrack collocates of a word across time periods\nNote which partners appear/disappear\nConclusion: Meaning is usage, and usage patterns change\n\n\n\n6.5.2 Case Study: Narrative Agency in Twilight\nLetâ€™s explore power dynamics through pronoun-verb clusters:\nWho does things? Who has things done to them?\n\n# Actions Bella (I) performs toward Edward (him)\ni_verb_him = ds.clusters_by_token(twilight_tokens, \"I\", node_position=1, span=3)\ni_verb_him = i_verb_him.filter(pl.col(\"Token_3\") == \"him\")\n\n# Top 5 \"I [verb] him\" patterns\ni_verb_him.head(5)\n\n\nshape: (5, 9)\n\n\n\nToken_1\nToken_2\nToken_3\nTag_1\nTag_2\nTag_3\nAF\nRF\nRange\n\n\nstr\nstr\nstr\nstr\nstr\nstr\nu32\nf64\nf64\n\n\n\n\n\"i\"\n\"reminded\"\n\"him\"\n\"PPIS1\"\n\"VVD\"\n\"PPHO1\"\n12\n98.444576\n100.0\n\n\n\"i\"\n\"assured\"\n\"him\"\n\"PPIS1\"\n\"VVD\"\n\"PPHO1\"\n7\n57.426002\n100.0\n\n\n\"i\"\n\"warned\"\n\"him\"\n\"PPIS1\"\n\"VVD\"\n\"PPHO1\"\n6\n49.222288\n100.0\n\n\n\"i\"\n\"watched\"\n\"him\"\n\"PPIS1\"\n\"VVD\"\n\"PPHO1\"\n6\n49.222288\n100.0\n\n\n\"i\"\n\"told\"\n\"him\"\n\"PPIS1\"\n\"VVD\"\n\"PPHO1\"\n6\n49.222288\n100.0\n\n\n\n\n\n\n\n# Actions Edward (he) performs toward Bella (me)\nhe_verb_me = ds.clusters_by_token(twilight_tokens, \"he\", node_position=1, span=3)\nhe_verb_me = he_verb_me.filter(pl.col(\"Token_3\") == \"me\")\n\n# Top 5 \"he [verb] me\" patterns\nhe_verb_me.head(5)\n\n\nshape: (5, 9)\n\n\n\nToken_1\nToken_2\nToken_3\nTag_1\nTag_2\nTag_3\nAF\nRF\nRange\n\n\nstr\nstr\nstr\nstr\nstr\nstr\nu32\nf64\nf64\n\n\n\n\n\"he\"\n\"reminded\"\n\"me\"\n\"PPHS1\"\n\"VVD\"\n\"PPIO1\"\n9\n73.833432\n100.0\n\n\n\"he\"\n\"ignored\"\n\"me\"\n\"PPHS1\"\n\"VVD\"\n\"PPIO1\"\n7\n57.426002\n100.0\n\n\n\"he\"\n\"pulled\"\n\"me\"\n\"PPHS1\"\n\"VVD\"\n\"PPIO1\"\n6\n49.222288\n100.0\n\n\n\"he\"\n\"told\"\n\"me\"\n\"PPHS1\"\n\"VVD\"\n\"PPIO1\"\n6\n49.222288\n100.0\n\n\n\"he\"\n\"held\"\n\"me\"\n\"PPHS1\"\n\"VVD\"\n\"PPIO1\"\n5\n41.018573\n100.0\n\n\n\n\n\n\nQuestions to ask:\n\nAre the verbs symmetric? Or does one character primarily act while the other is acted upon?\nWhat verbs appear? Active (push, pull) vs.Â perceptual (look, see) vs.Â communicative (tell, ask)?\nWhat does this reveal about power, agency, or narrative focalization?\n\nInterpreting the patterns: If â€œI [verb] himâ€ shows mostly perceptual verbs (â€œsaw him,â€ â€œwatched himâ€) while â€œhe [verb] meâ€ shows speech acts (â€œtold me,â€ â€œasked meâ€), this suggests asymmetric communicative rolesâ€”one character observes while the other directs discourse. This isnâ€™t just about frequency; itâ€™s about what kinds of actions each character performs, which can reveal narrative power structures even in a first-person narration.\n\n\n6.5.3 Visualization: Comparing Collocational Profiles\nWe can visualize differences between node words by comparing their top collocates:\n\n# Get top nouns for each\nme_nouns = (me_collocates\n    .filter(pl.col(\"Tag\").str.starts_with(\"NN\"), pl.col(\"Freq Total\") &gt; 15)\n    .sort(\"MI\", descending=True)\n    .head(10))\n\nedward_nouns = (edward_collocates\n    .filter(pl.col(\"Tag\").str.starts_with(\"NN\"), pl.col(\"Freq Total\") &gt; 15)\n    .sort(\"MI\", descending=True)\n    .head(10))\n\n# Create side-by-side bar charts\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n\n# Me collocates\nme_tokens = me_nouns.get_column(\"Token\").to_list()\nme_mi = me_nouns.get_column(\"MI\").to_list()\nax1.barh(range(len(me_tokens)), me_mi, color='steelblue')\nax1.set_yticks(range(len(me_tokens)))\nax1.set_yticklabels(me_tokens)\nax1.set_xlabel('NPMI Score', fontsize=11)\nax1.set_title(\"Noun collocates of 'me'\", fontsize=12, weight='bold')\nax1.invert_yaxis()\n\n# Edward collocates  \nedward_tokens = edward_nouns.get_column(\"Token\").to_list()\nedward_mi = edward_nouns.get_column(\"MI\").to_list()\nax2.barh(range(len(edward_tokens)), edward_mi, color='darkred')\nax2.set_yticks(range(len(edward_tokens)))\nax2.set_yticklabels(edward_tokens)\nax2.set_xlabel('NPMI Score', fontsize=11)\nax2.set_title(\"Noun collocates of 'edward'\", fontsize=12, weight='bold')\nax2.invert_yaxis()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigureÂ 6.1: Comparing noun collocates of â€˜meâ€™ vs.Â â€˜edwardâ€™ (Freq Total &gt; 15)",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Collocations and N-grams</span>"
    ]
  },
  {
    "objectID": "tutorials/collocations.html#when-to-use-collocation-analysis",
    "href": "tutorials/collocations.html#when-to-use-collocation-analysis",
    "title": "6Â  Collocations and N-grams",
    "section": "6.6 When to Use Collocation Analysis",
    "text": "6.6 When to Use Collocation Analysis\n\n\n\n\n\n\nWell-Suited Research Questions\n\n\n\nCollocation analysis works best when you want to:\n\nExplore semantic prosody: What connotations does a word carry?\nStudy phraseology: What are the formulaic patterns in a genre or register?\nCharacter analysis: How are characters constructed through language around their names?\nCompare varieties: How do different authors, genres, or time periods use the same word differently?\nHypothesis generation: Find unexpected patterns to investigate further\n\n\n\n\n\n\n\n\n\nLimitations and Alternatives\n\n\n\nWhen collocation analysis struggles:\n\nFixed phrases: If you want exact sequences, use n-gram extraction (not collocation windows)\nGrammatical relations: Collocation doesnâ€™t capture syntaxâ€”use dependency parsing instead\nMeaning disambiguation: High-frequency words (like â€œgetâ€, â€œmakeâ€) have many senses; collocates mix them all\nDirectionality: Basic collocation treats left and right context equally; sometimes direction matters\n\nAlternative methods:\n\nUse clusters_by_token() for fixed n-grams\nUse dependency parsing for grammatical relationships\nUse keyword analysis (keyness) for comparing corpora\nUse embeddings (word2vec) for semantic similarity beyond co-occurrence",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Collocations and N-grams</span>"
    ]
  },
  {
    "objectID": "tutorials/collocations.html#common-pitfalls",
    "href": "tutorials/collocations.html#common-pitfalls",
    "title": "6Â  Collocations and N-grams",
    "section": "6.7 Common Pitfalls",
    "text": "6.7 Common Pitfalls\n1. Ignoring frequency thresholds\nA word appearing 3 times total can have a high MI score if 2 of those are near your node word. Always filter by Freq Total or Freq Span to ensure robustness.\n2. Misinterpreting association measures\nPMI/NPMI measures strength of association, not frequency. A collocate with NPMI = 0.8 but Freq Span = 2 is a strong but rare pairing. Consider both strength and frequency.\n3. Assuming causality or meaning\nCollocation shows co-occurrence patterns, not semantic relationships. â€œDoctorâ€ and â€œpatientâ€ collocate not because they mean the same thing, but because they appear in related contexts. Interpretation requires close reading.\n4. Over-relying on default spans\nThe 4-4 window is conventional, but not universal. Grammatical collocations (like verb-preposition pairs) might need 1-2 words; semantic associations might need 5-8. Experiment with span sizes.\n5. Treating all collocates equally\nPart-of-speech matters enormously. Nouns, verbs, adjectives, and function words reveal different patterns. Always filter by POS tags to focus analysis.\n6. Forgetting about directionality\nSome relationships are asymmetric. A word might appear frequently before your node but rarely after. Use position-specific n-grams when direction matters.",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Collocations and N-grams</span>"
    ]
  },
  {
    "objectID": "tutorials/collocations.html#what-to-do-after-finding-collocations",
    "href": "tutorials/collocations.html#what-to-do-after-finding-collocations",
    "title": "6Â  Collocations and N-grams",
    "section": "6.8 What to Do After Finding Collocations",
    "text": "6.8 What to Do After Finding Collocations\nCollocations are a discovery tool, not an endpoint. Hereâ€™s how to build on initial findings:\n\n6.8.1 1. Concordancing for Context\nUse KWIC (Key Words in Context) to see collocates in actual sentences:\n\n# See \"told me\" in context\nds.kwic_center_node(twilight_tokens, \"told me\")\n\n\nshape: (0, 4)\n\n\n\nDoc ID\nPre-Node\nNode\nPost-Node\n\n\nstr\nstr\nstr\nstr\n\n\n\n\n\n\n\n\nThis grounds statistical patterns in readable text.\n\n\n6.8.2 2. Expand to Related Words\nIf â€œtold meâ€ is frequent, check related verbs:\n\n# What other communication verbs appear with \"me\"?\nme_collocates.filter(\n    pl.col(\"Tag\").str.contains(\"VV\"),\n    pl.col(\"Token\").str.contains(\"(tell|ask|say|whisper|murmur)\")\n).head(10)\n\n\nshape: (10, 5)\n\n\n\nToken\nTag\nFreq Span\nFreq Total\nMI\n\n\nstr\nstr\nu32\nu32\nf64\n\n\n\n\n\"tell\"\n\"VV0\"\n23\n34\n0.463423\n\n\n\"telling\"\n\"VVG\"\n7\n13\n0.383602\n\n\n\"tell\"\n\"VVI\"\n20\n84\n0.336175\n\n\n\"ask\"\n\"VV0\"\n2\n5\n0.313005\n\n\n\"asked\"\n\"VVD\"\n37\n320\n0.272532\n\n\n\"ask\"\n\"VVI\"\n7\n45\n0.256442\n\n\n\"asked\"\n\"VVN\"\n2\n10\n0.250094\n\n\n\"say\"\n\"VV0\"\n3\n18\n0.242469\n\n\n\"whispered\"\n\"VVD\"\n6\n61\n0.206247\n\n\n\"murmured\"\n\"VVD\"\n4\n42\n0.195023\n\n\n\n\n\n\n\n\n6.8.3 3. Compare Across Subcorpora\nIf you have multiple books, authors, or time periods:\n\nGenerate collocates for each subcorpus separately\nCompare: which collocates are shared? Which are distinctive?\nUse keyness analysis to find subcorpus-specific collocates\n\n\n\n6.8.4 4. Network Analysis\nCollocations form networksâ€”words that collocate with each other create clusters:\n\nNode A collocates with B, B with C, C with A = semantic field\nMap these networks to visualize lexical domains\n\n\n\n6.8.5 5. Qualitative Deep Dive\nStatistics show you what patterns exist. Close reading explains why:\n\nRead passages containing key collocations\nConsider narrative function, character development, thematic resonance\nConnect computational patterns to literary interpretation",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Collocations and N-grams</span>"
    ]
  },
  {
    "objectID": "tutorials/collocations.html#conclusion",
    "href": "tutorials/collocations.html#conclusion",
    "title": "6Â  Collocations and N-grams",
    "section": "6.9 Conclusion",
    "text": "6.9 Conclusion\nCollocation analysis reveals the phraseological fabric of languageâ€”patterns that native speakers know implicitly but rarely articulate. By quantifying word attraction, we can:\n\nMake the invisible visible: Surface patterns that operate below conscious awareness\nScale interpretation: Analyze thousands of instances that close reading alone couldnâ€™t cover\nGenerate hypotheses: Find unexpected patterns worth investigating qualitatively\nCompare systematically: Contrast how different texts, authors, or time periods use the same words\n\nThe computational reasoning workflow:\n\nIdentify the pattern (high MI score, unusual collocates)\nExamine the evidence (check frequency thresholds, read concordances)\nAsk why it matters (connect to research question)\nContextualize (compare across corpora, consider genre/register)\nInterpret cautiously (collocation shows attraction, not causation or meaning)\n\nRemember: statistical association is the starting point for investigation, not the conclusion. The goal isnâ€™t to reduce texts to numbers, but to use numbers to ask better questions about texts.\n\nMake implicit patterns explicit\nCompare texts, authors, genres, or time periods systematically\n\nGenerate hypotheses about meaning, style, and usage\nGround interpretation in observable linguistic behavior\n\nThe power of collocation analysis lies not in replacing close reading, but in directing itâ€”showing you where to look more carefully in large corpora.\n\n\n\n\n\n\nConnecting Concepts\n\n\n\n\nFrom keyness to collocations: Keyness finds distinctive words; collocations find distinctive word combinations\nFrom collocations to n-grams: Collocation windows capture associations; n-grams capture fixed sequences\nFrom collocations to embeddings: Both measure word relationships, but embeddings (Tutorial 9) capture semantic similarity beyond direct co-occurrence",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Collocations and N-grams</span>"
    ]
  },
  {
    "objectID": "tutorials/collocations.html#further-reading",
    "href": "tutorials/collocations.html#further-reading",
    "title": "6Â  Collocations and N-grams",
    "section": "6.10 Further Reading",
    "text": "6.10 Further Reading\nFor deeper exploration of collocation methods and interpretation:\n\nStubbs, M. (2001). Words and Phrases: Corpus Studies of Lexical Semantics. Oxford: Blackwell. [Classic treatment of semantic prosody]\nSinclair, J. (1991). Corpus, Concordance, Collocation. Oxford: Oxford University Press. [Foundational work on phraseology]\nGries, S. Th. (2013). â€œ50-something years of work on collocations.â€ International Journal of Corpus Linguistics 18(1): 137-166. [Survey of measures and methods]",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>6</span>Â  <span class='chapter-title'>Collocations and N-grams</span>"
    ]
  },
  {
    "objectID": "tutorials/time-series.html",
    "href": "tutorials/time-series.html",
    "title": "7Â  Time Series Analysis",
    "section": "",
    "text": "7.1 Introduction\nLanguage changes over timeâ€”words rise and fall in popularity, meanings shift, new expressions emerge while old ones fade. Time series analysis tracks these changes systematically, revealing patterns that span decades or centuries. Unlike synchronic analysis (studying language at a single point in time), diachronic analysis examines language evolution through historical data.\nThis tutorial covers:",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/time-series.html#introduction",
    "href": "tutorials/time-series.html#introduction",
    "title": "7Â  Time Series Analysis",
    "section": "",
    "text": "Why Time Series Analysis Matters\n\n\n\nUnderstanding linguistic change reveals:\n\nSemantic drift: How word meanings evolve (e.g., â€œsillyâ€ from â€œblessedâ€ to â€œfoolishâ€ through a process called pejoration)\nCultural shifts: Frequency changes reflecting social movements (e.g., â€œchairpersonâ€ vs.Â â€œchairman,â€ etc.)\nGenre evolution: How disciplinary or literary conventions change\nPeriodization: Natural boundaries in linguistic history vs.Â arbitrary century divisions\nHypothesis testing: Validating claims about historical trends with quantitative evidence\n\n\n\n\n\nGoogle Ngrams: Accessing and interpreting historical frequency data\nVisualization: Scatterplots, smoothing, and trend interpretation\nVariability-Based Neighbor Clustering (VNC): Data-driven periodization\nCritical evaluation: Assessing methodological claims in published research",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/time-series.html#understanding-google-ngrams",
    "href": "tutorials/time-series.html#understanding-google-ngrams",
    "title": "7Â  Time Series Analysis",
    "section": "7.2 Understanding Google Ngrams",
    "text": "7.2 Understanding Google Ngrams\n\n7.2.1 The Data Source\nGoogleâ€™s Ngram database contains word frequencies from millions of digitized books (1500-2019), available at:\nhttp://storage.googleapis.com/books/ngrams/books/datasetsv2.html\nKey characteristics:\n\nMassive scale: 8+ million books, multiple language varietiesâ€”unparalleled temporal and lexical coverage\nSparse early data: Pre-1800 data is limited and OCR quality varies\nVariety-specific: Can filter by British English, American English, fiction, etc.\nN-gram support: 1-grams (single words) through 5-grams (five-word sequences)\nNormalized frequencies: Counts adjusted for corpus size changes over time\n\nCritical limitations:\n\nNo sampling frame: We donâ€™t know the population being sampledâ€”what proportion of all published books? Which publishers, regions, or topics are over/under-represented?\nNo metadata: Limited information about author demographics, book genres, publication contexts, or intended audiences\nSelection bias: What gets digitized reflects library holdings, copyright status, and Googleâ€™s partnershipsâ€”not a representative sample of published materials\nCopyright gaps: 20th-century coverage has holes due to copyright restrictions\nNot representative of spoken language: Books â‰  everyday speech\nGenre imbalance over time: Earlier periods overrepresent religious and legal texts; later periods tilt toward popular fiction and self-help\nOCR errors: More common in older materials (damaged pages, archaic typefaces, non-standard fonts)\nFrequency â‰  cultural salience: A rare word can be culturally important; a frequent word can be semantically empty",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/time-series.html#fetching-and-exploring-data",
    "href": "tutorials/time-series.html#fetching-and-exploring-data",
    "title": "7Â  Time Series Analysis",
    "section": "7.3 Fetching and Exploring Data",
    "text": "7.3 Fetching and Exploring Data\n\n7.3.1 Basic Data Retrieval\nThe google_ngram() function streams data directly from Googleâ€™s repositories, filtering as it downloads to minimize memory usage:\n\n# Fetch lemmatized forms of \"quiz\"\nword_forms = [\"quiz\", \"quizzes\", \"quizzed\"]\nquiz_year = google_ngram(word_forms, variety=\"eng\", by=\"year\")\nquiz_year.head(10)\n\n\nshape: (10, 4)\n\n\n\nAF\nToken\nYear\nRF\n\n\ni64\nlist[str]\ni32\nf64\n\n\n\n\n0\n[\"quiz\", \"quizzes\", \"quizzed\"]\n1505\n0.0\n\n\n0\n[\"quiz\", \"quizzes\", \"quizzed\"]\n1506\n0.0\n\n\n0\n[\"quiz\", \"quizzes\", \"quizzed\"]\n1507\n0.0\n\n\n0\n[\"quiz\", \"quizzes\", \"quizzed\"]\n1508\n0.0\n\n\n0\n[\"quiz\", \"quizzes\", \"quizzed\"]\n1509\n0.0\n\n\n0\n[\"quiz\", \"quizzes\", \"quizzed\"]\n1510\n0.0\n\n\n0\n[\"quiz\", \"quizzes\", \"quizzed\"]\n1511\n0.0\n\n\n0\n[\"quiz\", \"quizzes\", \"quizzed\"]\n1512\n0.0\n\n\n0\n[\"quiz\", \"quizzes\", \"quizzed\"]\n1513\n0.0\n\n\n0\n[\"quiz\", \"quizzes\", \"quizzed\"]\n1514\n0.0\n\n\n\n\n\n\nUnderstanding the output:\n\nYear: Temporal unit (year or decade depending on by parameter)\nAF: Absolute frequency (raw count of occurrences)\nTotal: Total word count in corpus for that period\nRF: Relative frequency (per million words)\n\nThe function combines all forms in word_forms into a single frequency countâ€”effectively lemmatizing the data.\n\n\n7.3.2 Aggregation Choices\nYou can aggregate by year or decade:\n\n# Decade aggregation reduces noise\nquiz_decade = google_ngram(word_forms, variety=\"eng\", by=\"decade\")\nquiz_decade.head(10)\n\n\nshape: (10, 4)\n\n\n\nToken\nAF\nRF\nDecade\n\n\nlist[str]\ni64\nf64\ni32\n\n\n\n\n[\"quiz\", \"quizzes\", \"quizzed\"]\n0\n0.0\n1500\n\n\n[\"quiz\", \"quizzes\", \"quizzed\"]\n1\n3.460076\n1510\n\n\n[\"quiz\", \"quizzes\", \"quizzed\"]\n0\n0.0\n1520\n\n\n[\"quiz\", \"quizzes\", \"quizzed\"]\n0\n0.0\n1530\n\n\n[\"quiz\", \"quizzes\", \"quizzed\"]\n0\n0.0\n1540\n\n\n[\"quiz\", \"quizzes\", \"quizzed\"]\n0\n0.0\n1550\n\n\n[\"quiz\", \"quizzes\", \"quizzed\"]\n0\n0.0\n1560\n\n\n[\"quiz\", \"quizzes\", \"quizzed\"]\n0\n0.0\n1570\n\n\n[\"quiz\", \"quizzes\", \"quizzed\"]\n0\n0.0\n1580\n\n\n[\"quiz\", \"quizzes\", \"quizzed\"]\n0\n0.0\n1590\n\n\n\n\n\n\nWhen to use year vs.Â decade:\n\nBy year: Maximum temporal resolution, reveals short-term volatility, useful for modern data (post-1900)\nBy decade: Smooths noise, better for long-term trends, necessary for sparse earlier data",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/time-series.html#visualization-and-trend-analysis",
    "href": "tutorials/time-series.html#visualization-and-trend-analysis",
    "title": "7Â  Time Series Analysis",
    "section": "7.4 Visualization and Trend Analysis",
    "text": "7.4 Visualization and Trend Analysis\n\n7.4.1 Scatterplot with Smoothing\nTime series analysis starts with visualization. For by-year data, scatterplots show both raw variation and underlying trends:\n\nquiz_year_ts = TimeSeries(time_series=quiz_year, time_col='Year', values_col='RF')\nquiz_year_ts.timeviz_scatterplot();\n\n\n\n\n\n\n\n\nKey observations:\n\nEarly volatility (pre-1800): Wide scatter, extreme outliersâ€”reflects sparse data and OCR unreliability\nSmoothing line: LOESS (locally estimated scatterplot smoothing) or GAM (generalized additive model) fit showing underlying trend beneath yearly noise. Think of it as a moving average that adapts to local patterns.\nModern surge (post-1950): Dramatic frequency increaseâ€”but what does it mean?\n\n\n\n\n\n\n\nComparing to Googleâ€™s Ngram Viewer\n\n\n\nGoogleâ€™s online interface provides a different visualization:\n\nfrom IPython.display import IFrame\nIFrame(\"https://books.google.com/ngrams/interactive_chart?content=quiz,+quizzes,+quizzed&year_start=1500&year_end=2022&corpus=en&smoothing=3\", width=900, height=500)\n\n\n        \n        \n\n\nNote the differences: Google applies 3-year smoothing by default, hiding year-to-year variability. Which view is more â€œaccurateâ€? Neitherâ€”they emphasize different aspects of the same data.\n\n\n\n\n7.4.2 Triangulation with External Sources\nFrequency data alone doesnâ€™t explain linguistic history. Always triangulate with other sources:\n\nOxford English Dictionary: Check first attestation dates, sense developments\nHistorical events: Wars, social movements, technological innovations\nGenre shifts: Changes in what kinds of books get published\n\nFor â€œquizâ€: The OED shows first use ~1780s as a noun meaning â€œodd person,â€ later â€œtest/examination.â€ The modern surge post-1950 likely reflects the rise of educational testing culture, not the wordâ€™s initial coinage.",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/time-series.html#computational-reasoning-from-patterns-to-insights",
    "href": "tutorials/time-series.html#computational-reasoning-from-patterns-to-insights",
    "title": "7Â  Time Series Analysis",
    "section": "7.5 Computational Reasoning: From Patterns to Insights",
    "text": "7.5 Computational Reasoning: From Patterns to Insights\n\n7.5.1 The Interpretation Workflow\n1. Observe the pattern: Quiz frequencies rise dramatically after 1950\n2. Filter and focus: Remove sparse early data (pre-1800) to avoid OCR noise\n\nquiz_decade = quiz_decade.filter(pl.col(\"Decade\") &gt;= 1800)\nquiz_decade_ts = TimeSeries(time_series=quiz_decade, time_col='Decade', values_col='RF')\n\n3. Ask why it matters: - Does this reflect educational expansion (more schools = more quizzes)? - Genre shift (rise of textbooks and educational publishing)? - Semantic change (new meanings emerge)? - Publication bias (earlier books less likely to discuss pedagogy)?\n4. Test competing hypotheses: Compare against related words (â€œtest,â€ â€œexam,â€ â€œassessmentâ€), check different corpora (fiction vs.Â academic), examine collocates",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/time-series.html#periodization-data-driven-historical-boundaries",
    "href": "tutorials/time-series.html#periodization-data-driven-historical-boundaries",
    "title": "7Â  Time Series Analysis",
    "section": "7.6 Periodization: Data-Driven Historical Boundaries",
    "text": "7.6 Periodization: Data-Driven Historical Boundaries\n\n7.6.1 The Problem with Arbitrary Bins\nTraditional periodization uses convenient but arbitrary divisions: centuries, half-centuries, decades. These bins assume linguistic change aligns with calendar boundariesâ€”but does it?\nVariability-Based Neighbor Clustering (VNC) (gries2008variability?; Gries and Hilpert 2012) offers an alternative: let the data reveal its own period structure through hierarchical clustering based on frequency similarity.\n\n\n7.6.2 VNC Methodology\nCore idea: Time periods with similar frequencies cluster together. Abrupt frequency changes create cluster boundaries.\nWhy temporal order matters: Unlike standard hierarchical clustering, VNC cannot rearrange time points. The year 1850 must stay between 1840 and 1860. This constraint ensures clusters represent actual historical periods, not just similar frequencies from disconnected eras. Itâ€™s clustering within the flow of time, not clustering that ignores temporal sequence.\nStep 1: Calculate pairwise distances between adjacent time points based on standard deviation of frequencies\nStep 2: Hierarchically cluster time points, maintaining temporal order (unlike standard clustering)\nStep 3: Cut the dendrogram at a height that creates meaningful periods\n\n\n7.6.3 Scree Plots and Cluster Selection\nHow many clusters (periods) should you create? The scree plot shows variance explained by different cluster counts:\n\nquiz_decade_ts.timeviz_screeplot();\n\n\n\n\n\n\n\n\nInterpreting scree plots:\n\nElbow: Point where adding more clusters shows diminishing returns\nNo definitive cutoff: You must make an analytical judgment\nContext matters: Known historical events, research questions, and interpretive goals guide decisions\n\n\n\n7.6.4 Dendrogram Visualization\n\nquiz_decade_ts.timeviz_vnc();\n\n\n\n\n\n\n\n\nReading dendrograms:\n\nVertical axis: Distance (dissimilarity) between clusters\nHorizontal axis: Time periods (decades) in order\nHeight of joins: Larger distance = more dissimilar periods being merged\nLeaf clustering: Decades that cluster early are highly similar\n\nLooking at the dendrogram and scree plot together suggests 3-4 natural periods for â€œquizâ€ usage from 1800-2020.\n\n\n7.6.5 Cutting and Periodizing\nYou can visualize cluster boundaries directly:\n\n# Add cutline showing 4-cluster solution\nquiz_decade_ts.timeviz_vnc(n_periods=4, cut_line=True);\n\n\n\n\n\n\n\n\nOr simplify by collapsing leaves into period labels:\n\n# Vertical orientation with periodized leaves\nquiz_decade_ts.timeviz_vnc(n_periods=4, periodize=True, orientation=\"vertical\");\n\n\n\n\n\n\n\n\nAnd inspect cluster contents:\n\nquiz_decade_ts.cluster_summary()\n\nCluster 1 (n=14): ['1800', '1810', '1820', '1830', '1840', '1850', '1860', '1870', '1880', '1890', '1900', '1910', '1920', '1930']\nCluster 2 (n=5): ['1940', '1950', '1960', '1970', '1980']\nCluster 3 (n=1): ['1990']\nCluster 4 (n=1): ['2000']\n\n\nInterpreting results: Each cluster represents a distinct â€œperiodâ€ of usage. If clusters align with known events (1940s-1950s = post-WWII educational expansion), that strengthens interpretation. If they donâ€™t align, youâ€™ve discovered something new about the dataâ€™s structure.",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/time-series.html#case-study-pronouns-and-individualism",
    "href": "tutorials/time-series.html#case-study-pronouns-and-individualism",
    "title": "7Â  Time Series Analysis",
    "section": "7.7 Case Study: Pronouns and Individualism",
    "text": "7.7 Case Study: Pronouns and Individualism\n\n7.7.1 The Research Question\nTwenge et al.Â (2013) claimed that pronoun frequency changes in American books (1960-2008) reflect increasing individualism:\n\nâ€œThe use of first person plural pronouns (e.g., we, us) decreased 10%, first person singular pronouns (I, me) increased 42%, and second person pronouns (you, your) quadrupled.â€\n\nThe interpretation: These linguistic shifts mirror psychological changesâ€”Americans becoming more individualistic, less collectivist.\nCritical questions:\n\nDoes extending the time frame (1800-2008) change the story?\nAre pronoun frequencies a valid proxy for cultural psychology?\nWhat alternative explanations might account for the same patterns?\n\n\n\n7.7.2 Replicating and Extending the Analysis\nLoad preprocessed pronoun data:\n\nall_pronouns = pl.read_csv(\"https://github.com/browndw/humanities_analytics/raw/refs/heads/main/data/data_tables/pronoun_frequencies.csv\")\nall_pronouns.head()\n\n\nshape: (5, 7)\n\n\n\nYear\nAF_first_pl\nRF_first_pl\nAF_first_sing\nRF_first_sing\nAF_second\nRF_second\n\n\ni64\ni64\nf64\ni64\nf64\ni64\nf64\n\n\n\n\n1800\n104442\n5672.49\n236846\n12863.66\n98483\n5348.84\n\n\n1801\n126814\n6359.38\n200603\n10059.71\n78919\n3957.58\n\n\n1802\n115818\n4958.84\n162318\n6949.77\n69956\n2995.22\n\n\n1803\n170391\n6102.23\n183530\n6572.78\n91388\n3272.89\n\n\n1804\n134866\n3743.74\n170764\n4740.23\n80542\n2235.76\n\n\n\n\n\n\nThe original study used a line plot for 1960-2008. Letâ€™s recreate it with the full 1800-2008 time frame:\n\n# Prepare data for matplotlib\nx = all_pronouns[\"Year\"].to_numpy()\ny_first_pl = all_pronouns[\"RF_first_pl\"].to_numpy()\ny_first_sing = all_pronouns[\"RF_first_sing\"].to_numpy()\ny_second = all_pronouns[\"RF_second\"].to_numpy()\n\n# Create multi-line plot\nplt.figure(figsize=(10, 6))\nplt.plot(x, y_first_pl, label='First Person Plural (we, us)', color='steelblue', linewidth=1.5)\nplt.plot(x, y_first_sing, label='First Person Singular (I, me)', color='darkred', linewidth=1.5)\nplt.plot(x, y_second, label='Second Person (you, your)', color='darkgreen', linewidth=1.5)\n\nplt.xlabel('Year', fontsize=12)\nplt.ylabel('Relative Frequency (per million words)', fontsize=12)\nplt.title('Pronoun Frequencies in American English Books (1800-2008)', fontsize=14, weight='bold')\nplt.legend(fontsize=10)\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n7.7.3 Evaluating the Claims\nWhat the extended time frame reveals:\n\nLong-term context: The 1960-2008 trend may be part of a longer cyclical pattern, not a unidirectional shift. Do pronoun frequencies show a steady rise from 1800, or fluctuations with peaks and troughs?\nGenre effects: What if the rise of self-help books, memoirs, and first-person fiction drives â€œIâ€ increases, not cultural individualism? The rise of confessional writing â‰  the rise of individualistic psychology.\nPublication bias: What types of books get digitized and included in Google Books? If Google preferentially digitizes bestsellers and popular nonfiction (heavy on first-person), that skews the data.\nBaseline assumption: The study assumes 1960 represents â€œnormalâ€ collectivism. But what if 1960 was an anomaly (post-war conformity), and later decades represent a return to historical norms?\n\nMethodological strengths:\n\nLarge-scale data (hundreds of thousands of books)\nClear quantitative pattern (pronouns do change)\nReplicable analysis\n\nMethodological weaknesses:\n\nCorrelation â‰  causation (linguistic change and psychological change could both be caused by a third factor)\nNo validation with other cultural measures\nAssumes books reflect individual psychology (do they?)\nCherry-picked time frame (1960-2008) may exaggerate trend\n\n\n\n\n\n\n\nThe Computational Reasoning Lesson\n\n\n\nTime series data is powerful for description but weak for explanation. Frequencies can reveal patterns, but interpreting those patterns requires:\n\nTriangulation: Multiple data sources beyond word counts\nAlternative hypotheses: Genre, publication bias, semantic shift\nContextualization: Historical events, literary movements, technological changes\nHumility: Acknowledge what your data can and cannot show\n\nThe goal isnâ€™t to dismiss quantitative analysisâ€”itâ€™s to use it more carefully, recognizing both its power and its limits.",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/time-series.html#when-to-use-time-series-analysis",
    "href": "tutorials/time-series.html#when-to-use-time-series-analysis",
    "title": "7Â  Time Series Analysis",
    "section": "7.8 When to Use Time Series Analysis",
    "text": "7.8 When to Use Time Series Analysis\n\n\n\n\n\n\nWell-Suited Research Questions\n\n\n\n\nTracking lexical innovation: When did a new word/phrase emerge and spread?\nSemantic change: How do word frequencies correlate with meaning shifts?\nGenre evolution: How do disciplinary conventions change over time?\nValidating periodization: Do linguistic patterns align with literary/historical periods?\nCultural hypothesis testing: Do proposed social changes have linguistic correlates?\n\n\n\n\n\n\n\n\n\nLimitations and Alternatives\n\n\n\nWhen time series analysis struggles:\n\nCausation claims: Frequency correlation doesnâ€™t prove cultural causation\nRepresentativeness: Google Books isnâ€™t representative of all language use\nSemantic conflation: High-frequency words have multiple senses; ngrams canâ€™t distinguish them\nRare phenomena: Low-frequency words/phrases produce unreliable time series\n\nAlternative methods:\n\nClose reading: Examine actual usage contexts, not just frequencies\nCollocation analysis: Track what words appear with your target over time\nCorpus comparison: Compare multiple corpora (spoken vs.Â written, genres, regions)\nQualitative periodization: Literary history, OED citations, historical events",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/time-series.html#common-pitfalls",
    "href": "tutorials/time-series.html#common-pitfalls",
    "title": "7Â  Time Series Analysis",
    "section": "7.9 Common Pitfalls",
    "text": "7.9 Common Pitfalls\n1. Ignoring data quality variation over time\nPre-1800 data is sparse and unreliable. Always check scatterplots for volatility and consider filtering early years.\n2. Over-interpreting short time frames\nThe 1960-2008 window may exaggerate trends. Extend time frames when possible to see if patterns are long-term or cyclical.\n3. Assuming smoothing reveals â€œtruthâ€\nSmoothing highlights trends but hides genuine volatility. Choose smoothing parameters based on your question, not aesthetics.\n4. Conflating frequency with importance\nA rare word can be culturally significant. A frequent word can be semantically empty. Frequency is one dimension of importance, not the only one.\n5. Neglecting genre and publication bias\nWhat gets published (and digitized) changes over time. The rise of memoirs, self-help, and creative nonfiction affects pronoun frequencies independent of cultural psychology.\n6. Making causal claims from correlation\nLinguistic change and cultural change may correlate without one causing the other. Both could be caused by third variables (education policy, media technology, economic shifts).",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/time-series.html#what-to-do-after-finding-a-trend",
    "href": "tutorials/time-series.html#what-to-do-after-finding-a-trend",
    "title": "7Â  Time Series Analysis",
    "section": "7.10 What to Do After Finding a Trend",
    "text": "7.10 What to Do After Finding a Trend\nTime series analysis generates hypotheses, not conclusions. Hereâ€™s how to build on initial findings:\n\n7.10.1 1. Triangulate with Other Data Sources\n\nOED citations: Check historical attestations and sense developments\nCOHA (Corpus of Historical American English): Compare against a more balanced historical corpus\nHistorical archives: Newspapers, diaries, letters for validation\nCultural histories: Books on the relevant time period/topic\n\n\n\n7.10.2 2. Examine Collocational Changes\nIf â€œquizâ€ frequencies rise post-1950, what does â€œquizâ€ collocate with over time?\n\nPre-1950: â€œquiz night,â€ â€œquiz masterâ€ (entertainment)\nPost-1950: â€œquiz question,â€ â€œquiz gradeâ€ (education)\n\nCollocation shifts reveal how a word is being used, not just how often.\n\n\n7.10.3 3. Compare Across Varieties and Genres\n\nBritish vs.Â American English: Do trends differ?\nFiction vs.Â academic writing: Genre-specific patterns?\nTime-matched subcorpora: Control for publication type\n\n\n\n7.10.4 4. Apply VNC to Multiple Words\nDoes â€œtest,â€ â€œexam,â€ and â€œassessmentâ€ show the same periodization as â€œquizâ€? If yes, strengthens interpretation. If no, â€œquizâ€ has a unique trajectory worth investigating.\n\n\n7.10.5 5. Qualitative Deep Dive\nRead actual examples from high-frequency periods:\n\nWhat senses dominate in each period?\nWhat genres use the word most?\nAre there usage patterns invisible to frequency counts?",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/time-series.html#conclusion",
    "href": "tutorials/time-series.html#conclusion",
    "title": "7Â  Time Series Analysis",
    "section": "7.11 Conclusion",
    "text": "7.11 Conclusion\nTime series analysis transforms language history from anecdote to evidence. By tracking frequencies across decades or centuries, we can:\n\nValidate or challenge claims about linguistic change\nDiscover unexpected patterns worth investigating\nPeriodize language history based on data, not calendar convenience\nConnect linguistic shifts to cultural, technological, and social changes\n\nThe computational reasoning workflow:\n\nVisualize the trend (scatterplot, smoothing, aggregation)\nFilter and focus (remove sparse/noisy data, select meaningful time frames)\nPeriodize (VNC for data-driven boundaries)\nInterpret cautiously (frequency shows what changed, not why)\nTriangulate (external sources, alternative explanations, qualitative validation)\n\nRemember: quantitative analysis reveals patterns that demand explanation. The numbers show us where to lookâ€”close reading, historical context, and theoretical frameworks explain what we find.\n\n\n\n\n\n\nConnecting to Mini Lab 7\n\n\n\nMini Lab 7: Time Series provides hands-on practice with Google Ngrams and VNC analysis using Google Colab. The mini lab includes the pronoun case study and guides you through creating publication-quality visualizations.",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/time-series.html#works-cited",
    "href": "tutorials/time-series.html#works-cited",
    "title": "7Â  Time Series Analysis",
    "section": "7.12 Works Cited",
    "text": "7.12 Works Cited\n\n\n\n\nGries, Stefan, and Martin Hilpert. 2012. â€œVariability-Based Neighbor Clustering.â€ The Oxford Handbook of the History of English, 134â€“44. https://www.stgries.info/research/2012_STG-MH_VarNeighbClustering_OxfHBHistEngl.pdf.",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>7</span>Â  <span class='chapter-title'>Time Series Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/topic-modeling.html",
    "href": "tutorials/topic-modeling.html",
    "title": "8Â  Topic Modeling",
    "section": "",
    "text": "8.1 Introduction\nImagine reading 10,000 newspaper articles about climate change. You want to know: What themes recur? Which articles discuss policy vs.Â science vs.Â activism? How do themes shift over time?\nTopic modeling is an unsupervised machine learning technique that discovers abstract â€œtopicsâ€ in large text collections. Unlike supervised methods (where you pre-define categories), topic models infer thematic structures directly from word co-occurrence patterns.\nHow it works (simplified):\nThe most common algorithm is LDA (Latent Dirichlet Allocation), developed by Blei, Ng, and Jordan (2003).\nResearch questions topic modeling can help answer:\nWhat topic modeling cannot answer:",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Topic Modeling</span>"
    ]
  },
  {
    "objectID": "tutorials/topic-modeling.html#introduction",
    "href": "tutorials/topic-modeling.html#introduction",
    "title": "8Â  Topic Modeling",
    "section": "",
    "text": "Assume each document is a mixture of topics (a speech might be 60% â€œeconomy,â€ 30% â€œforeign policy,â€ 10% â€œeducationâ€)\nAssume each topic is a probability distribution over all words in the vocabulary\nUse an algorithm to infer both distributions from the data\n\n\n\n\n\n\n\n\nWhy â€œLatentâ€?\n\n\n\nTopics are latent (hidden) structures. They donâ€™t exist explicitly in the textâ€”the algorithm infers them from patterns of which words tend to co-occur across documents. This makes topic modeling powerful (discovers unexpected patterns) but also interpretively risky (topics are statistical artifacts, not authorial intentions).\n\n\n\n\nWhat themes appear in a corpus of historical newspapers?\nHow do topics shift over time (e.g., Cold War discourse 1950-1990)?\nWhich documents are thematically similar despite surface differences?\nWhat distinguishes one authorâ€™s corpus from anotherâ€™s in terms of topical focus?\n\n\n\nQuestions requiring fine-grained semantic nuance (metaphor, irony, tone)\nAuthorship attribution (unless topics correlate strongly with style)\nCausal claims (â€œThis topic caused this historical eventâ€)\nInterpretation without domain knowledge (topics are numbers until you make sense of them)",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Topic Modeling</span>"
    ]
  },
  {
    "objectID": "tutorials/topic-modeling.html#understanding-lda",
    "href": "tutorials/topic-modeling.html#understanding-lda",
    "title": "8Â  Topic Modeling",
    "section": "8.2 Understanding LDA",
    "text": "8.2 Understanding LDA\n\n8.2.1 The Generative Story\nLDA assumes documents are generated through this process:\n\nChoose topic proportions for the document (e.g., 40% topic A, 30% topic B, 30% topic C)\nFor each word position:\n\nPick a topic according to those proportions\nPick a word from that topicâ€™s distribution\n\n\nThe algorithm reverses this: given observed words, infer the hidden topic assignments and distributions.\n\n\n8.2.2 Key Concepts\nTopics: Probability distributions over words. Topic 1 might assign high probability to â€œgovernment,â€ â€œcongress,â€ â€œlaw,â€ â€œpolicyâ€ (a â€œgovernanceâ€ topic).\nDocument-topic distribution: Each documentâ€™s mixture of topics. Document A might be 80% Topic 1, 20% Topic 2.\nTopic-word distribution: Each topicâ€™s mixture of words. Topic 1 might assign 0.08 probability to â€œgovernment,â€ 0.06 to â€œcongress,â€ etc.\nHyperparameters: - Î± (alpha): Controls document-topic sparsity. Low Î± â†’ documents concentrate in few topics. - Î² (beta/eta): Controls topic-word sparsity. Low Î² â†’ topics concentrate in few words. - k: Number of topics. Must be set in advance (or tested via evaluation).\n\n\n8.2.3 What Makes a Good Topic?\nCoherence: Top words should be semantically related. A topic with â€œschool,â€ â€œstudent,â€ â€œteacher,â€ â€œeducationâ€ is coherent. A topic with â€œschool,â€ â€œdog,â€ â€œeconomy,â€ â€œyesterdayâ€ is incoherent (probably noise).\nDistinctiveness: Topics should capture different themes, not overlap heavily.\nInterpretability: Humans can assign meaningful labels (â€œeducation,â€ â€œwar,â€ â€œeconomyâ€).\nStatistical note: These are interpretive goals, not guaranteed algorithmic outcomes. LDA optimizes for likelihood, not human interpretability.",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Topic Modeling</span>"
    ]
  },
  {
    "objectID": "tutorials/topic-modeling.html#workflow-from-texts-to-topics",
    "href": "tutorials/topic-modeling.html#workflow-from-texts-to-topics",
    "title": "8Â  Topic Modeling",
    "section": "8.3 Workflow: From Texts to Topics",
    "text": "8.3 Workflow: From Texts to Topics\nWeâ€™ll analyze U.S. Presidential Inaugural Addresses to discover thematic patterns.\n\n8.3.1 Load Corpus\n\ncorp = Corpus.from_tabular(\n    \"https://github.com/browndw/humanities_analytics/raw/refs/heads/main/data/data_tables/inaugural_subset.csv\",\n    text_column='text', \n    id_column='year', \n    language='en'\n)\nprint_summary(corp)\n\nCorpus with 32 documents in English\n&gt; inaugural_subset-1913 (1901 tokens): There has been a change of government . It began t...\n&gt; inaugural_subset-1921 (3753 tokens): My Countrymen : When one surveys the world about h...\n&gt; inaugural_subset-1929 (3891 tokens): My Countrymen : This occasion is not alone the adm...\n&gt; inaugural_subset-1925 (4442 tokens): My countrymen , no one can contemplate current con...\n&gt; inaugural_subset-1933 (2063 tokens): I am certain that my fellow Americans expect that ...\n&gt; inaugural_subset-1909 (5845 tokens): My fellow citizens : Anyone who has taken the oath...\n&gt; inaugural_subset-1905 (1090 tokens): My fellow citizens , no people on earth have more ...\n&gt; inaugural_subset-1937 (2021 tokens): When four years ago we met to inaugurate a Preside...\n&gt; inaugural_subset-1901 (2451 tokens): My fellow - citizens , when we assembled here on t...\n&gt; inaugural_subset-1917 (1656 tokens): My Fellow citizens : The four years which have ela...\n(and 22 more documents)\ntotal number of tokens: 77593 / vocabulary size: 7108\n\n\n\n\n8.3.2 Preprocessing Pipeline\nWhy preprocess aggressively?\nTopic modeling relies on word co-occurrence. Preprocessing removes noise (punctuation, ultra-common words, typos) to surface meaningful patterns.\n\n# Lemmatize to group inflected forms\nlemmatize(corp)\n# Lowercase for consistency\nto_lowercase(corp)\n# Remove punctuation\nremove_punctuation(corp)\n\nprint_summary(corp)\n\nCorpus with 32 documents in English\n&gt; inaugural_subset-1913 (1901 tokens): there have be a change of government  it begin two...\n&gt; inaugural_subset-1921 (3753 tokens): my countryman  when one survey the world about he ...\n&gt; inaugural_subset-1929 (3891 tokens): my countryman  this occasion be not alone the admi...\n&gt; inaugural_subset-1925 (4442 tokens): my countryman  no one can contemplate current cond...\n&gt; inaugural_subset-1933 (2063 tokens): i be certain that my fellow americans expect that ...\n&gt; inaugural_subset-1909 (5845 tokens): my fellow citizen  anyone who have take the oath i...\n&gt; inaugural_subset-1905 (1090 tokens): my fellow citizen  no people on earth have more ca...\n&gt; inaugural_subset-1937 (2021 tokens): when four year ago we meet to inaugurate a preside...\n&gt; inaugural_subset-1901 (2451 tokens): my fellow  citizen  when we assemble here on the 4...\n&gt; inaugural_subset-1917 (1656 tokens): my fellow citizen  the four year which have elapse...\n(and 22 more documents)\ntotal number of tokens: 77593 / vocabulary size: 5040\n\n\nCritical preprocessing choice: Should we filter by part-of-speech?\n\n# Create noun-only version\ncorp_nouns = copy(corp)\nfilter_for_pos(corp_nouns, 'N')\nfilter_clean_tokens(corp_nouns, remove_shorter_than=2)\nremove_common_tokens(corp_nouns, df_threshold=0.8)\nremove_uncommon_tokens(corp_nouns, df_threshold=0.1)\n\nprint_summary(corp_nouns)\n\nCorpus with 32 documents in English\n&gt; inaugural_subset-1913 (239 tokens): change house majority senate office president vice...\n&gt; inaugural_subset-1921 (488 tokens): countryman storm destruction thing american passio...\n&gt; inaugural_subset-1929 (603 tokens): countryman occasion administration oath dedication...\n&gt; inaugural_subset-1925 (533 tokens): countryman condition result conflict burden effect...\n&gt; inaugural_subset-1933 (291 tokens): americans presidency decision truth truth conditio...\n&gt; inaugural_subset-1909 (772 tokens): oath weight responsibility conception duty office ...\n&gt; inaugural_subset-1905 (127 tokens): earth cause spirit strength good condition measure...\n&gt; inaugural_subset-1937 (263 tokens): president republic anxiety spirit fulfillment visi...\n&gt; inaugural_subset-1901 (311 tokens): fellow march anxiety regard currency credit obliga...\n&gt; inaugural_subset-1917 (178 tokens): place counsel action interest consequence period h...\n(and 22 more documents)\ntotal number of tokens: 9583 / vocabulary size: 646\n\n\nEffect of noun-only filtering: - Pro: Topics focus on entities, concepts, places (clearer semantic themes) - Con: Lose verbs (actions), adjectives (qualities), function words (rhetorical patterns)\nCompare to full-vocabulary version:\n\ncorp_full = copy(corp)\nfilter_clean_tokens(corp_full, remove_shorter_than=2)\nremove_common_tokens(corp_full, df_threshold=0.85)\nremove_uncommon_tokens(corp_full, df_threshold=0.05)\n\nprint_summary(corp_full)\n\nCorpus with 32 documents in English\n&gt; inaugural_subset-1913 (517 tokens): change begin ago house democratic decisive majorit...\n&gt; inaugural_subset-1921 (1194 tokens): countryman survey storm note mark destruction rejo...\n&gt; inaugural_subset-1929 (1264 tokens): countryman occasion administration sacred oath ass...\n&gt; inaugural_subset-1925 (1354 tokens): countryman contemplate current condition find sati...\n&gt; inaugural_subset-1933 (682 tokens): certain fellow americans expect induction presiden...\n&gt; inaugural_subset-1909 (1862 tokens): fellow take oath take feel heavy weight responsibi...\n&gt; inaugural_subset-1905 (299 tokens): fellow earth cause thankful say reverently spirit ...\n&gt; inaugural_subset-1937 (633 tokens): ago meet inaugurate president republic single mind...\n&gt; inaugural_subset-1901 (759 tokens): fellow assemble march anxiety regard currency cred...\n&gt; inaugural_subset-1917 (460 tokens): fellow place crowd counsel action vital interest c...\n(and 22 more documents)\ntotal number of tokens: 23025 / vocabulary size: 2439\n\n\n\n\n8.3.3 Create Document-Term Matrix\n\ndtm_nouns, doc_labels_nouns, vocab_nouns = dtm(corp_nouns, return_doc_labels=True, return_vocab=True)\ndtm_full, doc_labels_full, vocab_full = dtm(corp_full, return_doc_labels=True, return_vocab=True)\n\nprint(f\"Noun-only DTM: {dtm_nouns.shape}\")\nprint(f\"Full DTM: {dtm_full.shape}\")\n\nNoun-only DTM: (32, 646)\nFull DTM: (32, 2439)",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Topic Modeling</span>"
    ]
  },
  {
    "objectID": "tutorials/topic-modeling.html#running-the-model",
    "href": "tutorials/topic-modeling.html#running-the-model",
    "title": "8Â  Topic Modeling",
    "section": "8.4 Running the Model",
    "text": "8.4 Running the Model\n\n8.4.1 Fixed k Model\nStart with k=10 topics as a baseline:\n\nlda_params = {\n    'n_topics': 10,\n    'n_iter': 1000,\n    'random_state': 20191122\n}\n\ndtms = {'nouns': dtm_nouns, 'full': dtm_full}\nmodels = compute_models_parallel(dtms, constant_parameters=lda_params)\n\n\n\n8.4.2 Examine Topics\n\nmodel_nouns = models['nouns'][0][1]\nprint_ldamodel_topic_words(model_nouns.topic_word_, vocab_nouns, top_n=3)\n\ntopic_1\n&gt; #1. states (0.074897)\n&gt; #2. united (0.071776)\n&gt; #3. effort (0.040574)\ntopic_2\n&gt; #1. progress (0.063210)\n&gt; #2. opportunity (0.062039)\n&gt; #3. system (0.051506)\ntopic_3\n&gt; #1. responsibility (0.037967)\n&gt; #2. republic (0.036021)\n&gt; #3. law (0.031154)\ntopic_4\n&gt; #1. law (0.073032)\n&gt; #2. business (0.046790)\n&gt; #3. congress (0.039945)\ntopic_5\n&gt; #1. strength (0.082568)\n&gt; #2. earth (0.076775)\n&gt; #3. place (0.042015)\ntopic_6\n&gt; #1. spirit (0.056502)\n&gt; #2. thing (0.054933)\n&gt; #3. task (0.036102)\ntopic_7\n&gt; #1. change (0.068062)\n&gt; #2. self (0.050717)\n&gt; #3. success (0.044045)\ntopic_8\n&gt; #1. americans (0.064891)\n&gt; #2. history (0.059609)\n&gt; #3. president (0.052819)\ntopic_9\n&gt; #1. generation (0.045535)\n&gt; #2. word (0.041489)\n&gt; #3. moment (0.041489)\ntopic_10\n&gt; #1. right (0.058935)\n&gt; #2. liberty (0.040035)\n&gt; #3. party (0.038923)\n\n\nInterpretation task: Look at the top words. Can you assign thematic labels?\n\nTopic with â€œgovernment,â€ â€œlaw,â€ â€œcongressâ€ â†’ â€œGovernanceâ€\nTopic with â€œfreedom,â€ â€œliberty,â€ â€œrightsâ€ â†’ â€œDemocratic idealsâ€\nTopic with â€œwar,â€ â€œpeace,â€ â€œworldâ€ â†’ â€œForeign policyâ€\n\nCompare to full-vocabulary model:\n\nmodel_full = models['full'][0][1]\nprint(\"Full vocabulary model:\")\nprint_ldamodel_topic_words(model_full.topic_word_, vocab_full, top_n=3)\n\nFull vocabulary model:\ntopic_1\n&gt; #1. president (0.023343)\n&gt; #2. strong (0.021313)\n&gt; #3. heart (0.019960)\ntopic_2\n&gt; #1. economic (0.027413)\n&gt; #2. action (0.023987)\n&gt; #3. hold (0.021132)\ntopic_3\n&gt; #1. law (0.041958)\n&gt; #2. system (0.031293)\n&gt; #3. progress (0.029871)\ntopic_4\n&gt; #1. national (0.025714)\n&gt; #2. justice (0.021786)\n&gt; #3. good (0.021786)\ntopic_5\n&gt; #1. america (0.092663)\n&gt; #2. americans (0.043843)\n&gt; #3. century (0.028400)\ntopic_6\n&gt; #1. peace (0.062824)\n&gt; #2. shall (0.044392)\n&gt; #3. war (0.032354)\ntopic_7\n&gt; #1. right (0.040755)\n&gt; #2. power (0.034581)\n&gt; #3. united (0.031699)\ntopic_8\n&gt; #1. freedom (0.049676)\n&gt; #2. free (0.029401)\n&gt; #3. history (0.026698)\ntopic_9\n&gt; #1. business (0.024287)\n&gt; #2. congress (0.021252)\n&gt; #3. policy (0.020645)\ntopic_10\n&gt; #1. let (0.045623)\n&gt; #2. change (0.026395)\n&gt; #3. generation (0.021871)\n\n\nComparison question: Do the full-vocabulary topics include more verbs and adjectives? Do they feel more action-oriented or emotionally inflected compared to the noun-only topics?\n\n\n\n\n\n\nThe Interpretation Trap\n\n\n\nTopics are probability distributions, not themes. Assigning labels is your interpretive act, not a discovery. Two researchers might label the same topic differently (â€œforeign policyâ€ vs.Â â€œinternational conflictâ€). Always acknowledge this subjectivity.",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Topic Modeling</span>"
    ]
  },
  {
    "objectID": "tutorials/topic-modeling.html#model-evaluation",
    "href": "tutorials/topic-modeling.html#model-evaluation",
    "title": "8Â  Topic Modeling",
    "section": "8.5 Model Evaluation",
    "text": "8.5 Model Evaluation\n\n8.5.1 Testing Multiple k Values\nWhich k is optimal? Too few topics â†’ themes collapse together. Too many topics â†’ fragmented, hard to interpret.\nTest k=5, 10, 15, â€¦, 45:\n\nvar_params = [{'n_topics': k, 'alpha': 1/k} for k in range(5, 50, 5)]\nconst_params = {\n    'n_iter': 1000,\n    'random_state': 20191122,\n    'eta': 0.1\n}\n\neval_results = evaluate_topic_models(\n    dtm_full,\n    varying_parameters=var_params,\n    constant_parameters=const_params,\n    coherence_mimno_2011_top_n=10,\n    return_models=True\n)\n\n\n\n8.5.2 Metrics\nPerplexity: How well the model predicts held-out data. Lower = better. But: low perplexity â‰  interpretable topics.\nCoherence (Mimno 2011): Measures semantic relatedness of top words using PMI (pointwise mutual information). Higher = more coherent topics.\n\neval_results_by_k = results_by_parameter(eval_results, 'n_topics')\nplot_eval_results(eval_results_by_k)\nplt.show()\n\n\n\n\n\n\n\n\nHow to choose k:\n\nLook for coherence peak (diminishing returns after a certain k)\nCheck perplexity stabilization (rapid improvement levels off)\nInspect topics manually (do they make sense at different k values?)\nConsider interpretive goals (5 topics for broad overview, 30 for granular analysis)\n\n\n\n\n\n\n\nNo â€œCorrectâ€ k\n\n\n\nThe optimal k depends on your corpus and research questions. A corpus of 50 documents might work best with k=5. A corpus of 10,000 might need k=100. Use evaluation metrics as guides, not rules.",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Topic Modeling</span>"
    ]
  },
  {
    "objectID": "tutorials/topic-modeling.html#interpreting-results",
    "href": "tutorials/topic-modeling.html#interpreting-results",
    "title": "8Â  Topic Modeling",
    "section": "8.6 Interpreting Results",
    "text": "8.6 Interpreting Results\n\n8.6.1 Generate Topic Labels\n\n# Select model with k=15 (balance between metrics)\nbest_model = [m for k, m in eval_results_by_k if k == 15][0]['model']\n\n# Generate labels from top words\nvocab_full_array = np.array(vocab_full)\ndoc_lengths_full = doc_lengths(dtm_full)\n\ntopic_labels = generate_topic_labels_from_top_words(\n    best_model.topic_word_,\n    best_model.doc_topic_,\n    doc_lengths_full,\n    vocab_full_array,\n    lambda_=0.6  # Balance between word frequency and topic exclusivity (0=exclusive, 1=frequent)\n)\n\nprint(topic_labels)\n\n['1_security' '2_child' '3_tariff' '4_law' '5_shall' '6_mistake'\n '7_republic' '8_united' '9_policy' '10_freedom' '11_let' '12_leadership'\n '13_thank' '14_weapon' '15_care']\n\n\nLambda parameter: Controls the trade-off between word frequency and topic exclusivity. Lower values (0.0-0.4) favor distinctive words unique to each topic. Higher values (0.6-1.0) favor frequent words regardless of distinctiveness. We use 0.6 to balance interpretability with representativeness.\n\n\n8.6.2 Top Words per Topic\n\ntop_topic_words = ldamodel_top_topic_words(\n    best_model.topic_word_,\n    vocab_full,\n    row_labels=topic_labels\n)\n\n# Examine one topic\nprint(top_topic_words.iloc[0])\n\nrank_1            free (0.02134)\nrank_2        security (0.01725)\nrank_3         economic (0.0148)\nrank_4         defense (0.01398)\nrank_5       democracy (0.01235)\nrank_6       knowledge (0.01153)\nrank_7          common (0.01153)\nrank_8        program (0.009895)\nrank_9     strengthen (0.009895)\nrank_10          hold (0.009895)\nName: 1_security, dtype: object\n\n\n\n\n8.6.3 Document-Topic Distributions\nWhich speeches emphasize which topics?\n\nfrom tmtoolkit.topicmod.model_io import ldamodel_full_doc_topics\n\ndoc_topic_df = ldamodel_full_doc_topics(\n    best_model.doc_topic_,\n    doc_labels_full,\n    topic_labels=topic_labels\n)\n\ndoc_topic_df.head(10)\n\n\n\n\n\n\n\n\n_doc\n1_security\n2_child\n3_tariff\n4_law\n5_shall\n6_mistake\n7_republic\n8_united\n9_policy\n10_freedom\n11_let\n12_leadership\n13_thank\n14_weapon\n15_care\n\n\n\n\n0\ninaugural_subset-1901\n0.000088\n0.000088\n0.089561\n0.086930\n0.154035\n0.002719\n0.110614\n0.113246\n0.330351\n0.111930\n0.000088\n0.000088\n0.000088\n0.000088\n0.000088\n\n\n1\ninaugural_subset-1905\n0.003556\n0.103556\n0.000222\n0.000222\n0.420222\n0.000222\n0.186889\n0.000222\n0.000222\n0.183556\n0.000222\n0.000222\n0.000222\n0.073556\n0.026889\n\n\n2\ninaugural_subset-1909\n0.007014\n0.000036\n0.538952\n0.059080\n0.103632\n0.000036\n0.000573\n0.000573\n0.241045\n0.048882\n0.000036\n0.000036\n0.000036\n0.000036\n0.000036\n\n\n3\ninaugural_subset-1913\n0.000129\n0.162291\n0.021364\n0.098584\n0.372716\n0.164221\n0.000129\n0.000129\n0.058044\n0.079279\n0.042600\n0.000129\n0.000129\n0.000129\n0.000129\n\n\n4\ninaugural_subset-1917\n0.000145\n0.017498\n0.045698\n0.004483\n0.648735\n0.060882\n0.000145\n0.000145\n0.000145\n0.180188\n0.032683\n0.000145\n0.000145\n0.008821\n0.000145\n\n\n5\ninaugural_subset-1921\n0.082064\n0.000056\n0.044407\n0.128089\n0.173278\n0.062817\n0.302985\n0.017629\n0.029344\n0.103821\n0.055286\n0.000056\n0.000056\n0.000056\n0.000056\n\n\n6\ninaugural_subset-1925\n0.057614\n0.000049\n0.036212\n0.163149\n0.163887\n0.000049\n0.062780\n0.007429\n0.371267\n0.126986\n0.000049\n0.000049\n0.000049\n0.010381\n0.000049\n\n\n7\ninaugural_subset-1929\n0.030883\n0.000053\n0.058551\n0.588986\n0.149460\n0.000053\n0.000053\n0.035626\n0.083847\n0.051436\n0.000053\n0.000053\n0.000053\n0.000053\n0.000843\n\n\n8\ninaugural_subset-1933\n0.000098\n0.074768\n0.035237\n0.000098\n0.275354\n0.000098\n0.048414\n0.000098\n0.174329\n0.082089\n0.003026\n0.306101\n0.000098\n0.000098\n0.000098\n\n\n9\ninaugural_subset-1937\n0.014301\n0.165720\n0.003260\n0.107361\n0.243007\n0.000105\n0.077392\n0.260358\n0.003260\n0.121556\n0.001682\n0.000105\n0.000105\n0.000105\n0.001682\n\n\n\n\n\n\n\nComputational reasoning question: If Obama 2009 and Kennedy 1961 both have high proportions in the â€œfreedomâ€ topic, does that mean theyâ€™re saying the same thing? Or does â€œfreedomâ€ mean different things in different Cold War vs.Â post-9/11 contexts?\n\n\n8.6.4 Temporal Trends\nDo topics rise and fall over time?\n\n# Load metadata with years\ndf_meta = pd.read_csv(\"https://github.com/browndw/humanities_analytics/raw/refs/heads/main/data/data_tables/inaugural_subset.csv\")\ndoc_topic_df['year'] = df_meta['year'].values\n\n# Plot one topic over time\ntopic_col = topic_labels[0]  # First topic\nplt.figure(figsize=(12, 5))\nplt.plot(doc_topic_df['year'], doc_topic_df[topic_col], marker='o')\nplt.xlabel('Year')\nplt.ylabel(f'Proportion of {topic_col}')\nplt.title(f'Temporal Trend: {topic_col}')\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation: Does the trend correlate with historical events? For example, does a â€œwarâ€ topic spike during WWI, WWII, and Cold War periods?\n\n\n8.6.5 Multiple Topics Over Time\nCompare several topics simultaneously:\n\n# Select 4 interesting topics to compare\ntopics_to_plot = topic_labels[:4]\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.flatten()\n\nfor i, topic in enumerate(topics_to_plot):\n    axes[i].plot(doc_topic_df['year'], doc_topic_df[topic], marker='o', linewidth=2)\n    axes[i].set_xlabel('Year')\n    axes[i].set_ylabel('Topic Proportion')\n    axes[i].set_title(f'{topic}')\n    axes[i].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nComparative reasoning: Do some topics rise as others fall? Might suggest thematic displacement (e.g., â€œdomestic policyâ€ declining as â€œforeign policyâ€ rises during wartime).",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Topic Modeling</span>"
    ]
  },
  {
    "objectID": "tutorials/topic-modeling.html#visualizing-topic-distributions",
    "href": "tutorials/topic-modeling.html#visualizing-topic-distributions",
    "title": "8Â  Topic Modeling",
    "section": "8.7 Visualizing Topic Distributions",
    "text": "8.7 Visualizing Topic Distributions\n\n8.7.1 Heatmap: Documents Ã— Topics\nHeatmaps reveal which documents emphasize which topics and which documents cluster together thematically.\n\nfrom tmtoolkit.topicmod.visualize import plot_doc_topic_heatmap\n\n# Create heatmap of document-topic distributions\nfig, ax = plt.subplots(figsize=(16, 10))\n\nplot_doc_topic_heatmap(\n    fig, ax, \n    best_model.doc_topic_, \n    doc_labels_full,\n    topic_labels=topic_labels\n)\n\nplt.title('Document-Topic Distribution Heatmap', fontsize=16)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nReading the heatmap: - Rows = documents (speeches) - Columns = topics - Color intensity = topic proportion (darker = higher)\nPatterns to look for: - Horizontal bands: Documents dominated by one topic - Vertical stripes: Topics that appear across many documents - Blocks: Groups of documents sharing similar topic profiles\nInterpretation question: Do speeches from the same era cluster visually? Do Republican vs.Â Democratic presidents show different topic emphases?\n\n\n8.7.2 Hierarchical Clustering: Document Similarity\nDendrograms show which documents are most similar based on their topic distributions.\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\n\n# Extract numeric topic proportions\ndata_array = doc_topic_df[topic_labels].values\n\n# Compute hierarchical clustering (Ward's method minimizes within-cluster variance)\n# This produces compact, balanced clusters rather than chains\nlinked = linkage(data_array, 'ward')\n\n# Plot dendrogram\nplt.figure(figsize=(14, 8))\ndendrogram(\n    linked,\n    orientation='top',\n    labels=doc_labels_full,\n    distance_sort='descending',\n    leaf_rotation=90\n)\nplt.title('Hierarchical Clustering of Documents by Topic Similarity', fontsize=16)\nplt.xlabel('Document (Year)', fontsize=12)\nplt.ylabel('Distance (Dissimilarity)', fontsize=12)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nReading the dendrogram: - Leaf nodes (bottom) = individual documents - Branch height = dissimilarity (taller = more different) - Clusters = documents joined at low heights are thematically similar\nInterpretation: Do historical periods cluster together (all Cold War speeches in one branch)? Do clusters cross party lines, suggesting bipartisan consensus on certain topics?\n\n\n8.7.3 Stacked Area Chart: Topic Evolution\nVisualize how topic proportions change over time as a compositional whole:\n\n# Create time-series matrix of topic proportions\ntopic_by_year = doc_topic_df.groupby('year')[topic_labels].mean()\n\n# Stacked area chart\nfig, ax = plt.subplots(figsize=(14, 8))\nax.stackplot(\n    topic_by_year.index, \n    *[topic_by_year[col] for col in topic_labels],\n    labels=topic_labels,\n    alpha=0.7\n)\n\nax.set_xlabel('Year', fontsize=12)\nax.set_ylabel('Average Topic Proportion', fontsize=12)\nax.set_title('Topic Composition Over Time (Stacked)', fontsize=16)\nax.legend(loc='upper left', bbox_to_anchor=(1.05, 1), fontsize=8)\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation: This shows the relative balance of topics. Does one topic dominate certain eras? Do topics become more evenly distributed over time (suggesting rhetorical diversification)?\n\n\n\n\n\n\nChoosing Visualizations\n\n\n\n\nHeatmap: Best for seeing all document-topic relationships at once. Good for exploratory analysis.\nDendrogram: Best for identifying document clusters and outliers. Useful for hypothesis generation about periodization.\nLine plots: Best for temporal trends in specific topics. Ideal for hypothesis testing.\nStacked area: Best for understanding compositional changes (how the topic mix shifts over time).\n\nUse multiple visualizationsâ€”they reveal different aspects of the same model.",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Topic Modeling</span>"
    ]
  },
  {
    "objectID": "tutorials/topic-modeling.html#when-to-use-topic-modeling",
    "href": "tutorials/topic-modeling.html#when-to-use-topic-modeling",
    "title": "8Â  Topic Modeling",
    "section": "8.8 When to Use Topic Modeling",
    "text": "8.8 When to Use Topic Modeling\n\n\n\n\n\n\nWell-Suited Research Questions\n\n\n\n\nExploratory analysis: â€œWhat themes appear in this corpus I donâ€™t know well?â€\nCorpus comparison: Do 19th-century novels emphasize different topics than 20th-century ones?\nTemporal change: How did newspaper coverage of â€œimmigrationâ€ shift from 1900-2000?\nDocument clustering: Group similar texts without reading all of them\nHypothesis generation: Identify patterns to investigate with close reading\n\n\n\n\n\n\n\n\n\nLimitations and Alternatives\n\n\n\nWhen topic modeling struggles:\n\nSmall corpora (&lt; 100 documents): Not enough data for stable patterns\nShort texts (tweets, headlines): Too few words per document\nHighly heterogeneous corpora: Mixing poems, scientific articles, and novels produces noisy topics\nFine-grained semantic distinctions: Canâ€™t distinguish irony, sarcasm, metaphor\n\nAlternative approaches:\n\nManual coding: For small corpora, human annotation may be more accurate\nKeyword analysis: If you know what youâ€™re looking for, search directly\nClustering with embeddings: Use pre-trained models (BERT) for semantic similarity\nBERTopic: Combines embeddings with topic modeling for better coherence on short texts",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Topic Modeling</span>"
    ]
  },
  {
    "objectID": "tutorials/topic-modeling.html#common-pitfalls",
    "href": "tutorials/topic-modeling.html#common-pitfalls",
    "title": "8Â  Topic Modeling",
    "section": "8.9 Common Pitfalls",
    "text": "8.9 Common Pitfalls\n1. Treating topics as â€œrealâ€\nTopics are statistical artifacts, not authorial intentions or cultural concepts. Donâ€™t claim â€œ18th-century writers believed in Topic 5â€â€”they had no concept of it.\n2. Ignoring model sensitivity\nChange k, change preprocessing, get different topics. Always report modeling choices and test robustness.\n3. Cherry-picking topics\nPresenting only the 3 coherent topics out of 20 misleads. Report full results, including messy topics.\n4. Over-interpreting probabilities\nA document with 0.25 in Topic A isnâ€™t â€œ25% about that themeâ€â€”itâ€™s a statistical weight, not a precise measurement.\n5. Conflating co-occurrence with meaning\nWords co-occur for many reasons (syntax, genre conventions, corpus artifacts). Topic coherence â‰  semantic unity.\n6. Forgetting to triangulate\nNever rely on topic modeling alone. Validate with close reading, metadata analysis, and domain expertise.",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Topic Modeling</span>"
    ]
  },
  {
    "objectID": "tutorials/topic-modeling.html#what-to-do-after-topic-modeling",
    "href": "tutorials/topic-modeling.html#what-to-do-after-topic-modeling",
    "title": "8Â  Topic Modeling",
    "section": "8.10 What to Do After Topic Modeling",
    "text": "8.10 What to Do After Topic Modeling\n\n8.10.1 Connect to Close Reading\n\nIdentify exemplar documents: Find speeches with high proportions in a topic\nRead them closely: Does the topic label fit? What context is missing?\nRefine interpretation: Adjust labels based on actual content\n\n\n\n8.10.2 Connect to Historical Context\n\nDo topic trends align with known events (wars, economic crises, social movements)?\nDo unexpected patterns suggest overlooked historical dynamics?\nHow do topics relate to metadata (author, genre, publication venue)?\n\n\n\n8.10.3 Iterate\n\nRemove problematic words (names that dominate topics, corpus-specific noise)\nAdjust preprocessing (try different POS filters, stopword lists)\nTest different k values (broad overview vs.Â fine-grained analysis)\nCompare algorithms (LDA vs.Â NMF vs.Â BERTopic)\n\n\n\n8.10.4 Build On It\n\nSupervised classification: Use topic distributions as features for predicting metadata\nNetwork analysis: Connect documents via shared topics\nVisualization: Create topic timelines, heatmaps, or interactive explorers",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Topic Modeling</span>"
    ]
  },
  {
    "objectID": "tutorials/topic-modeling.html#troubleshooting-common-issues",
    "href": "tutorials/topic-modeling.html#troubleshooting-common-issues",
    "title": "8Â  Topic Modeling",
    "section": "8.11 Troubleshooting Common Issues",
    "text": "8.11 Troubleshooting Common Issues\nâ€œAll my topics look the sameâ€\n\nCause: k too small, or vocabulary too restricted\nFix: Increase k, relax stopword filtering, or use full vocabulary instead of POS-filtered\n\nâ€œTopics contain gibberish or proper namesâ€\n\nCause: Insufficient preprocessing\nFix: Add custom stopwords (place names, character names), increase df_threshold for common words\n\nâ€œTopics are incoherent (random word lists)â€\n\nCause: k too large, or corpus too heterogeneous\nFix: Reduce k, subset corpus by genre/time period, increase n_iter for better convergence\n\nâ€œDocument-topic distributions are too uniform (every doc has equal proportions in all topics)â€\n\nCause: alpha too high (documents encouraged to mix all topics)\nFix: Lower alpha (try alpha=1/k or alpha=0.1)\n\nâ€œTopics dominated by one or two wordsâ€\n\nCause: eta/beta too low, or corpus has strong term imbalance\nFix: Increase eta (try 0.1-0.5), or remove dominant terms from vocabulary\n\nâ€œResults change dramatically with random_stateâ€\n\nCause: Model hasnâ€™t converged, or corpus is too small\nFix: Increase n_iter (try 2000-5000), or collect more documents\n\n\n\n\n\n\n\nModel Stability Check\n\n\n\nRun the same model with 3-5 different random_state values. If topic assignments and labels remain consistent, your model is stable. If they change drastically, you need more iterations, more documents, or different preprocessing.",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Topic Modeling</span>"
    ]
  },
  {
    "objectID": "tutorials/topic-modeling.html#conclusion",
    "href": "tutorials/topic-modeling.html#conclusion",
    "title": "8Â  Topic Modeling",
    "section": "8.12 Conclusion",
    "text": "8.12 Conclusion\nTopic modeling is a generative toolâ€”it suggests patterns you didnâ€™t know to look for. Itâ€™s not a confirmatory toolâ€”it doesnâ€™t prove hypotheses.\nUse it to: - Survey large corpora quickly - Generate hypotheses for close reading - Discover unexpected thematic connections - Track topical change over time\nDonâ€™t use it to: - Make causal claims - Replace close reading - Assume topics reflect authorial intentions - Treat statistical patterns as cultural truths\nThe best topic modeling workflows combine: 1. Computational scale (process thousands of texts) 2. Statistical rigor (test multiple models, report uncertainties) 3. Interpretive depth (close reading, historical context, domain knowledge) 4. Methodological transparency (document all choices, acknowledge limitations)\n\n\n\n\n\n\nConnecting to Mini Lab 9\n\n\n\nMini Lab 9: Topic Modeling provides hands-on practice with the complete workflow: preprocessing choices, model evaluation, topic labeling, and temporal analysis using U.S. presidential inaugural addresses.",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Topic Modeling</span>"
    ]
  },
  {
    "objectID": "tutorials/topic-modeling.html#see-also",
    "href": "tutorials/topic-modeling.html#see-also",
    "title": "8Â  Topic Modeling",
    "section": "8.13 See Also",
    "text": "8.13 See Also\nTed Underwoodâ€™s Topic Modeling Guides: - Topic Modeling Made Just Simple Enough - What Kinds of Topics Does Topic Modeling Produce? - Visualizing Topic Models\ntmtoolkit Documentation: Topic Modeling Vignette\nEvaluation Methods: Topic Modeling Evaluation in Python\nAlternative Approaches: - BERTopic for short texts and semantic embeddings - Graph Neural Topic Models for document networks\nProject Examples: A Review of Topic Modeling Projects",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Topic Modeling</span>"
    ]
  },
  {
    "objectID": "tutorials/topic-modeling.html#works-cited",
    "href": "tutorials/topic-modeling.html#works-cited",
    "title": "8Â  Topic Modeling",
    "section": "8.14 Works Cited",
    "text": "8.14 Works Cited\nBlei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent Dirichlet allocation. Journal of Machine Learning Research, 3, 993-1022.\nMimno, D., Wallach, H., Talley, E., Leenders, M., & McCallum, A. (2011). Optimizing semantic coherence in topic models. Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, 262-272.",
    "crumbs": [
      "Core Methods",
      "<span class='chapter-number'>8</span>Â  <span class='chapter-title'>Topic Modeling</span>"
    ]
  },
  {
    "objectID": "tutorials/categorical-variables.html",
    "href": "tutorials/categorical-variables.html",
    "title": "9Â  ANOVA and RÂ²: Comparing Groups and Explaining Variance",
    "section": "",
    "text": "9.1 Introduction\nAfter performing dimension reduction (like PCA or Factor Analysis in MDA), a critical question emerges: Do the dimensions weâ€™ve extracted actually distinguish our text categories?\nFor example, if Multi-Dimensional Analysis produces a dimension labeled â€œInvolved vs.Â Informational Production,â€ we need to test:\nANOVA (Analysis of Variance) and RÂ² (R-squared, coefficient of determination) answer these questions. This tutorial explains:",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ANOVA and RÂ²: Comparing Groups and Explaining Variance</span>"
    ]
  },
  {
    "objectID": "tutorials/categorical-variables.html#introduction",
    "href": "tutorials/categorical-variables.html#introduction",
    "title": "9Â  ANOVA and RÂ²: Comparing Groups and Explaining Variance",
    "section": "",
    "text": "Do fiction texts score significantly higher than academic texts on this dimension?\nHow much of the variation in dimension scores is explained by genre?\nAre the differences large enough to matter, or just statistically detectable?\n\n\n\nWhat these statistics measure and why they matter\nWhere the numbers come from (the mathematical foundations)\nHow to interpret them in the context of text analysis and MDA\nHow to compute and report them using Python",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ANOVA and RÂ²: Comparing Groups and Explaining Variance</span>"
    ]
  },
  {
    "objectID": "tutorials/categorical-variables.html#core-concepts",
    "href": "tutorials/categorical-variables.html#core-concepts",
    "title": "9Â  ANOVA and RÂ²: Comparing Groups and Explaining Variance",
    "section": "9.2 Core Concepts",
    "text": "9.2 Core Concepts\n\n9.2.1 The Research Question\nImagine youâ€™ve extracted dimension scores for 500 documents across 15 genres. You hypothesize that Genre A differs from Genre B on Dimension 1. How do you test this?\nThree related questions:\n\nAre the means different? ANOVA tests if group means differ more than expected by chance\nHow much variance is explained? RÂ² quantifies what proportion of total variance is due to group membership\nIs the effect meaningful? Effect sizes (like Î·Â²) indicate practical significance beyond statistical significance\n\n\n\n9.2.2 ANOVA: Analysis of Variance\nANOVA is a statistical test that compares means across two or more groups by analyzing variance.\nCore insight: Variance can be partitioned into:\n\nBetween-group variance: How much do group means differ from the overall mean?\nWithin-group variance: How much do individual observations differ from their group mean?\n\nIf between-group variance is much larger than within-group variance, groups genuinely differ (not just random variation).\nExample: Dimension 1 scores for three genres:\n\nFiction: Mean = 15, scores range 10-20\nAcademic: Mean = -12, scores range -18 to -6\n\nNews: Mean = 2, scores range -5 to 9\n\nANOVA tests: Is the variation between these three means (15 vs.Â -12 vs.Â 2) greater than the variation within each group?\n\n\n9.2.3 RÂ²: Coefficient of Determination\nRÂ² (R-squared) measures proportion of variance explained by the grouping variable (genre).\n\nRÂ² = 0: Genre explains 0% of variance (dimension scores are identical across genres)\nRÂ² = 1: Genre explains 100% of variance (all variation is due to genre differences, no within-group variation)\nRÂ² = 0.35: Genre explains 35% of variance (remaining 65% is individual variation within genres)\n\nInterpretation: RÂ² = 0.35 means if you know a textâ€™s genre, you can explain 35% of the variation in its dimension score. The other 65% comes from other factors (author, topic, style).\nIn MDA context: After extracting dimensions, RÂ² tells you how well those dimensions differentiate your categories. High RÂ² (&gt; 0.25) indicates dimensions are strong category markers.\n\n\n9.2.4 Î·Â² (Eta-Squared): ANOVAâ€™s RÂ²\nÎ·Â² (eta-squared) is the RÂ² for ANOVA: proportion of total variance explained by group membership.\nFormula: Î·Â² = SS_between / SS_total\nEquivalent to RÂ² in one-way ANOVA (single grouping variable like â€œgenreâ€).\nEffect size benchmarks (Cohen, 1988):\n\nÎ·Â² = 0.01 (1%): Small effect\nÎ·Â² = 0.06 (6%): Medium effect\nÎ·Â² = 0.14 (14%): Large effect\n\nIn text analysis: Î·Â² = 0.20 for a dimension means 20% of linguistic variation is explained by genreâ€”a strong, meaningful pattern (well above Cohenâ€™s â€œlargeâ€ threshold).",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ANOVA and RÂ²: Comparing Groups and Explaining Variance</span>"
    ]
  },
  {
    "objectID": "tutorials/categorical-variables.html#understanding-the-mathematics",
    "href": "tutorials/categorical-variables.html#understanding-the-mathematics",
    "title": "9Â  ANOVA and RÂ²: Comparing Groups and Explaining Variance",
    "section": "9.3 Understanding the Mathematics",
    "text": "9.3 Understanding the Mathematics\n\n9.3.1 Variance Decomposition\nTotal variance in dimension scores can be partitioned:\nSS_total = SS_between + SS_within\nWhere:\n\nSS_total: Total sum of squares (total variance across all observations)\nSS_between: Between-group sum of squares (variance due to group differences)\nSS_within: Within-group sum of squares (variance within groups, â€œerrorâ€)\n\nIntuition: Imagine dimension scores for 50 texts (10 each from 5 genres). Total variance = how much all 50 scores vary. This variance has two sources:\n\nBetween-group: The 5 genre means differ (Fiction = +15, Academic = -12, etc.)\nWithin-group: Within Fiction, some texts score 12, others 18 (variation around Fictionâ€™s mean)\n\nANOVA tests if source #1 (between-group) is larger than expected if groups were identical.\n\n\n9.3.2 Computing Sum of Squares\n\n9.3.2.1 SS_total: Total Sum of Squares\nFormula:\n\\[SS_{total} = \\sum_{i=1}^{n}(x_i - \\bar{x})^2\\]\nWhere: - \\(x_i\\) = individual observation (dimension score for text i) - \\(\\bar{x}\\) = grand mean (average score across all texts) - \\(n\\) = total number of observations\nInterpretation: Sum of squared deviations from the overall mean. Measures total variation in the data.\nExample:\nScores: [15, 12, -10, -14, 3]\nGrand mean: (15+12-10-14+3)/5 = 1.2\n\nSS_total = (15-1.2)Â² + (12-1.2)Â² + (-10-1.2)Â² + (-14-1.2)Â² + (3-1.2)Â²\n         = 190.44 + 116.64 + 125.44 + 231.04 + 3.24\n         = 666.8\n\n\n9.3.2.2 SS_between: Between-Group Sum of Squares\nFormula:\n\\[SS_{between} = \\sum_{j=1}^{k} n_j(\\bar{x}_j - \\bar{x})^2\\]\nWhere: - \\(k\\) = number of groups (genres) - \\(n_j\\) = number of observations in group j - \\(\\bar{x}_j\\) = mean of group j - \\(\\bar{x}\\) = grand mean\nInterpretation: Weighted sum of squared differences between group means and grand mean. Measures variation due to group differences.\nExample (3 groups, 2 texts each):\nGroup A: [14, 16], mean = 15\nGroup B: [-13, -11], mean = -12\nGroup C: [1, 3], mean = 2\nGrand mean = 1.0\n\nSS_between = 2Ã—(15-1)Â² + 2Ã—(-12-1)Â² + 2Ã—(2-1)Â²\n           = 2Ã—196 + 2Ã—169 + 2Ã—1\n           = 392 + 338 + 2\n           = 732\n\n\n9.3.2.3 SS_within: Within-Group Sum of Squares\nFormula:\n\\[SS_{within} = \\sum_{j=1}^{k}\\sum_{i=1}^{n_j}(x_{ij} - \\bar{x}_j)^2\\]\nWhere: - \\(x_{ij}\\) = observation i in group j - \\(\\bar{x}_j\\) = mean of group j\nInterpretation: Sum of squared deviations from each observation to its group mean. Measures variation within groups (not explained by group membership).\nExample (same 3 groups):\nGroup A: [14, 16], mean = 15\n  SS_A = (14-15)Â² + (16-15)Â² = 1 + 1 = 2\n\nGroup B: [-13, -11], mean = -12\n  SS_B = (-13-(-12))Â² + (-11-(-12))Â² = 1 + 1 = 2\n\nGroup C: [1, 3], mean = 2\n  SS_C = (1-2)Â² + (3-2)Â² = 1 + 1 = 2\n\nSS_within = 2 + 2 + 2 = 6\nCheck: SS_total should equal SS_between + SS_within (within rounding error).\n\n\n\n9.3.3 F-Statistic: The ANOVA Test\nANOVA computes an F-statistic that compares between-group to within-group variance:\nFormula:\n\\[F = \\frac{MS_{between}}{MS_{within}} = \\frac{SS_{between}/(k-1)}{SS_{within}/(n-k)}\\]\nWhere:\n\nMS_between = Mean square between groups (variance between groups)\n\n\\(MS_{between} = SS_{between} / df_{between}\\)\n\\(df_{between} = k - 1\\) (degrees of freedom: number of groups - 1)\n\nMS_within = Mean square within groups (variance within groups, â€œerrorâ€)\n\n\\(MS_{within} = SS_{within} / df_{within}\\)\n\\(df_{within} = n - k\\) (degrees of freedom: total observations - number of groups)\n\n\nInterpretation:\n\nF â‰ˆ 1: Between-group variance equals within-group variance (groups donâ€™t differ)\nF &gt;&gt; 1: Between-group variance much larger (groups genuinely differ)\nP-value: Probability of observing F this large if groups were identical (p &lt; 0.05 = significant)\n\nExample:\nFrom earlier: SS_between = 732, SS_within = 6\nGroups k = 3, total n = 6\n\nMS_between = 732 / (3-1) = 732 / 2 = 366\nMS_within = 6 / (6-3) = 6 / 3 = 2\n\nF = 366 / 2 = 183\nInterpretation: Between-group variance is 183Ã— larger than within-group variance. Extremely strong evidence that groups differ (p-value would be tiny).\n\n\n9.3.4 Computing RÂ² and Î·Â²\nOnce we have sum of squares, RÂ² and Î·Â² are straightforward:\nFormula:\n\\[R^2 = \\eta^2 = \\frac{SS_{between}}{SS_{total}}\\]\nExample:\nSS_between = 732\nSS_total = SS_between + SS_within = 732 + 6 = 738\n\nÎ·Â² = 732 / 738 = 0.992 (99.2%)\nInterpretation: Genre explains 99.2% of variance in dimension scores. Nearly perfect separation (unrealistically highâ€”real text data usually shows 15-50%).\nAdjusted Î·Â² (accounts for sample size):\n\\[\\eta^2_{adj} = 1 - \\frac{SS_{within}/(n-k)}{SS_{total}/(n-1)}\\]\nMore conservative than Î·Â², penalizes models with many groups relative to sample size.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ANOVA and RÂ²: Comparing Groups and Explaining Variance</span>"
    ]
  },
  {
    "objectID": "tutorials/categorical-variables.html#anova-in-practice-example",
    "href": "tutorials/categorical-variables.html#anova-in-practice-example",
    "title": "9Â  ANOVA and RÂ²: Comparing Groups and Explaining Variance",
    "section": "9.4 ANOVA in Practice: Example",
    "text": "9.4 ANOVA in Practice: Example\nLetâ€™s test if genres differ on a dimension extracted from linguistic features:\n\n9.4.1 Generate Example Data\n\nimport polars as pl\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Simulate dimension scores for 150 texts across 3 genres\nnp.random.seed(42)\n\n# Fiction: High positive scores (involved production)\nfiction_scores = np.random.normal(loc=15, scale=5, size=50)\n\n# Academic: High negative scores (informational production)\nacademic_scores = np.random.normal(loc=-12, scale=4, size=50)\n\n# News: Near-zero scores (balanced)\nnews_scores = np.random.normal(loc=2, scale=6, size=50)\n\n# Combine into dataframe\ndata = pl.DataFrame({\n    'doc_id': [f'doc_{i:03d}' for i in range(150)],\n    'genre': ['Fiction']*50 + ['Academic']*50 + ['News']*50,\n    'dimension_1': np.concatenate([fiction_scores, academic_scores, news_scores])\n})\n\nprint(f\"Data shape: {data.shape}\")\nprint(\"\\nFirst few rows:\")\nprint(data.head())\nprint(\"\\nGenre distribution:\")\nprint(data.group_by('genre').agg(pl.count()))\n\nData shape: (150, 3)\n\nFirst few rows:\nshape: (5, 3)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ doc_id  â”† genre   â”† dimension_1 â”‚\nâ”‚ ---     â”† ---     â”† ---         â”‚\nâ”‚ str     â”† str     â”† f64         â”‚\nâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ doc_000 â”† Fiction â”† 17.483571   â”‚\nâ”‚ doc_001 â”† Fiction â”† 14.308678   â”‚\nâ”‚ doc_002 â”† Fiction â”† 18.238443   â”‚\nâ”‚ doc_003 â”† Fiction â”† 22.615149   â”‚\nâ”‚ doc_004 â”† Fiction â”† 13.829233   â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nGenre distribution:\nshape: (3, 2)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ genre    â”† count â”‚\nâ”‚ ---      â”† ---   â”‚\nâ”‚ str      â”† u32   â”‚\nâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\nâ”‚ News     â”† 50    â”‚\nâ”‚ Fiction  â”† 50    â”‚\nâ”‚ Academic â”† 50    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n/tmp/ipykernel_3079/335707288.py:31: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n(Deprecated in version 0.20.5)\n  print(data.group_by('genre').agg(pl.count()))\n\n\n\n\n9.4.2 Descriptive Statistics by Group\n\n# Compute group means and standard deviations\nsummary = data.group_by('genre').agg([\n    pl.col('dimension_1').mean().alias('mean'),\n    pl.col('dimension_1').std().alias('std'),\n    pl.col('dimension_1').min().alias('min'),\n    pl.col('dimension_1').max().alias('max'),\n    pl.count().alias('n')\n]).sort('mean', descending=True)\n\nprint(\"\\nDescriptive Statistics by Genre:\")\nprint(summary)\n\n\nDescriptive Statistics by Genre:\nshape: (3, 6)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”\nâ”‚ genre    â”† mean       â”† std      â”† min       â”† max       â”† n   â”‚\nâ”‚ ---      â”† ---        â”† ---      â”† ---       â”† ---       â”† --- â”‚\nâ”‚ str      â”† f64        â”† f64      â”† f64       â”† f64       â”† u32 â”‚\nâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•¡\nâ”‚ Fiction  â”† 13.87263   â”† 4.668344 â”† 5.201649  â”† 24.261391 â”† 50  â”‚\nâ”‚ News     â”† 1.764275   â”† 6.092482 â”† -9.512627 â”† 16.779453 â”† 50  â”‚\nâ”‚ Academic â”† -11.928877 â”† 3.4973   â”† -22.47898 â”† -5.741425 â”† 50  â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜\n\n\n/tmp/ipykernel_3079/3474101131.py:7: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n(Deprecated in version 0.20.5)\n  pl.count().alias('n')\n\n\nObservation: Fiction and Academic have very different means (â‰ˆ15 vs â‰ˆ-12), while News falls in between. Do these differences exceed within-group variation?\n\n\n9.4.3 Visualize Group Differences\n\n# Prepare data for seaborn (dictionary format avoids pyarrow dependency)\nplot_data = {\n    'genre': data['genre'].to_list(),\n    'dimension_1': data['dimension_1'].to_list()\n}\n\nplt.figure(figsize=(10, 6))\nsns.boxplot(data=plot_data, x='genre', y='dimension_1', palette='Set2')\nsns.swarmplot(data=plot_data, x='genre', y='dimension_1', color='black', alpha=0.3, size=3)\n\nplt.xlabel('Genre', fontsize=14)\nplt.ylabel('Dimension 1: Involved vs. Informational', fontsize=14)\nplt.title('Dimension Scores by Genre', fontsize=16)\nplt.axhline(y=0, color='red', linestyle='--', alpha=0.5, label='Zero line')\nplt.legend()\nplt.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n/tmp/ipykernel_3079/131551180.py:8: FutureWarning: \n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n  sns.boxplot(data=plot_data, x='genre', y='dimension_1', palette='Set2')\n\n\n\n\n\n\n\n\nFigureÂ 9.1: Box plots showing dimension score distributions by genre. Boxes show interquartile range, horizontal lines show medians, whiskers extend to 1.5Ã—IQR.\n\n\n\n\n\nVisual assessment:\n\nDo boxes (interquartile ranges) overlap substantially? (Less overlap = stronger separation)\nAre medians far apart? (Greater distance = larger effect)\nIs within-group variance small relative to between-group differences? (Compact boxes + large gaps = high F-statistic)\n\n\n\n9.4.4 Perform One-Way ANOVA\n\n# Separate data by group\nfiction = data.filter(pl.col('genre') == 'Fiction').select('dimension_1').to_numpy().flatten()\nacademic = data.filter(pl.col('genre') == 'Academic').select('dimension_1').to_numpy().flatten()\nnews = data.filter(pl.col('genre') == 'News').select('dimension_1').to_numpy().flatten()\n\n# Perform one-way ANOVA\nf_stat, p_value = stats.f_oneway(fiction, academic, news)\n\nprint(\"\\nOne-Way ANOVA Results:\")\nprint(f\"  F-statistic: {f_stat:.2f}\")\nprint(f\"  P-value: {p_value:.6f}\")\nprint(f\"  Significance: {'***' if p_value &lt; 0.001 else '**' if p_value &lt; 0.01 else '*' if p_value &lt; 0.05 else 'ns'}\")\nprint(f\"\\n  Interpretation: {'Groups differ significantly' if p_value &lt; 0.05 else 'No significant difference'}\")\n\n\nOne-Way ANOVA Results:\n  F-statistic: 351.35\n  P-value: 0.000000\n  Significance: ***\n\n  Interpretation: Groups differ significantly\n\n\nInterpreting p-value:\n\np &lt; 0.001: Extremely strong evidence of differences (***)\np &lt; 0.01: Strong evidence (**)\np &lt; 0.05: Moderate evidence (*)\np â‰¥ 0.05: Insufficient evidence (ns = not significant)\n\nCaution: Statistical significance â‰  practical significance. With large samples (n = 1000+), tiny differences become â€œsignificant.â€ Always check effect size (RÂ²).\n\n\n9.4.5 Compute Sum of Squares Manually\n\n# Extract all scores and compute grand mean\nall_scores = data.select('dimension_1').to_numpy().flatten()\ngrand_mean = np.mean(all_scores)\n\nprint(f\"\\nGrand mean: {grand_mean:.2f}\")\n\n# SS_total: Total sum of squares\nss_total = np.sum((all_scores - grand_mean)**2)\nprint(f\"SS_total: {ss_total:.2f}\")\n\n# SS_between: Between-group sum of squares\nn_fiction = len(fiction)\nn_academic = len(academic)\nn_news = len(news)\n\nmean_fiction = np.mean(fiction)\nmean_academic = np.mean(academic)\nmean_news = np.mean(news)\n\nss_between = (n_fiction * (mean_fiction - grand_mean)**2 + \n              n_academic * (mean_academic - grand_mean)**2 + \n              n_news * (mean_news - grand_mean)**2)\n\nprint(f\"SS_between: {ss_between:.2f}\")\n\n# SS_within: Within-group sum of squares\nss_within_fiction = np.sum((fiction - mean_fiction)**2)\nss_within_academic = np.sum((academic - mean_academic)**2)\nss_within_news = np.sum((news - mean_news)**2)\nss_within = ss_within_fiction + ss_within_academic + ss_within_news\n\nprint(f\"SS_within: {ss_within:.2f}\")\n\n# Check: SS_total should equal SS_between + SS_within\nprint(f\"\\nCheck: SS_between + SS_within = {ss_between + ss_within:.2f}\")\nprint(f\"       (should equal SS_total = {ss_total:.2f})\")\nprint(f\"       Difference: {abs(ss_total - (ss_between + ss_within)):.6f}\")\n\n\nGrand mean: 1.24\nSS_total: 20149.87\nSS_between: 16663.87\nSS_within: 3486.00\n\nCheck: SS_between + SS_within = 20149.87\n       (should equal SS_total = 20149.87)\n       Difference: 0.000000\n\n\nObservation: The components should sum exactly (or within tiny rounding error). This confirms variance partitioning is correct.\n\n\n9.4.6 Compute RÂ² (Î·Â²)\n\n# Eta-squared: Proportion of variance explained\neta_squared = ss_between / ss_total\n\nprint(f\"\\nEffect Size:\")\nprint(f\"  Î·Â² (eta-squared): {eta_squared:.4f} ({eta_squared*100:.2f}%)\")\n\n# Interpret effect size\nif eta_squared &gt;= 0.14:\n    effect = \"Large (Î·Â² â‰¥ 0.14)\"\nelif eta_squared &gt;= 0.06:\n    effect = \"Medium (0.06 â‰¤ Î·Â² &lt; 0.14)\"\nelif eta_squared &gt;= 0.01:\n    effect = \"Small (0.01 â‰¤ Î·Â² &lt; 0.06)\"\nelse:\n    effect = \"Negligible (Î·Â² &lt; 0.01)\"\n\nprint(f\"  Interpretation: {effect}\")\nprint(f\"\\n  Meaning: Genre explains {eta_squared*100:.1f}% of variance in dimension scores.\")\nprint(f\"           Remaining {(1-eta_squared)*100:.1f}% is individual variation within genres.\")\n\n\nEffect Size:\n  Î·Â² (eta-squared): 0.8270 (82.70%)\n  Interpretation: Large (Î·Â² â‰¥ 0.14)\n\n  Meaning: Genre explains 82.7% of variance in dimension scores.\n           Remaining 17.3% is individual variation within genres.\n\n\nIn text analysis: Î·Â² = 0.50 means dimension is a strong genre marker (genre explains half the variation). Î·Â² = 0.10 means weak marker (only 10% explained).\n\n\n9.4.7 Compute F-Statistic Manually\n\n# Degrees of freedom\nk = 3  # Number of groups\nn = len(all_scores)  # Total observations\ndf_between = k - 1\ndf_within = n - k\n\nprint(f\"\\nDegrees of Freedom:\")\nprint(f\"  Between groups (df_between): {df_between}\")\nprint(f\"  Within groups (df_within): {df_within}\")\n\n# Mean squares\nms_between = ss_between / df_between\nms_within = ss_within / df_within\n\nprint(f\"\\nMean Squares:\")\nprint(f\"  MS_between: {ms_between:.2f}\")\nprint(f\"  MS_within: {ms_within:.2f}\")\n\n# F-statistic\nf_statistic = ms_between / ms_within\n\nprint(f\"\\nF-Statistic:\")\nprint(f\"  F = {f_statistic:.2f}\")\nprint(f\"  (Should match scipy.stats.f_oneway result: {f_stat:.2f})\")\n\n# P-value from F-distribution\np_value_manual = 1 - stats.f.cdf(f_statistic, df_between, df_within)\nprint(f\"  P-value: {p_value_manual:.6f}\")\n\n\nDegrees of Freedom:\n  Between groups (df_between): 2\n  Within groups (df_within): 147\n\nMean Squares:\n  MS_between: 8331.94\n  MS_within: 23.71\n\nF-Statistic:\n  F = 351.35\n  (Should match scipy.stats.f_oneway result: 351.35)\n  P-value: 0.000000\n\n\nUnderstanding F:\n\nF = MS_between / MS_within: Ratio of between-group to within-group variance\nLarge F (e.g., F &gt; 10): Between-group variance much larger than within-group\nF â‰ˆ 1: Between-group and within-group variance similar (no group effect)\n\n\n\n9.4.8 ANOVA Table\n\n# Create standard ANOVA table\nanova_table = pd.DataFrame({\n    'Source': ['Between Groups', 'Within Groups', 'Total'],\n    'SS': [ss_between, ss_within, ss_total],\n    'df': [df_between, df_within, n-1],\n    'MS': [ms_between, ms_within, np.nan],\n    'F': [f_statistic, np.nan, np.nan],\n    'P-value': [p_value_manual, np.nan, np.nan]\n})\n\nprint(\"\\nANOVA Table:\")\nprint(anova_table.to_string(index=False))\n\n\nANOVA Table:\n        Source           SS  df          MS          F  P-value\nBetween Groups 16663.873876   2 8331.936938 351.346661      0.0\n Within Groups  3486.000769 147   23.714291        NaN      NaN\n         Total 20149.874645 149         NaN        NaN      NaN\n\n\nReading ANOVA table:\n\nSS column: Variance components (between + within = total)\ndf column: Degrees of freedom for each component\nMS column: Mean squares (SS / df) = variance estimates\nF column: Test statistic (MS_between / MS_within)\nP-value: Significance test result\n\nThis is the standard format for reporting ANOVA results in publications.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ANOVA and RÂ²: Comparing Groups and Explaining Variance</span>"
    ]
  },
  {
    "objectID": "tutorials/categorical-variables.html#post-hoc-tests-which-groups-differ",
    "href": "tutorials/categorical-variables.html#post-hoc-tests-which-groups-differ",
    "title": "9Â  ANOVA and RÂ²: Comparing Groups and Explaining Variance",
    "section": "9.5 Post-Hoc Tests: Which Groups Differ?",
    "text": "9.5 Post-Hoc Tests: Which Groups Differ?\nANOVA tells us groups differ overall, but which specific pairs differ?\n\n9.5.1 The Multiple Comparisons Problem\nWith 3 genres, there are 3 pairwise comparisons:\n\nFiction vs.Â Academic\nFiction vs.Â News\n\nAcademic vs.Â News\n\nTesting each with separate t-tests inflates Type I error (false positives). If each test has Î± = 0.05, probability of at least one false positive:\n\\[P(Type\\text{ }I\\text{ }error) = 1 - (1 - 0.05)^3 = 0.143\\]\nWith 15 genres (105 pairwise comparisons), this rises to 99.5% chance of false positive!\nSolution: Post-hoc tests adjust for multiple comparisons.\n\n\n9.5.2 Tukeyâ€™s HSD (Honestly Significant Difference)\nMost common post-hoc test:\n\nControls family-wise error rate (overall Î± = 0.05)\nMore conservative than unadjusted t-tests\nGood balance between power and protection\n\n\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\n\n# Perform Tukey HSD\ntukey = pairwise_tukeyhsd(endog=data['dimension_1'].to_numpy(), groups=data['genre'].to_numpy(), alpha=0.05)\n\nprint(\"\\nTukey HSD Post-Hoc Test:\")\nprint(tukey)\n\n\nTukey HSD Post-Hoc Test:\n  Multiple Comparison of Means - Tukey HSD, FWER=0.05  \n=======================================================\n group1   group2 meandiff p-adj  lower    upper  reject\n-------------------------------------------------------\nAcademic Fiction  25.8015   0.0  23.4955 28.1075   True\nAcademic    News  13.6932   0.0  11.3871 15.9992   True\n Fiction    News -12.1084   0.0 -14.4144 -9.8023   True\n-------------------------------------------------------\n\n\nReading Tukey output:\n\nmeandiff: Difference between group means\nlower/upper: 95% confidence interval for difference\nreject: True if difference is significant (p &lt; 0.05)\n\nExample interpretation:\ngroup1    group2     meandiff  lower   upper   reject\nAcademic  Fiction    -27.3     -29.1   -25.5   True\nAcademic  News       -14.2     -16.0   -12.4   True  \nFiction   News       13.1      11.3    14.9    True\nAll three pairs differ significantly. Fiction scores highest (+15), News middle (+2), Academic lowest (-12).\n\n\n9.5.3 Visualize Post-Hoc Results\n\n# Compute means and confidence intervals\ngenre_means = summary.sort('mean', descending=True)\n\n# For 95% CI: mean Â± (critical_value Ã— SE)\n# SE = std / sqrt(n)\n# critical_value â‰ˆ 1.96 for large samples\ngenre_means = genre_means.with_columns([\n    (pl.col('std') / pl.col('n').sqrt()).alias('se'),\n    (pl.col('mean') - 1.96 * pl.col('std') / pl.col('n').sqrt()).alias('ci_lower'),\n    (pl.col('mean') + 1.96 * pl.col('std') / pl.col('n').sqrt()).alias('ci_upper')\n])\n\n# Extract values from Polars DataFrame using to_list()\ngenres_list = genre_means['genre'].to_list()\nmeans_list = genre_means['mean'].to_list()\nci_lower_list = genre_means['ci_lower'].to_list()\nci_upper_list = genre_means['ci_upper'].to_list()\n\nplt.figure(figsize=(10, 6))\nplt.errorbar(\n    x=range(len(genres_list)),\n    y=means_list,\n    yerr=[[means_list[i] - ci_lower_list[i] for i in range(len(means_list))],\n          [ci_upper_list[i] - means_list[i] for i in range(len(means_list))]],\n    fmt='o',\n    markersize=10,\n    capsize=10,\n    capthick=2,\n    linewidth=2\n)\n\nplt.xticks(range(len(genres_list)), genres_list, fontsize=12)\nplt.ylabel('Mean Dimension 1 Score', fontsize=14)\nplt.xlabel('Genre', fontsize=14)\nplt.title('Genre Means with 95% Confidence Intervals', fontsize=16)\nplt.axhline(y=0, color='red', linestyle='--', alpha=0.5, label='Zero line')\nplt.grid(axis='y', alpha=0.3)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigureÂ 9.2: Mean dimension scores by genre with 95% confidence intervals. Non-overlapping intervals indicate significant differences (Tukey-adjusted).\n\n\n\n\n\nInterpretation: Non-overlapping error bars suggest significant differences (though this is approximateâ€”Tukey is more conservative).\n\n\n9.5.4 Bonferroni Correction (Alternative)\nMore conservative than Tukey:\n\nDivide Î± by number of comparisons: Î±_adjusted = 0.05 / 3 = 0.0167\nReject Hâ‚€ only if p &lt; 0.0167\n\nWhen to use:\n\nPlanned comparisons (you know which pairs to test before analysis)\nFewer comparisons (3-5 pairs)\n\nWhen not to use:\n\nExploratory analysis (many unplanned comparisons)\nRisk of Type II error (false negatives) increases\n\n\n# Bonferroni-corrected pairwise t-tests\nfrom scipy.stats import ttest_ind\n\ncomparisons = [\n    ('Fiction', 'Academic'),\n    ('Fiction', 'News'),\n    ('Academic', 'News')\n]\n\nn_comparisons = len(comparisons)\nalpha_bonferroni = 0.05 / n_comparisons\n\nprint(f\"\\nBonferroni Correction:\")\nprint(f\"  Number of comparisons: {n_comparisons}\")\nprint(f\"  Adjusted Î±: {alpha_bonferroni:.4f}\")\nprint(f\"\\nPairwise t-tests:\")\n\nfor group1, group2 in comparisons:\n    data1 = data.filter(pl.col('genre') == group1).select('dimension_1').to_numpy().flatten()\n    data2 = data.filter(pl.col('genre') == group2).select('dimension_1').to_numpy().flatten()\n    \n    t_stat, p_val = ttest_ind(data1, data2)\n    \n    sig = \"***\" if p_val &lt; 0.001 else \"**\" if p_val &lt; 0.01 else \"*\" if p_val &lt; alpha_bonferroni else \"ns\"\n    \n    print(f\"  {group1} vs {group2}:\")\n    print(f\"    t = {t_stat:.2f}, p = {p_val:.6f} {sig}\")\n    print(f\"    Mean diff: {np.mean(data1) - np.mean(data2):.2f}\")\n\n\nBonferroni Correction:\n  Number of comparisons: 3\n  Adjusted Î±: 0.0167\n\nPairwise t-tests:\n  Fiction vs Academic:\n    t = 31.28, p = 0.000000 ***\n    Mean diff: 25.80\n  Fiction vs News:\n    t = 11.15, p = 0.000000 ***\n    Mean diff: 12.11\n  Academic vs News:\n    t = -13.78, p = 0.000000 ***\n    Mean diff: -13.69",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ANOVA and RÂ²: Comparing Groups and Explaining Variance</span>"
    ]
  },
  {
    "objectID": "tutorials/categorical-variables.html#anova-in-mda-example",
    "href": "tutorials/categorical-variables.html#anova-in-mda-example",
    "title": "9Â  ANOVA and RÂ²: Comparing Groups and Explaining Variance",
    "section": "9.6 ANOVA in MDA: Example",
    "text": "9.6 ANOVA in MDA: Example\nAfter extracting dimensions with Factor Analysis or PCA, we test if they distinguish genres:\n\n9.6.1 Load MDA Results\n\n# Assume we've run MDA on Brown Corpus and extracted dimension scores\n# In practice, this would come from pybiber:\n# df = pb.BiberAnalyzer(dfm_biber, id_column=True)\n# df.mda(n_factors=3)\n# dimension_scores = df.mda_scores  # DataFrame with dimension scores\n\n# For this tutorial, simulate realistic MDA dimension scores\nnp.random.seed(123)\n\ngenres = ['Fiction', 'Academic', 'News', 'Government', 'Humor'] * 30  # 150 texts\nn_texts = len(genres)\n\n# Dimension 1: Involved vs. Informational (Fiction high, Academic low)\ndim1_base = {'Fiction': 18, 'Academic': -15, 'News': 3, 'Government': -8, 'Humor': 12}\ndim1_scores = [np.random.normal(dim1_base[g], 6) for g in genres]\n\n# Dimension 2: Narrative vs. Non-narrative (Fiction high)\ndim2_base = {'Fiction': 14, 'Academic': -5, 'News': 4, 'Government': -3, 'Humor': 8}\ndim2_scores = [np.random.normal(dim2_base[g], 5) for g in genres]\n\n# Dimension 3: Abstract vs. Concrete (Academic high)\ndim3_base = {'Fiction': -2, 'Academic': 12, 'News': 1, 'Government': 6, 'Humor': -4}\ndim3_scores = [np.random.normal(dim3_base[g], 4) for g in genres]\n\nmda_data = pl.DataFrame({\n    'doc_id': [f'text_{i:03d}' for i in range(n_texts)],\n    'genre': genres,\n    'dimension_1': dim1_scores,\n    'dimension_2': dim2_scores,\n    'dimension_3': dim3_scores\n})\n\nprint(\"MDA Dimension Scores:\")\nprint(mda_data.head(10))\n\nMDA Dimension Scores:\nshape: (10, 5)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ doc_id   â”† genre      â”† dimension_1 â”† dimension_2 â”† dimension_3 â”‚\nâ”‚ ---      â”† ---        â”† ---         â”† ---         â”† ---         â”‚\nâ”‚ str      â”† str        â”† f64         â”† f64         â”† f64         â”‚\nâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ text_000 â”† Fiction    â”† 11.486216   â”† 15.269082   â”† 1.060219    â”‚\nâ”‚ text_001 â”† Academic   â”† -9.015927   â”† -3.581373   â”† 8.684045    â”‚\nâ”‚ text_002 â”† News       â”† 4.697871    â”† -3.059444   â”† -1.636605   â”‚\nâ”‚ text_003 â”† Government â”† -17.037768  â”† -12.384343  â”† 8.444494    â”‚\nâ”‚ text_004 â”† Humor      â”† 8.528398    â”† 2.901725    â”† -4.576053   â”‚\nâ”‚ text_005 â”† Fiction    â”† 27.908619   â”† 14.839711   â”† 3.266422    â”‚\nâ”‚ text_006 â”† Academic   â”† -29.560075  â”† -2.230719   â”† 9.182631    â”‚\nâ”‚ text_007 â”† News       â”† 0.426524    â”† 1.346627    â”† 4.00244     â”‚\nâ”‚ text_008 â”† Government â”† -0.404382   â”† 3.886287    â”† 7.370552    â”‚\nâ”‚ text_009 â”† Humor      â”† 6.799558    â”† 7.28412     â”† -4.50575    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n\n9.6.2 Test Each Dimension\n\n# Separate by genre using Polars and convert to numpy\ngenre_groups = {\n    genre: mda_data.filter(pl.col('genre') == genre)\n    for genre in ['Fiction', 'Academic', 'News', 'Government', 'Humor']\n}\n\n# Test each dimension\nfor dim in ['dimension_1', 'dimension_2', 'dimension_3']:\n    print(f\"\\n{'='*60}\")\n    print(f\"ANOVA for {dim.replace('_', ' ').title()}\")\n    print('='*60)\n    \n    # Extract dimension scores by genre (convert to numpy arrays)\n    groups = [genre_groups[g][dim].to_numpy() for g in ['Fiction', 'Academic', 'News', 'Government', 'Humor']]\n    \n    # One-way ANOVA\n    f_stat, p_val = stats.f_oneway(*groups)\n    \n    # Compute eta-squared\n    all_scores = mda_data.select(dim).to_numpy().flatten()\n    grand_mean = np.mean(all_scores)\n    ss_total = np.sum((all_scores - grand_mean)**2)\n    \n    ss_between = sum([\n        len(group) * (np.mean(group) - grand_mean)**2 \n        for group in groups\n    ])\n    \n    eta_sq = ss_between / ss_total\n    \n    # Report\n    print(f\"  F({4}, {n_texts-5}) = {f_stat:.2f}, p = {p_val:.6f}\")\n    print(f\"  Î·Â² = {eta_sq:.3f} ({eta_sq*100:.1f}% variance explained)\")\n    \n    if eta_sq &gt;= 0.14:\n        effect = \"Large\"\n    elif eta_sq &gt;= 0.06:\n        effect = \"Medium\"\n    elif eta_sq &gt;= 0.01:\n        effect = \"Small\"\n    else:\n        effect = \"Negligible\"\n    \n    print(f\"  Effect size: {effect}\")\n    \n    # Interpretation\n    if p_val &lt; 0.001:\n        print(f\"  *** Dimension strongly distinguishes genres (p &lt; 0.001)\")\n    elif p_val &lt; 0.05:\n        print(f\"  * Dimension distinguishes genres (p &lt; 0.05)\")\n    else:\n        print(f\"  Dimension does not distinguish genres (p â‰¥ 0.05)\")\n\n\n============================================================\nANOVA for Dimension 1\n============================================================\n  F(4, 145) = 130.99, p = 0.000000\n  Î·Â² = 0.783 (78.3% variance explained)\n  Effect size: Large\n  *** Dimension strongly distinguishes genres (p &lt; 0.001)\n\n============================================================\nANOVA for Dimension 2\n============================================================\n  F(4, 145) = 77.39, p = 0.000000\n  Î·Â² = 0.681 (68.1% variance explained)\n  Effect size: Large\n  *** Dimension strongly distinguishes genres (p &lt; 0.001)\n\n============================================================\nANOVA for Dimension 3\n============================================================\n  F(4, 145) = 88.82, p = 0.000000\n  Î·Â² = 0.710 (71.0% variance explained)\n  Effect size: Large\n  *** Dimension strongly distinguishes genres (p &lt; 0.001)\n\n\nInterpretation:\n\nHigh Î·Â² (&gt; 0.14): Dimension is strong genre marker (use for classification)\nModerate Î·Â² (0.06-0.14): Dimension shows genre patterns (interesting for interpretation)\nLow Î·Â² (&lt; 0.06): Dimension captures variation within genres (individual differences, not genre)\n\nIn MDA publications, report Î·Â² for each dimension to show which dimensions best distinguish categories.\n\n\n9.6.3 Visualize Dimension Performance\n\n# Compute eta-squared for all dimensions\neta_squared_results = []\n\nfor dim in ['dimension_1', 'dimension_2', 'dimension_3']:\n    all_scores = mda_data.select(dim).to_numpy().flatten()\n    grand_mean = np.mean(all_scores)\n    ss_total = np.sum((all_scores - grand_mean)**2)\n    \n    groups = [genre_groups[g][dim].to_numpy() for g in ['Fiction', 'Academic', 'News', 'Government', 'Humor']]\n    ss_between = sum([len(group) * (np.mean(group) - grand_mean)**2 for group in groups])\n    \n    eta_sq = ss_between / ss_total\n    eta_squared_results.append({'Dimension': dim.replace('_', ' ').title(), 'Eta_Squared': eta_sq})\n\neta_df = pd.DataFrame(eta_squared_results)\n\nplt.figure(figsize=(10, 6))\nbars = plt.bar(eta_df['Dimension'], eta_df['Eta_Squared'], color=['#1f77b4', '#ff7f0e', '#2ca02c'], alpha=0.8)\n\n# Add effect size threshold lines\nplt.axhline(y=0.14, color='red', linestyle='--', alpha=0.7, label='Large effect (Î·Â² = 0.14)')\nplt.axhline(y=0.06, color='orange', linestyle='--', alpha=0.7, label='Medium effect (Î·Â² = 0.06)')\nplt.axhline(y=0.01, color='yellow', linestyle='--', alpha=0.7, label='Small effect (Î·Â² = 0.01)')\n\n# Add value labels on bars\nfor bar in bars:\n    height = bar.get_height()\n    plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n             f'{height:.3f}\\n({height*100:.1f}%)',\n             ha='center', va='bottom', fontsize=12, fontweight='bold')\n\nplt.ylabel('Eta-Squared (Î·Â²)', fontsize=14)\nplt.xlabel('Dimension', fontsize=14)\nplt.title('Proportion of Variance Explained by Genre Membership', fontsize=16)\nplt.ylim(0, max(eta_df['Eta_Squared']) * 1.2)\nplt.legend(fontsize=10)\nplt.grid(axis='y', alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigureÂ 9.3: Variance explained (Î·Â²) by each dimension. Higher values indicate stronger genre differentiation.\n\n\n\n\n\nDecision guide:\n\nDimensions with Î·Â² &gt; 0.14 are strong genre markers (feature in main analysis)\nDimensions with Î·Â² &lt; 0.06 may capture stylistic variation orthogonal to genre (secondary interest)",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ANOVA and RÂ²: Comparing Groups and Explaining Variance</span>"
    ]
  },
  {
    "objectID": "tutorials/categorical-variables.html#assumptions-and-diagnostics",
    "href": "tutorials/categorical-variables.html#assumptions-and-diagnostics",
    "title": "9Â  ANOVA and RÂ²: Comparing Groups and Explaining Variance",
    "section": "9.7 Assumptions and Diagnostics",
    "text": "9.7 Assumptions and Diagnostics\nANOVA makes assumptions that should be checked:\n\n9.7.1 1. Independence\nAssumption: Observations are independent (one textâ€™s score doesnâ€™t affect anotherâ€™s).\nViolation: Clustered data (multiple texts by same author, same time period).\nCheck: Know your data structure. If texts are nested (authors within genres), use nested ANOVA or mixed-effects models.\nIn text analysis: Usually okay if texts are from different sources. Problematic if entire corpus is one authorâ€™s works.\n\n\n9.7.2 2. Normality\nAssumption: Dimension scores are approximately normally distributed within each group.\nViolation: Severely skewed or bimodal distributions.\nCheck: Histogram or Q-Q plot for each group.\nRobustness: ANOVA is fairly robust to non-normality with moderate sample sizes (n &gt; 30 per group).\nAlternative: If severely violated, use Kruskal-Wallis test (non-parametric ANOVA).\n\n# Check normality for each group (Dimension 1)\nfrom scipy.stats import shapiro\n\nprint(\"\\nNormality Tests (Shapiro-Wilk):\")\nfor genre in ['Fiction', 'Academic', 'News']:\n    genre_data = data.filter(pl.col('genre') == genre).select('dimension_1').to_numpy().flatten()\n    stat, p = shapiro(genre_data)\n    \n    print(f\"  {genre}: W = {stat:.4f}, p = {p:.4f}\")\n    if p &lt; 0.05:\n        print(f\"    â†’ Non-normal distribution (p &lt; 0.05)\")\n    else:\n        print(f\"    â†’ Normal distribution (p â‰¥ 0.05)\")\n\n\nNormality Tests (Shapiro-Wilk):\n  Fiction: W = 0.9827, p = 0.6722\n    â†’ Normal distribution (p â‰¥ 0.05)\n  Academic: W = 0.9713, p = 0.2616\n    â†’ Normal distribution (p â‰¥ 0.05)\n  News: W = 0.9775, p = 0.4534\n    â†’ Normal distribution (p â‰¥ 0.05)\n\n\nInterpretation: p &gt; 0.05 suggests normality assumption is met. If p &lt; 0.05 but sample is large and distributions are symmetric, ANOVA still appropriate.\n\n\n9.7.3 3. Homogeneity of Variance\nAssumption: Variance is equal across groups (homoscedasticity).\nViolation: One group has much larger variance than others.\nCheck: Leveneâ€™s test or variance ratio (largest SD / smallest SD &lt; 2).\nRobustness: ANOVA is robust if group sizes are equal. Violated assumption + unequal sizes = unreliable.\nAlternative: If violated, use Welchâ€™s ANOVA (doesnâ€™t assume equal variances).\n\n# Levene's test for homogeneity of variance\nfrom scipy.stats import levene\n\nfiction = data.filter(pl.col('genre') == 'Fiction').select('dimension_1').to_numpy().flatten()\nacademic = data.filter(pl.col('genre') == 'Academic').select('dimension_1').to_numpy().flatten()\nnews = data.filter(pl.col('genre') == 'News').select('dimension_1').to_numpy().flatten()\n\nstat, p = levene(fiction, academic, news)\n\nprint(f\"\\nLevene's Test for Homogeneity of Variance:\")\nprint(f\"  W = {stat:.4f}, p = {p:.4f}\")\n\nif p &lt; 0.05:\n    print(f\"  â†’ Variances differ significantly (p &lt; 0.05)\")\n    print(f\"  â†’ Consider Welch's ANOVA or transformation\")\nelse:\n    print(f\"  â†’ Homogeneity assumption met (p â‰¥ 0.05)\")\n\n\nLevene's Test for Homogeneity of Variance:\n  W = 6.1480, p = 0.0027\n  â†’ Variances differ significantly (p &lt; 0.05)\n  â†’ Consider Welch's ANOVA or transformation\n\n\n\n\n9.7.4 Welchâ€™s ANOVA (Robust Alternative)\n\n# Welch's ANOVA (doesn't assume equal variances)\n# Note: scipy doesn't have built-in Welch ANOVA, but we can use statsmodels\n# For demonstration, show concept\n\nprint(\"\\nWelch's ANOVA (if Levene's test failed):\")\nprint(\"  Use this when groups have unequal variances\")\nprint(\"  Similar to regular ANOVA but adjusts degrees of freedom\")\nprint(\"  Generally more robust for real-world data\")\n\n# scipy.stats doesn't have Welch ANOVA; use regular ANOVA if variances are reasonable\n# Or use statsmodels or pingouin package for Welch's test\n\n\nWelch's ANOVA (if Levene's test failed):\n  Use this when groups have unequal variances\n  Similar to regular ANOVA but adjusts degrees of freedom\n  Generally more robust for real-world data",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ANOVA and RÂ²: Comparing Groups and Explaining Variance</span>"
    ]
  },
  {
    "objectID": "tutorials/categorical-variables.html#reporting-results",
    "href": "tutorials/categorical-variables.html#reporting-results",
    "title": "9Â  ANOVA and RÂ²: Comparing Groups and Explaining Variance",
    "section": "9.8 Reporting Results",
    "text": "9.8 Reporting Results\n\n9.8.1 APA Style\nExample (for publication):\n\nA one-way analysis of variance (ANOVA) tested whether genres differed on Dimension 1 (Involved vs.Â Informational Production). Results showed significant differences among Fiction, Academic, and News genres, F(2, 147) = 183.2, p &lt; .001, Î·Â² = .71. Post-hoc comparisons using Tukeyâ€™s HSD indicated that all three genres differed significantly (all ps &lt; .001). Fiction texts scored highest (M = 15.2, SD = 5.1), followed by News (M = 2.1, SD = 6.0), with Academic texts scoring lowest (M = -12.1, SD = 4.3). Genre membership explained 71% of the variance in Dimension 1 scores, indicating that this dimension is a robust marker of genre differences.\n\nComponents:\n\nTest type: One-way ANOVA\nWhat was tested: Dimension 1 across genres\nF-statistic with df: F(2, 147) = 183.2\nP-value: p &lt; .001 (exact p if &gt; .001)\nEffect size: Î·Â² = .71\nPost-hoc results: Which pairs differ (Tukeyâ€™s HSD)\nDescriptive statistics: Means and SDs for each group\nInterpretation: What the effect size means substantively\n\n\n\n9.8.2 In Tables\nTable 1: ANOVA Results for MDA Dimensions\n\n\n\n\n\n\n\n\n\n\nDimension\nF(df1, df2)\np\nÎ·Â²\nInterpretation\n\n\n\n\nDimension 1: Involved vs.Â Informational\n183.2 (2, 147)\n&lt;.001\n.71\nLarge effect; strong genre marker\n\n\nDimension 2: Narrative vs.Â Non-narrative\n45.6 (2, 147)\n&lt;.001\n.38\nLarge effect; distinguishes fiction\n\n\nDimension 3: Abstract vs.Â Concrete\n12.3 (2, 147)\n&lt;.001\n.14\nMedium effect; weak differentiation\n\n\n\nTable 2: Genre Means by Dimension\n\n\n\n\n\n\n\n\n\nGenre\nDimension 1 (M, SD)\nDimension 2 (M, SD)\nDimension 3 (M, SD)\n\n\n\n\nFiction\n15.2 (5.1)\n14.3 (4.8)\n-2.1 (3.9)\n\n\nAcademic\n-12.1 (4.3)\n-5.2 (5.1)\n11.8 (4.2)\n\n\nNews\n2.1 (6.0)\n3.9 (5.4)\n1.2 (4.5)",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ANOVA and RÂ²: Comparing Groups and Explaining Variance</span>"
    ]
  },
  {
    "objectID": "tutorials/categorical-variables.html#connections-to-other-methods",
    "href": "tutorials/categorical-variables.html#connections-to-other-methods",
    "title": "9Â  ANOVA and RÂ²: Comparing Groups and Explaining Variance",
    "section": "9.9 Connections to Other Methods",
    "text": "9.9 Connections to Other Methods\n\n9.9.1 ANOVA and MDA\nStandard MDA workflow:\n\nExtract dimensions (Factor Analysis or PCA)\nCompute dimension scores for each text\nANOVA: Test if genres differ on each dimension\nRÂ²/Î·Â²: Quantify how much genre explains variance\nPost-hoc: Identify which genre pairs differ\n\nWhy this matters: Dimensions are only useful if they distinguish categories. ANOVA + RÂ² validate that extracted dimensions are meaningful.\nExample: If Dimension 1 has Î·Â² &lt; 0.05, it captures individual variation (stylistic idiosyncrasies) but not genre patterns. Consider dropping or reinterpreting.\n\n\n9.9.2 ANOVA and Classification\nFeature selection:\n\nUse ANOVA to identify which features differ significantly across classes\nSelect features with high F-statistics and Î·Â² for classification models\nReduces dimensionality (keep only discriminating features)\n\nExample: You have 67 linguistic features. ANOVA shows 15 have Î·Â² &gt; 0.10 for genre. Use those 15 as classifier inputs.\n\n\n9.9.3 ANOVA and Regression\nRÂ² in regression measures variance explained by predictor(s):\n\\[R^2 = 1 - \\frac{SS_{residual}}{SS_{total}}\\]\nIn ANOVA context:\n\nSingle categorical predictor (genre) â†’ RÂ² = Î·Â²\nContinuous predictor (word length) â†’ RÂ² from linear regression\nMultiple predictors â†’ Multiple RÂ²\n\nConnection: ANOVA is a special case of regression where predictors are categorical.\n\n\n9.9.4 ANOVA and Time Series\nDiachronic analysis: Do linguistic features change across time periods?\nApplication:\n\nBin texts by decade (1800s, 1900s, 2000s)\nCompute feature rates (e.g., passive frequency)\nANOVA: Does passive rate differ by decade?\nRÂ²: How much temporal variation is explained?\n\nExample: If Î·Â² = 0.45 for â€œpassives across centuries,â€ historical period explains nearly half the variationâ€”strong evidence of language change.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ANOVA and RÂ²: Comparing Groups and Explaining Variance</span>"
    ]
  },
  {
    "objectID": "tutorials/categorical-variables.html#methodological-considerations",
    "href": "tutorials/categorical-variables.html#methodological-considerations",
    "title": "9Â  ANOVA and RÂ²: Comparing Groups and Explaining Variance",
    "section": "9.10 Methodological Considerations",
    "text": "9.10 Methodological Considerations\n\n9.10.1 1. Sample Size\nMinimum: 20-30 observations per group for reliable results.\nPower: Larger samples detect smaller effects. With n = 500 per group, even Î·Â² = 0.02 is detectable.\nBalance: Equal group sizes maximize power. Unequal sizes (50 vs.Â 500) reduce ability to detect differences.\nIn text analysis: Aim for balanced sampling (50 fiction, 50 academic, 50 news) rather than opportunistic corpora (500 fiction, 12 academic).\n\n\n9.10.2 2. Multiple Testing\nProblem: Testing 10 dimensions across 5 genres = 10 ANOVAs. Chance of false positive increases.\nSolution:\n\nBonferroni correction: Divide Î± by number of tests (0.05 / 10 = 0.005)\nFalse Discovery Rate (FDR): Controls expected proportion of false positives (less conservative)\nPre-registration: Specify which tests are confirmatory vs.Â exploratory\n\n\n\n9.10.3 3. Practical vs.Â Statistical Significance\nLarge samples (n &gt; 1000): Tiny effects become â€œsignificantâ€ (p &lt; 0.05) even if meaningless.\nExample: Î·Â² = 0.008 (0.8% variance explained) with p &lt; 0.001. Statistically significant but trivial effect.\nBest practice: Always report effect size (Î·Â², not just p-value). Focus on Î·Â² &gt; 0.06 (medium+) for substantive claims.\n\n\n9.10.4 4. Nested Data\nProblem: Texts arenâ€™t truly independent (multiple texts per author, per time period).\nExample: 50 fiction texts by 5 authors (10 texts each). Texts by same author are correlated (shared style).\nSolution: Mixed-effects models or nested ANOVA accounting for clustering:\n\nLevel 1: Texts\nLevel 2: Authors\nLevel 3: Genres\n\nIn Python: Use statsmodels.MixedLM or pymer4.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ANOVA and RÂ²: Comparing Groups and Explaining Variance</span>"
    ]
  },
  {
    "objectID": "tutorials/categorical-variables.html#ethical-considerations",
    "href": "tutorials/categorical-variables.html#ethical-considerations",
    "title": "9Â  ANOVA and RÂ²: Comparing Groups and Explaining Variance",
    "section": "9.11 Ethical Considerations",
    "text": "9.11 Ethical Considerations\n\n9.11.1 1. P-Hacking\nRisk: Running many ANOVAs, reporting only significant ones.\nExample: Test 50 dimensions, report the 5 with p &lt; 0.05 (ignoring 45 non-significant tests).\nSolution: Pre-register analyses, report all tests, or adjust Î± for multiple comparisons.\n\n\n9.11.2 2. HARKing (Hypothesizing After Results Known)\nRisk: Presenting post-hoc patterns as if they were predicted a priori.\nExample: Dimension 3 unexpectedly distinguishes Government from News. Retroactively claim you hypothesized this (you didnâ€™t).\nSolution: Distinguish exploratory from confirmatory findings. Exploratory findings are valuable but need replication.\n\n\n9.11.3 3. Overgeneralizing\nRisk: Assuming patterns in one corpus apply universally.\nExample: â€œAcademic writing is always highly informational (Î·Â² = 0.60 in my corpus of chemistry papers).â€ But humanities papers might differ.\nSolution: Specify corpus boundaries. Replicate across multiple corpora before generalizing.\n\n\n9.11.4 4. Ignoring Context\nRisk: Treating statistical differences as natural categories.\nExample: Fiction scores +15, Academic -12 on Dimension 1. Concluding â€œFiction is fundamentally different from Academic writingâ€ ignores historical, social, and institutional forces shaping these genres.\nSolution: Statistical patterns describe, they donâ€™t explain. Combine quantitative findings with qualitative interpretation and theoretical framing.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ANOVA and RÂ²: Comparing Groups and Explaining Variance</span>"
    ]
  },
  {
    "objectID": "tutorials/categorical-variables.html#summary",
    "href": "tutorials/categorical-variables.html#summary",
    "title": "9Â  ANOVA and RÂ²: Comparing Groups and Explaining Variance",
    "section": "9.12 Summary",
    "text": "9.12 Summary\nANOVA (Analysis of Variance) tests if group means differ significantly:\n\nF-statistic: Ratio of between-group to within-group variance\nP-value: Probability groups differ by chance (p &lt; 0.05 = significant)\nAssumptions: Independence, normality, homogeneity of variance\n\nRÂ²/Î·Â² (coefficient of determination/eta-squared) quantifies proportion of variance explained:\n\nFormula: Î·Â² = SS_between / SS_total\nInterpretation: 0 to 1 (0% to 100% variance explained)\nEffect sizes: Small (0.01), Medium (0.06), Large (0.14)\n\nIn MDA context:\n\nAfter extracting dimensions, ANOVA tests if they distinguish genres\nÎ·Â² shows which dimensions are strong vs.Â weak category markers\nPost-hoc tests (Tukey, Bonferroni) identify which specific pairs differ\n\nKey insights:\n\nVariance decomposition: Total variance = between-group + within-group\nF-statistic: MS_between / MS_within (large F = strong evidence of differences)\nEffect size matters: Report Î·Â², not just p-values (practical vs.Â statistical significance)\nAssumptions: Check normality, homogeneity; use robust alternatives if violated\nMultiple comparisons: Adjust Î± when testing many groups/dimensions\n\nBest practices:\n\nReport full ANOVA table (SS, df, MS, F, p, Î·Â²)\nInclude descriptive statistics (means, SDs by group)\nUse post-hoc tests for pairwise comparisons\nReport effect sizes, not just significance\nCheck assumptions; use robust methods if violated\n\nNext steps: Apply ANOVA to your MDA results. Test which dimensions distinguish genres, quantify effect sizes, identify specific group differences. Combine statistical tests with substantive interpretation grounded in linguistic theory.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ANOVA and RÂ²: Comparing Groups and Explaining Variance</span>"
    ]
  },
  {
    "objectID": "tutorials/categorical-variables.html#further-reading",
    "href": "tutorials/categorical-variables.html#further-reading",
    "title": "9Â  ANOVA and RÂ²: Comparing Groups and Explaining Variance",
    "section": "9.13 Further Reading",
    "text": "9.13 Further Reading\nFoundational:\n\nCohen, J. (1988). Statistical Power Analysis for the Behavioral Sciences (2nd ed.). Routledge. (Effect sizes, power)\nField, A., Miles, J., & Field, Z. (2012). Discovering Statistics Using R. Sage. (Chapter 10: ANOVA)\n\nANOVA specifics:\n\nMaxwell, S. E., & Delaney, H. D. (2004). Designing Experiments and Analyzing Data (2nd ed.). Psychology Press. (Comprehensive ANOVA coverage)\nKeppel, G., & Wickens, T. D. (2004). Design and Analysis: A Researcherâ€™s Handbook (4th ed.). Pearson. (Experimental design, post-hoc tests)\n\nEffect sizes:\n\nLakens, D. (2013). Calculating and reporting effect sizes to facilitate cumulative science. Frontiers in Psychology, 4, 863. DOI\nFritz, C. O., Morris, P. E., & Richler, J. J. (2012). Effect size estimates: Current use, calculations, and interpretation. Journal of Experimental Psychology: General, 141(1), 2-18. DOI\n\nMDA applications:\n\nBiber, D. (1988). Variation Across Speech and Writing. Cambridge University Press. (Foundational MDA, uses ANOVA to validate dimensions)\nBiber, D., & Conrad, S. (2009). Register, Genre, and Style. Cambridge University Press. (Chapter 6: MDA methodology)\n\nPractical guides:\n\nScipy documentation: scipy.stats.f_oneway\nStatsmodels: ANOVA\nPingouin: ANOVA and post-hoc tests",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>9</span>Â  <span class='chapter-title'>ANOVA and RÂ²: Comparing Groups and Explaining Variance</span>"
    ]
  },
  {
    "objectID": "tutorials/cluster-analysis.html",
    "href": "tutorials/cluster-analysis.html",
    "title": "10Â  Clustering Methods",
    "section": "",
    "text": "10.1 Introduction\nClustering is an unsupervised machine learning technique that groups similar observations together without predefined labels. Unlike classification (which requires labeled training data), clustering discovers patterns and structures in data automatically.\nWhy clustering matters for humanities research:\nThis tutorial introduces two fundamental clustering methods widely used in humanities text analysis:\nBoth methods appear throughout this course: hierarchical clustering in time series analysis (Variability-Based Neighbor Clustering), topic modeling (document similarity), and spaCy workflows; k-means in contextual embeddings for semantic grouping.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>Clustering Methods</span>"
    ]
  },
  {
    "objectID": "tutorials/cluster-analysis.html#introduction",
    "href": "tutorials/cluster-analysis.html#introduction",
    "title": "10Â  Clustering Methods",
    "section": "",
    "text": "Exploratory analysis: Discover natural groupings in texts, time periods, or authors\nPattern recognition: Identify thematic similarities across disparate sources\nPeriodization: Group time periods by linguistic similarity rather than arbitrary boundaries\nDocument organization: Sort large corpora into coherent themes or styles\nHypothesis generation: Let data suggest which texts merit comparative close reading\n\n\n\nHierarchical Clustering: Creates nested groupings visualized as dendrograms (tree diagrams)\nK-Means Clustering: Partitions data into a specified number of distinct groups",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>Clustering Methods</span>"
    ]
  },
  {
    "objectID": "tutorials/cluster-analysis.html#core-concepts",
    "href": "tutorials/cluster-analysis.html#core-concepts",
    "title": "10Â  Clustering Methods",
    "section": "10.2 Core Concepts",
    "text": "10.2 Core Concepts\n\n10.2.1 What Makes Observations â€œSimilarâ€?\nClustering requires a distance metric to measure similarity:\n\nEuclidean distance: Straight-line distance in multi-dimensional space (most common)\nCosine similarity: Angle between vectors (often used for text, converted to distance)\nManhattan distance: Sum of absolute differences along each dimension\nCorrelation distance: Based on correlation coefficient\n\nFor text data, we typically represent documents as:\n\nWord frequency vectors: Each dimension is a word, each value is a count/proportion\nTF-IDF vectors: Weighted by how distinctive each word is\nTopic distributions: Proportions across latent topics (from topic models)\nEmbeddings: Dense semantic vectors from neural models (word2vec, BERT)\n\n\n\n10.2.2 Unsupervised vs.Â Supervised\nClustering (unsupervised):\n\nNo predefined labels (â€œwartime speechâ€, â€œHamiltonâ€)\nAlgorithm discovers groupings based on similarity\nResearcher interprets what clusters mean\nExploratory, hypothesis-generating\n\nClassification (supervised):\n\nRequires labeled training data\nAlgorithm learns to predict labels for new observations\nConfirmatory, hypothesis-testing\nSee classification tutorial for details",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>Clustering Methods</span>"
    ]
  },
  {
    "objectID": "tutorials/cluster-analysis.html#hierarchical-clustering",
    "href": "tutorials/cluster-analysis.html#hierarchical-clustering",
    "title": "10Â  Clustering Methods",
    "section": "10.3 Hierarchical Clustering",
    "text": "10.3 Hierarchical Clustering\nHierarchical clustering creates a tree structure (dendrogram) showing how observations group at different similarity thresholds. Itâ€™s â€œbottom-upâ€ (agglomerative): each observation starts as its own cluster, then pairs merge iteratively based on similarity.\n\n10.3.1 How It Works\n\nStart: Each observation is its own cluster (n clusters)\nMerge: Find the two closest clusters and combine them\nRepeat: Continue merging until all observations are in one cluster\nResult: Tree structure showing all possible groupings\n\n\n\n10.3.2 Linkage Methods\nLinkage defines how to measure distance between clusters (not individual points):\nSingle linkage (nearest neighbor): - Distance = minimum distance between any two points in different clusters - Creates â€œchainingâ€ effect (long, stretched clusters) - Sensitive to outliers - Rarely used for text\nComplete linkage (farthest neighbor): - Distance = maximum distance between any two points in different clusters - Creates compact, spherical clusters - Good for well-separated groups\nAverage linkage: - Distance = average of all pairwise distances between clusters - Compromise between single and complete - Robust to noise\nWardâ€™s linkage (minimum variance): - Minimizes within-cluster variance at each merge - Creates balanced, compact clusters - Most common for humanities text analysis - Default in many tools\n\n\n10.3.3 Example: Inaugural Address Periodization\nLetâ€™s cluster presidential inaugural addresses by word frequencies to discover historical periods:\n\nimport polars as pl\nimport numpy as np\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\nfrom scipy.spatial.distance import pdist, squareform\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load inaugural address data (word frequencies by document)\nurl = \"https://github.com/browndw/humanities_analytics/raw/refs/heads/main/data/data_tables/inaugural_subset.csv\"\ninaugural_df = pl.read_csv(url)\n\nprint(f\"Data shape: {inaugural_df.shape}\")\nprint(\"\\nFirst few rows:\")\nprint(inaugural_df.head())\n\nData shape: (32, 4)\n\nFirst few rows:\nshape: (5, 4)\nâ”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ year â”† president â”† doc_id         â”† text                            â”‚\nâ”‚ ---  â”† ---       â”† ---            â”† ---                             â”‚\nâ”‚ i64  â”† str       â”† str            â”† str                             â”‚\nâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ 1901 â”† McKinley  â”† 1901-McKinley  â”† My fellow-citizens, when we asâ€¦ â”‚\nâ”‚ 1905 â”† Roosevelt â”† 1905-Roosevelt â”† My fellow citizens, no people â€¦ â”‚\nâ”‚ 1909 â”† Taft      â”† 1909-Taft      â”† My fellow citizens: Anyone whoâ€¦ â”‚\nâ”‚ 1913 â”† Wilson    â”† 1913-Wilson    â”† There has been a change of govâ€¦ â”‚\nâ”‚ 1917 â”† Wilson    â”† 1917-Wilson    â”† My Fellow citizens: The four yâ€¦ â”‚\nâ””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nData structure: Each row is a president, each column is a word frequency (or TF-IDF score).\n\n# Prepare data for clustering\n# Extract document identifiers and feature matrix\ndoc_ids = inaugural_df.select('doc_id').to_numpy().flatten()\n\n# Select only numeric columns (word frequencies, TF-IDF scores, etc.)\n# Include additional numeric types to capture all possible numeric columns\nnumeric_cols = [col for col in inaugural_df.columns \n                if inaugural_df[col].dtype in [pl.Float64, pl.Float32, pl.Int64, pl.Int32, \n                                                pl.UInt32, pl.UInt64, pl.Int16, pl.Int8, \n                                                pl.UInt16, pl.UInt8]]\n\n# If no numeric columns found, select all columns except known metadata\nif len(numeric_cols) == 0:\n    print(\"Warning: No numeric columns detected. Using all columns except metadata.\")\n    numeric_cols = [col for col in inaugural_df.columns \n                   if col not in ['doc_id', 'year', 'president', 'party', 'text', 'full_text']]\n\nprint(f\"Selected {len(numeric_cols)} numeric columns\")\n\n# Convert to numpy array (observations Ã— features) with explicit float dtype\nX = inaugural_df.select(numeric_cols).to_numpy().astype(np.float64)\n\nprint(f\"Feature matrix: {X.shape} ({X.shape[0]} documents, {X.shape[1]} features)\")\n\nSelected 1 numeric columns\nFeature matrix: (32, 1) (32 documents, 1 features)\n\n\n\n\n10.3.4 Compute Hierarchical Clustering\n\n# Compute pairwise distances (Euclidean)\ndistances = pdist(X, metric='euclidean')\n\n# Perform hierarchical clustering (Ward's linkage)\nlinkage_matrix = linkage(distances, method='ward')\n\nprint(f\"Linkage matrix shape: {linkage_matrix.shape}\")\nprint(\"\\nLast 5 merges (final clustering steps):\")\nprint(linkage_matrix[-5:])\n\nLinkage matrix shape: (31, 4)\n\nLast 5 merges (final clustering steps):\n[[ 52.          53.          32.           8.        ]\n [ 54.          55.          32.           8.        ]\n [ 56.          57.          90.50966799  16.        ]\n [ 58.          59.          90.50966799  16.        ]\n [ 60.          61.         256.          32.        ]]\n\n\nUnderstanding the linkage matrix: Each row represents one merge:\n\nColumns 0-1: Which clusters merged\nColumn 2: Distance at which they merged\nColumn 3: Number of observations in new cluster\n\n\n\n10.3.5 Visualize: Dendrogram\n\nplt.figure(figsize=(12, 6))\n\ndendrogram(\n    linkage_matrix,\n    labels=doc_ids,\n    leaf_font_size=10,\n    leaf_rotation=90\n)\n\nplt.title(\"Hierarchical Clustering of Inaugural Addresses (Ward's Linkage)\", fontsize=14)\nplt.xlabel(\"Document (President-Year)\", fontsize=12)\nplt.ylabel(\"Distance (Ward)\", fontsize=12)\nplt.axhline(y=50, color='red', linestyle='--', label='Suggested cut height')\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigureÂ 10.1: Hierarchical clustering of inaugural addresses (Wardâ€™s linkage). Height indicates dissimilarity; cutting at different heights produces different numbers of clusters.\n\n\n\n\n\nReading the dendrogram:\n\nX-axis: Individual documents (leaves)\nY-axis: Distance at which clusters merge (dissimilarity)\nHeight of branches: Smaller height = more similar\nHorizontal cut line: Determines number of clusters\n\nDocuments connected at lower heights are more similar. Cutting the tree at different heights creates different numbers of clusters.\n\n\n10.3.6 Choosing Number of Clusters\nVisual inspection: Look for long vertical branches (large distances between merges) suggesting natural groupings.\nScree plot (elbow method): Plot distances where merges occur, look for â€œelbowâ€ where merging becomes less informative:\n\n# Extract distances from last 20 merges\nlast_merges = linkage_matrix[-20:, 2]\nn_clusters = np.arange(1, 21)\n\nplt.figure(figsize=(8, 5))\nplt.plot(n_clusters, last_merges[::-1], marker='o')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Merge Distance')\nplt.title('Scree Plot: Merge Distances for Last 20 Steps')\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigureÂ 10.2: Scree plot showing merge distances. The â€˜elbowâ€™ suggests optimal number of clusters.\n\n\n\n\n\nInterpretation: Look for where the curve â€œbendsâ€ (the elbow). Before the elbow, merges create distinct clusters; after, youâ€™re just splitting similar observations.\n\n\n10.3.7 Assign Cluster Labels\n\n# Cut dendrogram at specified height or number of clusters\nn_clusters = 4\ncluster_labels = fcluster(linkage_matrix, n_clusters, criterion='maxclust')\n\n# Add to dataframe\ninaugural_clustered = inaugural_df.with_columns(\n    pl.Series('cluster', cluster_labels)\n)\n\nprint(f\"Cluster assignments (first 10):\")\nprint(inaugural_clustered.select(['doc_id', 'year', 'cluster']).head(10))\n\nprint(\"\\nCluster sizes:\")\nprint(inaugural_clustered.group_by('cluster').agg(pl.count()).sort('cluster'))\n\nCluster assignments (first 10):\nshape: (10, 3)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ doc_id         â”† year â”† cluster â”‚\nâ”‚ ---            â”† ---  â”† ---     â”‚\nâ”‚ str            â”† i64  â”† i32     â”‚\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•¡\nâ”‚ 1901-McKinley  â”† 1901 â”† 1       â”‚\nâ”‚ 1905-Roosevelt â”† 1905 â”† 1       â”‚\nâ”‚ 1909-Taft      â”† 1909 â”† 1       â”‚\nâ”‚ 1913-Wilson    â”† 1913 â”† 1       â”‚\nâ”‚ 1917-Wilson    â”† 1917 â”† 1       â”‚\nâ”‚ 1921-Harding   â”† 1921 â”† 1       â”‚\nâ”‚ 1925-Coolidge  â”† 1925 â”† 1       â”‚\nâ”‚ 1929-Hoover    â”† 1929 â”† 1       â”‚\nâ”‚ 1933-Roosevelt â”† 1933 â”† 2       â”‚\nâ”‚ 1937-Roosevelt â”† 1937 â”† 2       â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nCluster sizes:\nshape: (4, 2)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ cluster â”† count â”‚\nâ”‚ ---     â”† ---   â”‚\nâ”‚ i32     â”† u32   â”‚\nâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\nâ”‚ 1       â”† 8     â”‚\nâ”‚ 2       â”† 8     â”‚\nâ”‚ 3       â”† 8     â”‚\nâ”‚ 4       â”† 8     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n/tmp/ipykernel_3123/464779580.py:14: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n(Deprecated in version 0.20.5)\n  print(inaugural_clustered.group_by('cluster').agg(pl.count()).sort('cluster'))\n\n\n\n\n10.3.8 Interpret Clusters\n\n# Examine which documents are in each cluster\nfor i in range(1, n_clusters + 1):\n    cluster_docs = inaugural_clustered.filter(pl.col('cluster') == i)\n    doc_list = cluster_docs.select('doc_id').to_numpy().flatten()\n    years = cluster_docs.select('year').to_numpy().flatten()\n    \n    print(f\"\\nCluster {i} (n={len(doc_list)}):\")\n    if len(years) &gt; 0:\n        print(f\"  Years: {int(min(years))} - {int(max(years))}\")\n    print(f\"  Documents: {', '.join(str(d) for d in doc_list[:5])}\", end=\"\")\n    if len(doc_list) &gt; 5:\n        print(f\", ... (+{len(doc_list)-5} more)\")\n    else:\n        print()\n\n\nCluster 1 (n=8):\n  Years: 1901 - 1929\n  Documents: 1901-McKinley, 1905-Roosevelt, 1909-Taft, 1913-Wilson, 1917-Wilson, ... (+3 more)\n\nCluster 2 (n=8):\n  Years: 1933 - 1961\n  Documents: 1933-Roosevelt, 1937-Roosevelt, 1941-Roosevelt, 1945-Roosevelt, 1949-Truman, ... (+3 more)\n\nCluster 3 (n=8):\n  Years: 1965 - 1993\n  Documents: 1965-Johnson, 1969-Nixon, 1973-Nixon, 1977-Carter, 1981-Reagan, ... (+3 more)\n\nCluster 4 (n=8):\n  Years: 1997 - 2025\n  Documents: 1997-Clinton, 2001-Bush, 2005-Bush, 2009-Obama, 2013-Obama, ... (+3 more)\n\n\nResearch questions to ask:\n\nDo clusters align with known historical periods (Founding era, Civil War, Progressive era, modern)?\nDo clusters reflect thematic concerns (war, economy, civil rights) or stylistic patterns?\nAre there surprising groupings that challenge conventional periodization?\n\n\n\n10.3.9 Comparing Linkage Methods\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\nlinkage_methods = ['single', 'complete', 'average', 'ward']\n\nfor idx, method in enumerate(linkage_methods):\n    ax = axes[idx // 2, idx % 2]\n    \n    # Compute linkage\n    Z = linkage(distances, method=method)\n    \n    # Plot dendrogram\n    dendrogram(Z, ax=ax, labels=doc_ids, leaf_font_size=8, leaf_rotation=90)\n    ax.set_title(f\"{method.capitalize()} Linkage\", fontsize=12)\n    ax.set_xlabel(\"Document\")\n    ax.set_ylabel(\"Distance\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigureÂ 10.3: Comparison of linkage methods. Wardâ€™s typically creates the most balanced, interpretable clusters for text data.\n\n\n\n\n\nObservations:\n\nSingle linkage: â€œChainingâ€ effect (one long cluster)\nComplete linkage: More balanced, but may split natural groups\nAverage linkage: Middle ground\nWardâ€™s linkage: Most balanced, compact clusters (usually preferred)",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>Clustering Methods</span>"
    ]
  },
  {
    "objectID": "tutorials/cluster-analysis.html#k-means-clustering",
    "href": "tutorials/cluster-analysis.html#k-means-clustering",
    "title": "10Â  Clustering Methods",
    "section": "10.4 K-Means Clustering",
    "text": "10.4 K-Means Clustering\nK-means is a partitioning method that divides data into k distinct, non-overlapping clusters. Unlike hierarchical clustering, you must specify k in advance.\n\n10.4.1 How It Works\n\nInitialize: Randomly place k cluster centers (centroids) in feature space\nAssign: Assign each observation to nearest centroid\nUpdate: Recalculate centroid as mean of assigned observations\nRepeat: Steps 2-3 until centroids stop moving (convergence)\nResult: k clusters with each observation assigned to exactly one cluster\n\n\n\n10.4.2 Advantages & Disadvantages\nAdvantages:\n\nFast: Scales well to large datasets\nSimple: Easy to implement and interpret\nDefinite assignments: Each observation belongs to exactly one cluster\nWorks with many features: Handles high-dimensional data (embeddings)\n\nDisadvantages:\n\nRequires k: Must specify number of clusters in advance\nSensitive to initialization: Different starting points can produce different results (use n_init parameter)\nAssumes spherical clusters: Struggles with elongated or irregular shapes\nSensitive to outliers: Extreme values distort centroids\n\n\n\n10.4.3 Example: Clustering by Semantic Content\nUsing contextual embeddings (from Mini Lab 11), we can cluster presidential speeches by semantic content rather than word overlap:\n\n# Assume we have sentence embeddings for inaugural addresses\n# (In practice, these would come from a transformer model like BERT)\n\n# For this example, simulate embeddings from the word frequency matrix\n# In real analysis, use actual embeddings from sentence-transformers\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n\n# Reduce dimensionality for visualization (simulate embeddings)\n# Set n_components to min of 50 or the available features/samples\nn_components = min(50, X.shape[0] - 1, X.shape[1])\npca = PCA(n_components=n_components, random_state=42)\nembeddings = pca.fit_transform(X)\n\nprint(f\"Embedding matrix: {embeddings.shape}\")\n\nEmbedding matrix: (32, 1)\n\n\n\n\n10.4.4 Choosing k: Elbow Method\n\n# Test different values of k\ninertias = []\nk_range = range(2, 11)\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(embeddings)\n    inertias.append(kmeans.inertia_)\n\n# Plot\nplt.figure(figsize=(8, 5))\nplt.plot(k_range, inertias, marker='o')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Inertia (Within-Cluster Sum of Squares)')\nplt.title('Elbow Method for Optimal k')\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigureÂ 10.4: Elbow plot for k-means. Choose k where adding more clusters provides diminishing returns (the â€˜elbowâ€™).\n\n\n\n\n\nInertia: Sum of squared distances from each point to its cluster centroid. Lower is better, but we want the â€œelbowâ€ where improvement plateaus.\n\n\n10.4.5 Choosing k: Silhouette Score\n\nfrom sklearn.metrics import silhouette_score\n\n# Calculate silhouette scores\nsilhouette_scores = []\n\nfor k in k_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    labels = kmeans.fit_predict(embeddings)\n    score = silhouette_score(embeddings, labels)\n    silhouette_scores.append(score)\n\n# Plot\nplt.figure(figsize=(8, 5))\nplt.plot(k_range, silhouette_scores, marker='o', color='orange')\nplt.xlabel('Number of Clusters (k)')\nplt.ylabel('Silhouette Score')\nplt.title('Silhouette Method for Optimal k')\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nSilhouette scores by k:\")\nfor k, score in zip(k_range, silhouette_scores):\n    print(f\"  k={k}: {score:.3f}\")\n\n\n\n\n\n\n\n\n\nSilhouette scores by k:\n  k=2: 0.604\n  k=3: 0.548\n  k=4: 0.521\n  k=5: 0.496\n  k=6: 0.471\n  k=7: 0.444\n  k=8: 0.431\n  k=9: 0.407\n  k=10: 0.391\n\n\nSilhouette score (-1 to 1): Measures how similar observations are to their own cluster vs.Â other clusters. Higher is better.\n\n&gt; 0.7: Strong structure\n0.5-0.7: Reasonable structure\n0.25-0.5: Weak structure\n&lt; 0.25: No substantial structure\n\n\n\n10.4.6 Fit K-Means Model\n\n# Choose k based on elbow/silhouette analysis\nk = 4\n\n# Fit k-means\nkmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\nkmeans_labels = kmeans.fit_predict(embeddings)\n\n# Add to dataframe\ninaugural_kmeans = inaugural_df.with_columns(\n    pl.Series('kmeans_cluster', kmeans_labels)\n)\n\nprint(f\"K-means cluster assignments (first 10):\")\nprint(inaugural_kmeans.select(['doc_id', 'year', 'kmeans_cluster']).head(10))\n\nprint(\"\\nCluster sizes:\")\nprint(inaugural_kmeans.group_by('kmeans_cluster').agg(pl.count()).sort('kmeans_cluster'))\n\nK-means cluster assignments (first 10):\nshape: (10, 3)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ doc_id         â”† year â”† kmeans_cluster â”‚\nâ”‚ ---            â”† ---  â”† ---            â”‚\nâ”‚ str            â”† i64  â”† i32            â”‚\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ 1901-McKinley  â”† 1901 â”† 1              â”‚\nâ”‚ 1905-Roosevelt â”† 1905 â”† 1              â”‚\nâ”‚ 1909-Taft      â”† 1909 â”† 1              â”‚\nâ”‚ 1913-Wilson    â”† 1913 â”† 1              â”‚\nâ”‚ 1917-Wilson    â”† 1917 â”† 1              â”‚\nâ”‚ 1921-Harding   â”† 1921 â”† 1              â”‚\nâ”‚ 1925-Coolidge  â”† 1925 â”† 1              â”‚\nâ”‚ 1929-Hoover    â”† 1929 â”† 1              â”‚\nâ”‚ 1933-Roosevelt â”† 1933 â”† 3              â”‚\nâ”‚ 1937-Roosevelt â”† 1937 â”† 3              â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nCluster sizes:\nshape: (4, 2)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ kmeans_cluster â”† count â”‚\nâ”‚ ---            â”† ---   â”‚\nâ”‚ i32            â”† u32   â”‚\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\nâ”‚ 0              â”† 8     â”‚\nâ”‚ 1              â”† 8     â”‚\nâ”‚ 2              â”† 8     â”‚\nâ”‚ 3              â”† 8     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n/tmp/ipykernel_3123/1313964696.py:17: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n(Deprecated in version 0.20.5)\n  print(inaugural_kmeans.group_by('kmeans_cluster').agg(pl.count()).sort('kmeans_cluster'))\n\n\n\n\n10.4.7 Visualize Clusters\n\n# Further reduce to 2D for visualization\nn_dims_viz = min(2, embeddings.shape[1])\npca_2d = PCA(n_components=n_dims_viz, random_state=42)\ncoords_2d = pca_2d.fit_transform(embeddings)\n\n# Plot\nplt.figure(figsize=(10, 8))\n\n# If we only have 1 dimension, plot as 1D scatter with jitter\nif n_dims_viz == 1:\n    scatter = plt.scatter(\n        coords_2d[:, 0], \n        np.random.normal(0, 0.02, size=len(coords_2d)),  # Add jitter on y-axis\n        c=kmeans_labels,\n        cmap='viridis',\n        s=100,\n        alpha=0.6,\n        edgecolors='black'\n    )\n    plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.1%} variance)')\n    plt.ylabel('Random jitter')\nelse:\n    scatter = plt.scatter(\n        coords_2d[:, 0], \n        coords_2d[:, 1],\n        c=kmeans_labels,\n        cmap='viridis',\n        s=100,\n        alpha=0.6,\n        edgecolors='black'\n    )\n    plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.1%} variance)')\n    plt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.1%} variance)')\n\n# Add labels for select points\nfor i, doc_id in enumerate(doc_ids):\n    if i % 3 == 0:  # Label every 3rd document to avoid crowding\n        y_coord = coords_2d[i, 1] if n_dims_viz == 2 else np.random.normal(0, 0.02)\n        plt.annotate(\n            str(doc_id),\n            (coords_2d[i, 0], y_coord),\n            fontsize=8,\n            alpha=0.7\n        )\n\nplt.colorbar(scatter, label='Cluster')\nplt.title('K-Means Clustering (Visualized in 2D via PCA)')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigureÂ 10.5: K-means clusters visualized in 2D (PCA projection). Colors indicate cluster assignments.\n\n\n\n\n\nNote: This is a 2D projection of high-dimensional data. Clusters may appear to overlap in 2D even if theyâ€™re well-separated in the full feature space.\n\n\n10.4.8 Interpret Clusters\n\n# Examine cluster compositions\nfor i in range(k):\n    cluster_docs = inaugural_kmeans.filter(pl.col('kmeans_cluster') == i)\n    doc_list = cluster_docs.select('doc_id').to_numpy().flatten()\n    years = cluster_docs.select('year').to_numpy().flatten()\n    \n    print(f\"\\nCluster {i} (n={len(doc_list)}):\")\n    if len(years) &gt; 0:\n        print(f\"  Years: {int(min(years))} - {int(max(years))}\")\n    print(f\"  Documents: {', '.join(str(d) for d in doc_list[:5])}\", end=\"\")\n    if len(doc_list) &gt; 5:\n        print(f\", ... (+{len(doc_list)-5} more)\")\n    else:\n        print()\n\n\nCluster 0 (n=8):\n  Years: 1965 - 1993\n  Documents: 1965-Johnson, 1969-Nixon, 1973-Nixon, 1977-Carter, 1981-Reagan, ... (+3 more)\n\nCluster 1 (n=8):\n  Years: 1901 - 1929\n  Documents: 1901-McKinley, 1905-Roosevelt, 1909-Taft, 1913-Wilson, 1917-Wilson, ... (+3 more)\n\nCluster 2 (n=8):\n  Years: 1997 - 2025\n  Documents: 1997-Clinton, 2001-Bush, 2005-Bush, 2009-Obama, 2013-Obama, ... (+3 more)\n\nCluster 3 (n=8):\n  Years: 1933 - 1961\n  Documents: 1933-Roosevelt, 1937-Roosevelt, 1941-Roosevelt, 1945-Roosevelt, 1949-Truman, ... (+3 more)\n\n\nResearch questions:\n\nDo clusters correspond to thematic concerns (war, prosperity, reform)?\nDo clusters align with political parties or historical eras?\nWhich speeches bridge multiple clusters (high silhouette vs.Â low silhouette)?\n\n\n\n10.4.9 Cluster Centroids\n\n# Get centroid coordinates\ncentroids = kmeans.cluster_centers_\n\n# Find most representative document in each cluster (closest to centroid)\nfrom scipy.spatial.distance import cdist\n\ndistances_to_centroids = cdist(embeddings, centroids, metric='euclidean')\n\nprint(\"\\nMost representative document per cluster:\")\nfor i in range(k):\n    closest_idx = np.argmin(distances_to_centroids[:, i])\n    print(f\"  Cluster {i}: {str(doc_ids[closest_idx])}\")\n\n\nMost representative document per cluster:\n  Cluster 0: 1977-Carter\n  Cluster 1: 1913-Wilson\n  Cluster 2: 2009-Obama\n  Cluster 3: 1945-Roosevelt\n\n\nInterpretation: The document closest to the centroid is the most â€œtypicalâ€ member of that clusterâ€”a good starting point for close reading.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>Clustering Methods</span>"
    ]
  },
  {
    "objectID": "tutorials/cluster-analysis.html#hierarchical-vs.-k-means-when-to-use-each",
    "href": "tutorials/cluster-analysis.html#hierarchical-vs.-k-means-when-to-use-each",
    "title": "10Â  Clustering Methods",
    "section": "10.5 Hierarchical vs.Â K-Means: When to Use Each",
    "text": "10.5 Hierarchical vs.Â K-Means: When to Use Each\n\n\n\n\n\n\n\n\nAspect\nHierarchical\nK-Means\n\n\n\n\nNumber of clusters\nDonâ€™t need to specify; explore dendrogram\nMust specify k in advance\n\n\nVisualization\nDendrogram shows all possible groupings\nRequires dimensionality reduction for viz\n\n\nComputation\nSlower (O(nÂ² log n))\nFaster (O(nki))\n\n\nDataset size\nWorks for small-medium datasets (&lt;1000)\nScales to large datasets (&gt;10,000)\n\n\nCluster shape\nHandles irregular shapes\nAssumes spherical clusters\n\n\nInterpretation\nTree structure shows relationships\nClear assignments, but no hierarchy\n\n\nStability\nDeterministic (same result every time)\nRandom initialization (use n_init)\n\n\nUse cases\nPeriodization, document similarity, exploratory\nThematic grouping, large corpora\n\n\n\nRule of thumb:\n\nSmall datasets (&lt;500 observations) with exploratory goals: Use hierarchical\nLarge datasets (&gt;1000 observations) with known k: Use k-means\nBoth together: Use hierarchical to choose k, then k-means for final clustering",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>Clustering Methods</span>"
    ]
  },
  {
    "objectID": "tutorials/cluster-analysis.html#applications-in-humanities-research",
    "href": "tutorials/cluster-analysis.html#applications-in-humanities-research",
    "title": "10Â  Clustering Methods",
    "section": "10.6 Applications in Humanities Research",
    "text": "10.6 Applications in Humanities Research\n\n10.6.1 Historical Periodization\nProblem: When did linguistic shifts occur? Do conventional period boundaries (centuries, decades) align with actual language change?\nSolution: Hierarchical clustering of time periods by word frequencies (Variability-Based Neighbor Clustering).\nExample: Mini Lab 7 clusters decades of the Brown corpus, revealing that 1800-1930 form one period, 1940-1980 another, with 1990-2000 as distinct modern periods.\nAdvantages:\n\nData-driven periodization (not arbitrary bins)\nDendrograms show gradual vs.Â sharp transitions\nQuantifies similarity across time\n\n\n\n10.6.2 Thematic Discovery in Corpora\nProblem: You have 500 journal articles. What thematic groups exist?\nSolution: K-means clustering on contextual embeddings.\nExample: Cluster research articles by abstract embeddings. Discover that apparent clusters correspond to methodological approaches (quantitative vs.Â qualitative) rather than topic areas.\nAdvantages:\n\nSemantic similarity (not just word overlap)\nHandles large corpora efficiently\nReveals unexpected groupings\n\n\n\n10.6.3 Authorship and Style\nProblem: Do texts cluster by author, genre, or time period?\nSolution: Hierarchical clustering on stylometric features (function words, sentence length, etc.)\nExample: Cluster novels by function word frequencies. If they cluster by author (not by genre or era), style is author-specific.\nAdvantages:\n\nDendrogram shows hierarchical relationships (author â†’ period â†’ genre)\nVisualizes degrees of similarity\nIdentifies outliers (unusual texts)\n\n\n\n10.6.4 Document Similarity in Topic Modeling\nProblem: After fitting a topic model, which documents are similar based on topic distributions?\nSolution: Hierarchical clustering on topic proportion vectors.\nExample: Cluster presidential speeches by topic distributions. Wartime speeches cluster together regardless of era.\nAdvantages:\n\nTopic-based similarity (reduced dimensionality)\nInterpretable clusters (shared topics)\nSee topic modeling tutorial for details",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>Clustering Methods</span>"
    ]
  },
  {
    "objectID": "tutorials/cluster-analysis.html#methodological-considerations",
    "href": "tutorials/cluster-analysis.html#methodological-considerations",
    "title": "10Â  Clustering Methods",
    "section": "10.7 Methodological Considerations",
    "text": "10.7 Methodological Considerations\n\n10.7.1 Distance Metrics Matter\nDifferent metrics emphasize different aspects of similarity:\n\nEuclidean: Total difference in all dimensions (standard)\nCosine: Directional similarity (ignores magnitude, good for sparse text)\nCorrelation: Pattern similarity (ignores scale)\nManhattan: Sum of absolute differences (less sensitive to outliers)\n\nFor text data: Cosine distance often works better than Euclidean because it ignores document length (just looks at proportional similarity).\n\n\n10.7.2 Feature Selection\nClustering quality depends on features:\nGood features:\n\nFunction words: Capture style (authorship, genre)\nTopic distributions: Capture content (themes)\nEmbeddings: Capture semantics (meaning)\nTF-IDF: Captures distinctive vocabulary\n\nBad features:\n\nRaw word counts: Biased by document length\nAll words: Noise overwhelms signal (dimensionality curse)\nRare words: Unstable, unreliable\n\nRule of thumb: Select features relevant to your research question. For style, use function words or syntactic features. For theme, use content words or embeddings.\n\n\n10.7.3 Choosing the Number of Clusters\nThereâ€™s no â€œcorrectâ€ kâ€”itâ€™s a research decision informed by:\n\nQuantitative criteria:\n\nElbow method (inertia or merge distances)\nSilhouette score (cohesion vs.Â separation)\nGap statistic (compare to random data)\n\nQualitative criteria:\n\nInterpretability (do clusters make sense?)\nResearch goals (what granularity is useful?)\nPrior knowledge (do clusters align with known categories?)\n\nPractical criteria:\n\nSample size (avoid tiny clusters: n &lt; 5)\nComparison goals (need same k across datasets?)\n\n\nBest practice: Test multiple values of k, examine cluster compositions, choose the most interpretable solution that addresses your research question.\n\n\n10.7.4 Validating Clusters\nInternal validation (uses only the data):\n\nSilhouette coefficient: Are clusters cohesive and separated?\nDavies-Bouldin index: Ratio of within-cluster to between-cluster distances (lower is better)\nCalinski-Harabasz index: Ratio of between-cluster to within-cluster variance (higher is better)\n\nExternal validation (uses known labels):\n\nAdjusted Rand Index: Do clusters align with known categories?\nV-measure: Harmonic mean of homogeneity and completeness\n\nQualitative validation:\n\nExamine cluster members: Do they share meaningful properties?\nCompare to expert judgments: Do historians agree with periodization?\nRead representative texts: Does thematic interpretation hold up?\n\nReality check: Clusters are exploratory. They suggest patterns, not prove hypotheses. Always validate computationally-discovered groups through close reading and domain expertise.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>Clustering Methods</span>"
    ]
  },
  {
    "objectID": "tutorials/cluster-analysis.html#connections-to-other-methods",
    "href": "tutorials/cluster-analysis.html#connections-to-other-methods",
    "title": "10Â  Clustering Methods",
    "section": "10.8 Connections to Other Methods",
    "text": "10.8 Connections to Other Methods\n\n10.8.1 Clustering and Classification\nClustering can inform classification:\n\nCluster unlabeled data\nManually label clusters (inspect members, assign category)\nUse labeled clusters to train supervised classifier\n\nExample: Cluster tweets by content, manually label clusters as â€œpositiveâ€, â€œnegativeâ€, â€œneutralâ€, then train sentiment classifier.\n\n\n10.8.2 Clustering and Topic Modeling\nTopic modeling is a form of soft clustering:\n\nTopic models: Each document is a mixture of topics (partial membership)\nClustering: Each document belongs to one cluster (hard assignment)\n\nCombined approach:\n\nFit topic model to discover themes\nCluster documents by topic distributions\nResult: Documents with similar thematic profiles group together\n\nSee topic modeling tutorial for hierarchical clustering of topic distributions.\n\n\n10.8.3 Clustering and Embeddings\nEmbeddings provide rich features for clustering:\n\nStatic embeddings (word2vec, GloVe): Average word vectors per document\nContextual embeddings (BERT, sentence-transformers): Sentence/document vectors\n\nWhy embeddings improve clustering:\n\nCapture semantic similarity (synonyms cluster together)\nReduce dimensionality (768 dimensions vs.Â 10,000+ words)\nPretrained on large corpora (better generalization)\n\nSee contextual embeddings tutorial for k-means on sentence embeddings.\n\n\n10.8.4 Clustering and MDA\nMulti-Dimensional Analysis uses clustering indirectly:\n\nCompute linguistic features (rates of nominalizations, passives, etc.)\nReduce to dimensions via factor analysis\nCluster texts by dimension scores\nResult: Texts grouped by register/genre\n\nSee MDA tutorial for dimension-based grouping.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>Clustering Methods</span>"
    ]
  },
  {
    "objectID": "tutorials/cluster-analysis.html#practical-tips",
    "href": "tutorials/cluster-analysis.html#practical-tips",
    "title": "10Â  Clustering Methods",
    "section": "10.9 Practical Tips",
    "text": "10.9 Practical Tips\n\n10.9.1 1. Preprocessing Matters\nBefore clustering:\n\nNormalize: Scale features to same range (StandardScaler) if using Euclidean distance\nRemove outliers: Extreme values distort clusters (especially in k-means)\nHandle missing data: Impute or remove (clustering algorithms canâ€™t handle NaN)\n\n\n\n10.9.2 2. Visualize Before and After\nBefore clustering:\n\nPCA or t-SNE plots: Do natural groupings exist?\nCorrelation heatmaps: Are features redundant?\n\nAfter clustering:\n\nDendrograms or scatter plots: Do clusters make sense?\nSilhouette plots: Are assignments confident?\n\n\n\n10.9.3 3. Interpret with Domain Knowledge\nComputational patterns are starting points:\n\nWhy do these texts cluster together?\nWhat do cluster members share (theme, style, era)?\nAre there surprising members that challenge assumptions?\n\nCombine distant and close reading: Let clustering suggest which texts to read closely, then use close reading to interpret why they cluster.\n\n\n10.9.4 4. Report Transparently\nIn publications, report:\n\nDistance metric and linkage method (hierarchical) or k and initialization (k-means)\nHow k was chosen (elbow, silhouette, theory)\nValidation metrics (silhouette score, qualitative assessment)\nCluster compositions (which texts are in which clusters)\n\nReproducibility: Set random_state for k-means so results are replicable.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>Clustering Methods</span>"
    ]
  },
  {
    "objectID": "tutorials/cluster-analysis.html#ethical-considerations",
    "href": "tutorials/cluster-analysis.html#ethical-considerations",
    "title": "10Â  Clustering Methods",
    "section": "10.10 Ethical Considerations",
    "text": "10.10 Ethical Considerations\n\n10.10.1 Overgeneralization\nRisk: Treating clusters as natural categories rather than analytical constructs.\nExample: Clustering social media posts into â€œliberalâ€ and â€œconservativeâ€ may obscure ideological complexity and diversity within groups.\nBest practice: Present clusters as patterns in the data for this specific corpus with these features, not universal truths.\n\n\n10.10.2 Decontextualization\nRisk: Grouping texts by statistical similarity without considering historical/cultural context.\nExample: Clustering enslaved personsâ€™ narratives with abolitionist speeches because both use â€œfreedomâ€ frequently obscures power dynamics and authorial agency.\nBest practice: Interpret clusters in context. Ask why these texts are similar and whether that similarity is meaningful given their social/historical positions.\n\n\n10.10.3 Stereotype Reinforcement\nRisk: Clusters may reflect and amplify societal biases present in training data.\nExample: Clustering job applicants by rÃ©sumÃ© language might group by gender or race due to systematic differences in phrasing, perpetuating discrimination.\nBest practice: Examine features driving clusters. Are they substantive or proxies for protected categories? Validate against known biases.\n\n\n10.10.4 Reductive Categorization\nRisk: Forcing continuous variation into discrete clusters obscures nuance.\nExample: Clustering authors into â€œearly modernâ€ vs.Â â€œmodernâ€ styles may miss authors who blend traditions or evolved across their careers.\nBest practice: Acknowledge clustering simplifies complexity. Consider fuzzy clustering (soft assignments) or report silhouette scores (confidence in assignments) to capture ambiguity.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>Clustering Methods</span>"
    ]
  },
  {
    "objectID": "tutorials/cluster-analysis.html#summary",
    "href": "tutorials/cluster-analysis.html#summary",
    "title": "10Â  Clustering Methods",
    "section": "10.11 Summary",
    "text": "10.11 Summary\nClustering discovers structure in data without predefined labels:\nHierarchical clustering:\n\nCreates tree of nested groupings (dendrogram)\nLinkage methods: single, complete, average, Wardâ€™s (most common)\nDonâ€™t need to choose k in advance\nBest for exploration, small-medium datasets\nApplications: periodization, document similarity, authorship\n\nK-means clustering:\n\nPartitions data into k distinct clusters\nFast, scales well to large datasets\nMust choose k (use elbow method, silhouette score)\nBest for large corpora, semantic grouping\nApplications: thematic discovery, embedding-based clustering\n\nKey insights:\n\nDistance metrics matter: Euclidean for general use, cosine for text\nFeature selection is critical: Use features relevant to research question\nChoosing k is a research decision: Balance quantitative criteria with interpretability\nValidate qualitatively: Computational patterns suggest hypotheses; close reading confirms\nReport transparently: Document methods, parameters, and validation metrics\n\nNext steps: Apply clustering to your own data. Use hierarchical clustering for exploratory periodization, k-means for thematic grouping. Combine with other methods (topic modeling, embeddings, classification) for richer analysis.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>Clustering Methods</span>"
    ]
  },
  {
    "objectID": "tutorials/cluster-analysis.html#further-reading",
    "href": "tutorials/cluster-analysis.html#further-reading",
    "title": "10Â  Clustering Methods",
    "section": "10.12 Further Reading",
    "text": "10.12 Further Reading\nFoundational:\n\nJain, A. K. (2010). Data clustering: 50 years beyond K-means. Pattern Recognition Letters, 31(8), 651-666. DOI\nMurtagh, F., & Contreras, P. (2012). Algorithms for hierarchical clustering: An overview. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery, 2(1), 86-97. DOI\n\nHumanities applications:\n\nGries, S. T., & Hilpert, M. (2008). The identification of stages in diachronic data: Variability-based neighbour clustering. Corpora, 3(1), 59-81. DOI\nEder, M. (2015). Visualization in stylometry: Cluster analysis using networks. Digital Scholarship in the Humanities, 32(1), 50-64. DOI\nUnderwood, T. (2019). Distant Horizons: Digital Evidence and Literary Change. University of Chicago Press. (Chapter 5: â€œThe Life Spans of Genresâ€)\n\nPractical guides:\n\nScikit-learn documentation: Clustering\nSciPy documentation: Hierarchical clustering",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>10</span>Â  <span class='chapter-title'>Clustering Methods</span>"
    ]
  },
  {
    "objectID": "tutorials/correlations.html",
    "href": "tutorials/correlations.html",
    "title": "11Â  Correlation and Multi-Collinearity",
    "section": "",
    "text": "11.1 Introduction\nCorrelation measures the strength and direction of the linear relationship between two variables. In text analysis, correlation helps us understand which linguistic features vary togetherâ€”a fundamental question for understanding language use.\nWhy correlation matters for humanities research:\nThis tutorial introduces correlation analysis with special attention to multi-collinearity: when predictor variables are highly correlated with each other. Weâ€™ll explore:",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>Correlation and Multi-Collinearity</span>"
    ]
  },
  {
    "objectID": "tutorials/correlations.html#introduction",
    "href": "tutorials/correlations.html#introduction",
    "title": "11Â  Correlation and Multi-Collinearity",
    "section": "",
    "text": "Feature relationships: Do passives increase when nominalizations increase?\nMulti-collinearity detection: Which features are redundant (measuring the same thing)?\nDimension reduction: Can we combine correlated features into composite variables?\nAssumption checking: Many statistical models assume features are independentâ€”are they?\nTheoretical insights: Correlation patterns reveal functional groupings (features that serve similar communicative purposes)\n\n\n\nWhen multi-collinearity is a problem (regression, classification models assume independence)\nWhen multi-collinearity is useful (factor analysis, PCA leverage it for dimension reduction)\nHow to detect and interpret correlation patterns using linguistic features from the pybiber package",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>Correlation and Multi-Collinearity</span>"
    ]
  },
  {
    "objectID": "tutorials/correlations.html#core-concepts",
    "href": "tutorials/correlations.html#core-concepts",
    "title": "11Â  Correlation and Multi-Collinearity",
    "section": "11.2 Core Concepts",
    "text": "11.2 Core Concepts\n\n11.2.1 What is Correlation?\nCorrelation coefficient (Pearsonâ€™s r) ranges from -1 to +1:\n\nr = +1: Perfect positive relationship (as X increases, Y increases proportionally)\nr = 0: No linear relationship\nr = -1: Perfect negative relationship (as X increases, Y decreases proportionally)\n\nExample:\n\nPositive correlation: Nominalizations and attributive adjectives (both increase in informational writing)\nNegative correlation: Personal pronouns and word length (pronouns are short; informational writing uses long words)\nNo correlation: Past tense and type-token ratio (independent dimensions of variation)\n\n\n\n11.2.2 Interpreting Correlation Strength\nConventional benchmarks (Cohen, 1988):\n\n|r| &lt; 0.3: Weak correlation\n0.3 â‰¤ |r| &lt; 0.5: Moderate correlation\n\n|r| â‰¥ 0.5: Strong correlation\n\nContext matters: In social sciences, r = 0.3 might be meaningful. In physics, r = 0.9 might be insufficient. For linguistic features, correlations around 0.4-0.6 are common and theoretically important.\n\n\n\n\n\n\nCorrelation â‰  Causation\n\n\n\nCorrelation measures co-occurrence, not causation. If passives and nominalizations correlate, it doesnâ€™t mean one causes the otherâ€”both may be responses to a third factor (e.g., genre constraints requiring informational density).\n\n\n\n\n11.2.3 Types of Correlation\nPearson correlation (parametric): - Measures linear relationships - Assumes normally distributed variables - Most common, what weâ€™ll use\nSpearman correlation (non-parametric): - Measures monotonic relationships (consistently increasing or decreasing, but not necessarily linear) - Based on ranks, not raw values - Better for non-normal distributions or ordinal data\nKendallâ€™s tau (non-parametric): - Also rank-based - More robust to outliers than Spearman - Better for small samples\nFor most text analysis with count-based features (especially normalized frequencies), Pearson correlation is appropriate.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>Correlation and Multi-Collinearity</span>"
    ]
  },
  {
    "objectID": "tutorials/correlations.html#multi-collinearity-explained",
    "href": "tutorials/correlations.html#multi-collinearity-explained",
    "title": "11Â  Correlation and Multi-Collinearity",
    "section": "11.3 Multi-Collinearity Explained",
    "text": "11.3 Multi-Collinearity Explained\n\n11.3.1 What is Multi-Collinearity?\nMulti-collinearity occurs when predictor variables in a model are highly correlated with each other. This creates problems for some statistical procedures but enables others.\nExample: If weâ€™re predicting genre from linguistic features, and nominalizations (r = 0.85) and prepositions are highly correlated, they provide redundant informationâ€”both measure similar underlying dimension (informativeness).\n\n\n11.3.2 When Multi-Collinearity is a Problem\n\n11.3.2.1 1. Regression Models\nWhy itâ€™s problematic:\n\nUnstable coefficients: Small changes in data cause large changes in estimated effects\nInflated standard errors: Confidence intervals become unreliable\nDifficulty interpreting: Canâ€™t tell which variable is â€œreallyâ€ important\nVariance inflation: Collinear variables â€œcompeteâ€ to explain the same variance\n\nExample: If nominalizations and prepositions both predict academic writing (r = 0.85 with each other), their individual regression coefficients become unreliable. Adding or removing one changes the otherâ€™s coefficient dramatically.\nDiagnostic: Variance Inflation Factor (VIF):\n\nVIF = 1 / (1 - RÂ²) where RÂ² is from regressing predictor X on all other predictors\nVIF &gt; 10 indicates severe multi-collinearity\nVIF &gt; 5 suggests caution\n\nSolutions:\n\nDrop redundant variables (keep most theoretically important)\nCombine correlated variables into composite scores\nUse dimension reduction (PCA) to create orthogonal predictors\nUse regularization (Ridge regression, Lasso) which penalizes large coefficients\n\n\n\n11.3.2.2 2. Classification Models\nSome algorithms struggle with multi-collinearity:\n\nLogistic regression: Same issues as linear regression (unstable coefficients, inflated SE)\nLinear Discriminant Analysis: Assumes features are independent\n\nSome algorithms handle it well:\n\nRandom Forest: Tree-based methods are robust to multi-collinearity\nNaive Bayes: Assumes independence but often works despite violations\nNeural networks: Can learn despite multi-collinearity (but may be inefficient)\n\nPractical impact: Multi-collinearity often reduces interpretability more than predictive accuracy. A Random Forest might classify well despite correlated features, but you canâ€™t interpret feature importance reliably.\n\n\n\n11.3.3 When Multi-Collinearity is Useful\n\n11.3.3.1 1. Factor Analysis\nFactor analysis (used in Multi-Dimensional Analysis) requires correlations among variables:\nCore principle: If variables donâ€™t correlate, they canâ€™t reflect underlying latent factors.\nExample:\n\nObserved variables: First-person pronouns, present tense, contractions (all correlated r â‰ˆ 0.5-0.7)\nLatent factor: â€œInvolved productionâ€ (personal interaction)\n\nFactor analysis identifies groups of correlated variables and represents them as latent dimensions. Without correlation, thereâ€™s nothing to reduce.\nRequirements:\n\nSufficient correlations: Need some |r| &gt; 0.3 among variables\nNot perfect correlations: Avoid |r| &gt; 0.9 (redundancy, not shared factor)\nBartlettâ€™s test: Tests if correlation matrix differs from identity matrix (p &lt; 0.05 indicates adequate correlations)\nKMO (Kaiser-Meyer-Olkin): Measures sampling adequacy (&gt; 0.6 acceptable, &gt; 0.8 good)\n\n\n\n11.3.3.2 2. Principal Component Analysis (PCA)\nPCA creates new variables (components) that are linear combinations of correlated original variables:\nExample:\n\nComponent 1: 0.8Ã—nominalizations + 0.7Ã—prepositions + 0.6Ã—attributive_adjectives\nThis component captures shared variance (informational density)\n\nWhy correlation helps: PCA maximizes variance explained by components. If variables donâ€™t correlate, each component captures variance from just one variable (inefficient).\nDifference from factor analysis:\n\nPCA: Reduces dimensions by explaining total variance (data reduction)\nFactor Analysis: Identifies latent factors explaining correlations (theory discovery)\n\nSee MDA tutorial for how this works with linguistic features.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>Correlation and Multi-Collinearity</span>"
    ]
  },
  {
    "objectID": "tutorials/correlations.html#correlation-analysis-with-pybiber-features",
    "href": "tutorials/correlations.html#correlation-analysis-with-pybiber-features",
    "title": "11Â  Correlation and Multi-Collinearity",
    "section": "11.4 Correlation Analysis with Pybiber Features",
    "text": "11.4 Correlation Analysis with Pybiber Features\nLetâ€™s analyze correlations among Biberâ€™s 67 lexicogrammatical features using the Brown Corpus. These features capture functional linguistic categories that often correlate because they serve related communicative purposes.\n\n11.4.1 Load and Prepare Data\n\nimport polars as pl\nimport numpy as np\nimport pandas as pd\nimport pybiber as pb\nimport spacy\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import pearsonr\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load Brown Corpus with genre labels\nbrown_corpus = pl.read_parquet(\n    \"https://github.com/browndw/humanities_analytics/raw/refs/heads/main/data/data_tables/brown_corpus.parquet\"\n)\n\n# Select document IDs and texts\nbc = brown_corpus.select(\"doc_id\", \"text\")\n\n# Load spaCy model (disable NER for speed)\nnlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n\n# Parse corpus with spaCy (this takes ~30 seconds with n_process=4)\nprint(\"Parsing corpus with spaCy...\")\ndf_spacy = pb.spacy_parse(corp=bc, nlp_model=nlp, n_process=4)\n\n# Aggregate into 67 Biber feature categories\nprint(\"Extracting Biber features...\")\ndfm_biber = pb.biber(df_spacy)\n\n# Add genre labels from original corpus\ndfm_biber = dfm_biber.join(\n    brown_corpus.select(\"doc_id\", \"text_type\"), \n    on=\"doc_id\"\n)\n\nprint(f\"Data shape: {dfm_biber.shape}\")\nprint(f\"Number of features: {len([col for col in dfm_biber.columns if col.startswith('f_')])}\")\nprint(\"\\nFirst few rows:\")\nprint(dfm_biber.head())\n\nParsing corpus with spaCy...\nPerformance: Corpus processing completed in 59.32s\nExtracting Biber features...\nData shape: (470, 69)\nNumber of features: 67\n\nFirst few rows:\nshape: (5, 69)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ doc_id â”† f_01_pa â”† f_02_pe â”† f_03_pr â”† f_04_pl â”† â€¦ â”† f_64_ph â”† f_65_c â”† f_66_n â”† f_67_n â”† text_t â”‚\nâ”‚ ---    â”† st_tens â”† rfect_a â”† esent_t â”† ace_adv â”†   â”† rasal_c â”† lausal â”† eg_syn â”† eg_ana â”† ype    â”‚\nâ”‚ str    â”† e       â”† spect   â”† ense    â”† erbials â”†   â”† oordina â”† _coord â”† thetic â”† lytic  â”† ---    â”‚\nâ”‚        â”† ---     â”† ---     â”† ---     â”† ---     â”†   â”† tion    â”† inatio â”† ---    â”† ---    â”† str    â”‚\nâ”‚        â”† f64     â”† f64     â”† f64     â”† f64     â”†   â”† ---     â”† n      â”† f64    â”† f64    â”†        â”‚\nâ”‚        â”†         â”†         â”†         â”†         â”†   â”† f64     â”† ---    â”†        â”†        â”†        â”‚\nâ”‚        â”†         â”†         â”†         â”†         â”†   â”†         â”† f64    â”†        â”†        â”†        â”‚\nâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•¡\nâ”‚ A01    â”† 52.4752 â”† 5.44554 â”† 22.2772 â”† 0.0     â”† â€¦ â”† 7.42574 â”† 4.9504 â”† 1.4851 â”† 4.4554 â”† PRESS: â”‚\nâ”‚        â”† 48      â”† 5       â”† 28      â”†         â”†   â”† 3       â”† 95     â”† 49     â”† 46     â”† REPORT â”‚\nâ”‚        â”†         â”†         â”†         â”†         â”†   â”†         â”†        â”†        â”†        â”† AGE    â”‚\nâ”‚ A02    â”† 36.3994 â”† 4.91883 â”† 20.6591 â”† 0.0     â”† â€¦ â”† 3.93507 â”† 1.4756 â”† 0.4918 â”† 3.4431 â”† PRESS: â”‚\nâ”‚        â”† 1       â”† 9       â”† 24      â”†         â”†   â”† 1       â”† 52     â”† 84     â”† 87     â”† REPORT â”‚\nâ”‚        â”†         â”†         â”†         â”†         â”†   â”†         â”†        â”†        â”†        â”† AGE    â”‚\nâ”‚ A03    â”† 47.4095 â”† 4.39882 â”† 15.1515 â”† 0.48875 â”† â€¦ â”† 9.77517 â”† 1.4662 â”† 1.4662 â”† 5.3763 â”† PRESS: â”‚\nâ”‚        â”† 8       â”† 7       â”† 15      â”† 9       â”†   â”† 1       â”† 76     â”† 76     â”† 44     â”† REPORT â”‚\nâ”‚        â”†         â”†         â”†         â”†         â”†   â”†         â”†        â”†        â”†        â”† AGE    â”‚\nâ”‚ A04    â”† 33.3660 â”† 17.1737 â”† 30.9126 â”† 2.45338 â”† â€¦ â”† 6.86948 â”† 5.3974 â”† 2.9440 â”† 6.3788 â”† PRESS: â”‚\nâ”‚        â”† 45      â”†         â”† 59      â”† 6       â”†   â”†         â”† 48     â”† 63     â”† 03     â”† REPORT â”‚\nâ”‚        â”†         â”†         â”†         â”†         â”†   â”†         â”†        â”†        â”†        â”† AGE    â”‚\nâ”‚ A05    â”† 39.9221 â”† 6.81596 â”† 32.1324 â”† 1.94742 â”† â€¦ â”† 3.89483 â”† 2.4342 â”† 1.9474 â”† 3.8948 â”† PRESS: â”‚\nâ”‚        â”† 03      â”† 9       â”† 25      â”†         â”†   â”† 9       â”† 75     â”† 2      â”† 39     â”† REPORT â”‚\nâ”‚        â”†         â”†         â”†         â”†         â”†   â”†         â”†        â”†        â”†        â”† AGE    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n[INFO] Using MATTR for f_43_type_token (window=100)\n[INFO] All features normalized per 1000 tokens except: f_43_type_token and f_44_mean_word_length\n\n\n\n\n11.4.2 Extract Feature Columns\n\n# Get all Biber feature columns (f_01, f_02, etc.)\nfeature_cols = [col for col in dfm_biber.columns if col.startswith('f_')]\n\nprint(f\"Analyzing {len(feature_cols)} linguistic features\")\nprint(f\"\\nSample features:\")\nfor i, col in enumerate(feature_cols[:10], 1):\n    print(f\"  {i}. {col}\")\n    \n# Check if expected features exist\nif 'f_42_nominalizations' not in feature_cols:\n    print(\"\\nWarning: Feature names may differ from expected. Actual feature columns:\")\n    print(feature_cols[:20])\n\nAnalyzing 67 linguistic features\n\nSample features:\n  1. f_01_past_tense\n  2. f_02_perfect_aspect\n  3. f_03_present_tense\n  4. f_04_place_adverbials\n  5. f_05_time_adverbials\n  6. f_06_first_person_pronouns\n  7. f_07_second_person_pronouns\n  8. f_08_third_person_pronouns\n  9. f_09_pronoun_it\n  10. f_10_demonstrative_pronoun\n\nWarning: Feature names may differ from expected. Actual feature columns:\n['f_01_past_tense', 'f_02_perfect_aspect', 'f_03_present_tense', 'f_04_place_adverbials', 'f_05_time_adverbials', 'f_06_first_person_pronouns', 'f_07_second_person_pronouns', 'f_08_third_person_pronouns', 'f_09_pronoun_it', 'f_10_demonstrative_pronoun', 'f_11_indefinite_pronouns', 'f_12_proverb_do', 'f_13_wh_question', 'f_14_nominalizations', 'f_15_gerunds', 'f_16_other_nouns', 'f_17_agentless_passives', 'f_18_by_passives', 'f_19_be_main_verb', 'f_20_existential_there']\n\n\nFeature examples (see pybiber documentation for full list):\n\nf_01_past_tense: Past tense verb forms\nf_02_perfect_aspect: Perfect aspect (have/has + past participle)\nf_18_first_person_pronouns: I, me, my, we, us, our\nf_19_second_person_pronouns: you, your, yourself\nf_42_nominalizations: -tion, -ment, -ness, -ity endings\nf_43_type_token: Lexical diversity (MATTR)\nf_44_mean_word_length: Average word length in characters\n\n\n\n11.4.3 Compute Correlation Matrix\n\n# Extract features as numpy array and compute correlations\nfeatures_array = dfm_biber.select(feature_cols).to_numpy().astype(np.float64)\n\n# Compute pairwise Pearson correlations using numpy\ncorrelation_matrix = np.corrcoef(features_array, rowvar=False)\n\n# Convert to pandas DataFrame for easier manipulation and visualization\ncorrelation_matrix = pd.DataFrame(\n    correlation_matrix,\n    index=feature_cols,\n    columns=feature_cols\n)\n\nprint(f\"Correlation matrix shape: {correlation_matrix.shape}\")\nprint(f\"Total pairwise correlations: {correlation_matrix.shape[0] * (correlation_matrix.shape[0] - 1) // 2}\")\n\nCorrelation matrix shape: (67, 67)\nTotal pairwise correlations: 2211\n\n\n\n\n11.4.4 Visualize: Correlation Heatmap\n\nplt.figure(figsize=(14, 12))\n\n# Create heatmap\nsns.heatmap(\n    correlation_matrix,\n    cmap='RdBu_r',  # Red-blue diverging colormap\n    center=0,        # Center colormap at 0\n    vmin=-1,\n    vmax=1,\n    square=True,\n    linewidths=0.5,\n    cbar_kws={'label': 'Pearson r', 'shrink': 0.8}\n)\n\nplt.title('Correlation Matrix: Biber Features (Brown Corpus)', fontsize=16, pad=20)\nplt.xlabel('Linguistic Features', fontsize=12)\nplt.ylabel('Linguistic Features', fontsize=12)\nplt.xticks(rotation=90, fontsize=7)\nplt.yticks(rotation=0, fontsize=7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigureÂ 11.1: Correlation matrix for Biberâ€™s 67 linguistic features. Red indicates positive correlation, blue indicates negative correlation. Patterns reveal functional groupings.\n\n\n\n\n\nReading the heatmap:\n\nDark red: Strong positive correlation (features co-occur)\nDark blue: Strong negative correlation (features anti-correlate)\nWhite: No correlation (independent variation)\nClusters: Blocks of similar colors indicate groups of related features\n\n\n\n11.4.5 Identify Strongest Correlations\n\n# Extract upper triangle (avoid duplicates and diagonal)\nupper_tri = correlation_matrix.where(\n    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n)\n\n# Convert to long format for sorting\ncorrelations_long = []\nfor i in range(len(correlation_matrix.columns)):\n    for j in range(i+1, len(correlation_matrix.columns)):\n        correlations_long.append({\n            'feature_1': correlation_matrix.columns[i],\n            'feature_2': correlation_matrix.columns[j],\n            'correlation': correlation_matrix.iloc[i, j]\n        })\n\ncorr_df = pd.DataFrame(correlations_long)\n\n# Sort by absolute correlation\ncorr_df['abs_corr'] = corr_df['correlation'].abs()\ncorr_df_sorted = corr_df.sort_values('abs_corr', ascending=False)\n\nprint(\"Top 20 strongest correlations (positive and negative):\\n\")\nprint(corr_df_sorted.head(20).to_string(index=False))\n\nTop 20 strongest correlations (positive and negative):\n\n                 feature_1                  feature_2  correlation  abs_corr\n      f_14_nominalizations      f_44_mean_word_length     0.815438  0.815438\n           f_01_past_tense f_08_third_person_pronouns     0.776922  0.776922\n           f_01_past_tense         f_03_present_tense    -0.771474  0.771474\n          f_16_other_nouns               f_42_adverbs    -0.755530  0.755530\n             f_40_adj_attr      f_44_mean_word_length     0.749926  0.749926\n              f_42_adverbs          f_67_neg_analytic     0.698791  0.698791\n      f_14_nominalizations              f_40_adj_attr     0.673373  0.673373\n         f_19_be_main_verb              f_41_adj_pred     0.672024  0.672024\n     f_44_mean_word_length  f_65_clausal_coordination    -0.669879  0.669879\n         f_59_contractions          f_67_neg_analytic     0.667174  0.667174\nf_08_third_person_pronouns      f_44_mean_word_length    -0.662088  0.662088\n           f_01_past_tense              f_40_adj_attr    -0.658111  0.658111\n         f_39_prepositions          f_59_contractions    -0.655534  0.655534\n           f_01_past_tense      f_44_mean_word_length    -0.649804  0.649804\n     f_44_mean_word_length          f_59_contractions    -0.647208  0.647208\n           f_01_past_tense       f_14_nominalizations    -0.645142  0.645142\n          f_16_other_nouns          f_67_neg_analytic    -0.642385  0.642385\n  f_11_indefinite_pronouns          f_67_neg_analytic     0.638629  0.638629\n         f_39_prepositions      f_44_mean_word_length     0.636497  0.636497\nf_08_third_person_pronouns    f_25_present_participle     0.627919  0.627919\n\n\nInterpretation questions:\n\nWhich features show strongest positive correlations? (Likely features from same functional category)\nWhich features show strongest negative correlations? (Likely features from opposing communicative modes)\nAre there surprising pairs? (Features you wouldnâ€™t expect to correlate based on linguistic theory)\n\n\n\n11.4.6 Examine Specific Correlation\nLetâ€™s examine one strong positive correlation in detail:\n\n# Example: Correlation between two features\n# Use available features (names may vary by pybiber version)\n\n# Try to find nominalization and preposition features\nnom_features = [col for col in feature_cols if 'nominal' in col.lower()]\nprep_features = [col for col in feature_cols if 'prep' in col.lower() or 'preposition' in col.lower()]\n\n# Use first available, or fall back to first two features\nif nom_features and prep_features:\n    feature_x = nom_features[0]\n    feature_y = prep_features[0]\nelse:\n    feature_x = feature_cols[0] if len(feature_cols) &gt; 0 else 'f_01'\n    feature_y = feature_cols[1] if len(feature_cols) &gt; 1 else 'f_02'\n\n# Get correlation coefficient and p-value\nr, p = pearsonr(dfm_biber[feature_x].to_numpy(), dfm_biber[feature_y].to_numpy())\n\nprint(f\"\\nCorrelation Analysis:\")\nprint(f\"  Feature 1: {feature_x}\")\nprint(f\"  Feature 2: {feature_y}\")\nprint(f\"  Pearson r: {r:.3f}\")\nprint(f\"  P-value: {p:.4f}\")\nprint(f\"  Interpretation: {'Significant' if p &lt; 0.05 else 'Not significant'}\")\nprint(f\"  Strength: \", end=\"\")\nif abs(r) &lt; 0.3:\n    print(\"Weak\")\nelif abs(r) &lt; 0.5:\n    print(\"Moderate\")\nelse:\n    print(\"Strong\")\n\n\nCorrelation Analysis:\n  Feature 1: f_14_nominalizations\n  Feature 2: f_39_prepositions\n  Pearson r: 0.610\n  P-value: 0.0000\n  Interpretation: Significant\n  Strength: Strong\n\n\n\n\n11.4.7 Visualize: Scatterplot\n\nplt.figure(figsize=(8, 6))\n\n# Extract data as numpy arrays\nx_data = dfm_biber[feature_x].to_numpy()\ny_data = dfm_biber[feature_y].to_numpy()\n\nplt.scatter(\n    x_data,\n    y_data,\n    alpha=0.5,\n    s=30,\n    edgecolors='black',\n    linewidths=0.5\n)\n\n# Add trend line\nz = np.polyfit(x_data, y_data, 1)\np = np.poly1d(z)\nx_line = np.linspace(x_data.min(), x_data.max(), 100)\nplt.plot(x_line, p(x_line), \"r--\", alpha=0.8, linewidth=2, label=f'Trend line (r={r:.3f})')\n\nplt.xlabel(feature_x.replace('_', ' ').title(), fontsize=12)\nplt.ylabel(feature_y.replace('_', ' ').title(), fontsize=12)\nplt.title(f'Correlation: {feature_x} vs {feature_y}', fontsize=14)\nplt.legend()\nplt.grid(alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigureÂ 11.2: Scatterplot showing relationship between two correlated features. Each point is a document; trend line shows direction and strength of correlation.\n\n\n\n\n\nPattern recognition:\n\nTight clustering around trend line â†’ strong correlation\nWide spread â†’ weak correlation\nUpward slope â†’ positive correlation\nDownward slope â†’ negative correlation",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>Correlation and Multi-Collinearity</span>"
    ]
  },
  {
    "objectID": "tutorials/correlations.html#detecting-multi-collinearity",
    "href": "tutorials/correlations.html#detecting-multi-collinearity",
    "title": "11Â  Correlation and Multi-Collinearity",
    "section": "11.5 Detecting Multi-Collinearity",
    "text": "11.5 Detecting Multi-Collinearity\n\n11.5.1 Correlation Threshold\nCommon rule: Features with |r| &gt; 0.7 or 0.8 are considered highly collinear.\n\n# Find pairs with |r| &gt; 0.7\nhigh_collinearity = corr_df[corr_df['abs_corr'] &gt; 0.7].sort_values('abs_corr', ascending=False)\n\nprint(f\"\\nFeature pairs with |r| &gt; 0.7 (high multi-collinearity):\")\nprint(f\"  Count: {len(high_collinearity)} pairs\\n\")\nprint(high_collinearity.head(15).to_string(index=False))\n\n\nFeature pairs with |r| &gt; 0.7 (high multi-collinearity):\n  Count: 5 pairs\n\n           feature_1                  feature_2  correlation  abs_corr\nf_14_nominalizations      f_44_mean_word_length     0.815438  0.815438\n     f_01_past_tense f_08_third_person_pronouns     0.776922  0.776922\n     f_01_past_tense         f_03_present_tense    -0.771474  0.771474\n    f_16_other_nouns               f_42_adverbs    -0.755530  0.755530\n       f_40_adj_attr      f_44_mean_word_length     0.749926  0.749926\n\n\nDecision: If building a regression model, consider:\n\nDropping one feature from each pair\nCombining them into composite score (average or sum)\nUsing dimension reduction (PCA, factor analysis)\n\n\n\n11.5.2 Variance Inflation Factor (VIF)\nVIF quantifies how much a predictorâ€™s variance is inflated by multi-collinearity:\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Compute VIF for each feature\n# Extract features as numpy array\nfeatures_array = dfm_biber.select(feature_cols).to_numpy().astype(np.float64)\n\nvif_data = pd.DataFrame()\nvif_data['Feature'] = feature_cols\nvif_data['VIF'] = [\n    variance_inflation_factor(features_array, i) \n    for i in range(len(feature_cols))\n]\n\nvif_data = vif_data.sort_values('VIF', ascending=False)\n\nprint(\"\\nTop 20 features by VIF (highest multi-collinearity):\\n\")\nprint(vif_data.head(20).to_string(index=False))\n\nprint(f\"\\nSummary:\")\nprint(f\"  Features with VIF &gt; 10 (severe): {(vif_data['VIF'] &gt; 10).sum()}\")\nprint(f\"  Features with VIF &gt; 5 (moderate): {(vif_data['VIF'] &gt; 5).sum()}\")\nprint(f\"  Features with VIF &lt; 5 (acceptable): {(vif_data['VIF'] &lt; 5).sum()}\")\n\n\nTop 20 features by VIF (highest multi-collinearity):\n\n                   Feature         VIF\n     f_44_mean_word_length 1690.164719\n           f_43_type_token 1254.412056\n          f_16_other_nouns  273.946439\n         f_39_prepositions  128.226261\n              f_42_adverbs  119.101396\n           f_01_past_tense   80.699838\n             f_40_adj_attr   60.557059\n         f_19_be_main_verb   48.671424\n        f_03_present_tense   48.050500\n          f_13_wh_question   46.890158\nf_08_third_person_pronouns   23.161913\n      f_14_nominalizations   22.655285\n         f_56_verb_private   19.516941\n          f_24_infinitives   18.819103\n         f_67_neg_analytic   15.945096\n             f_41_adj_pred   13.979006\n f_65_clausal_coordination   13.061420\n   f_17_agentless_passives   11.978803\n           f_09_pronoun_it   10.815481\n        f_38_other_adv_sub   10.631935\n\nSummary:\n  Features with VIF &gt; 10 (severe): 21\n  Features with VIF &gt; 5 (moderate): 41\n  Features with VIF &lt; 5 (acceptable): 26\n\n\nInterpretation:\n\nVIF = 1: No multi-collinearity (feature independent of all others)\nVIF = 5: Featureâ€™s variance inflated 5Ã— by correlations with other features\nVIF = 10: Severe multi-collinearity (80% of variance explained by other features: 1 - 1/10 = 0.9)\n\nHigh VIF is expected for linguistic features because they naturally cluster by function. This is a problem for regression interpretation but an opportunity for dimension reduction.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>Correlation and Multi-Collinearity</span>"
    ]
  },
  {
    "objectID": "tutorials/correlations.html#multi-collinearity-and-dimension-reduction",
    "href": "tutorials/correlations.html#multi-collinearity-and-dimension-reduction",
    "title": "11Â  Correlation and Multi-Collinearity",
    "section": "11.6 Multi-Collinearity and Dimension Reduction",
    "text": "11.6 Multi-Collinearity and Dimension Reduction\n\n11.6.1 Why Correlation Enables Factor Analysis\nFactor analysis (used in MDA) leverages multi-collinearity to discover latent dimensions:\nLogic:\n\nIdentify correlated clusters: Features that correlate form functional groups\nExtract latent factors: Each cluster reflects an underlying dimension\nInterpret factors: Give communicative/functional label based on loadings\n\nExample (from MDA tutorial):\nDimension 1: Involved vs.Â Informational Production\n\nPositive loadings (involved): first-person pronouns, contractions, present tense (correlated with each other)\nNegative loadings (informational): nominalizations, prepositions, attributive adjectives (correlated with each other)\n\nThese two clusters are negatively correlated with each other (involved features decrease when informational features increase), forming opposite poles of one dimension.\n\n\n11.6.2 Correlation Matrix for Factor Analysis\n\n# Assess suitability for factor analysis\n\n# 1. Count correlations above threshold\nsufficient_corrs = (correlation_matrix.abs() &gt; 0.3).sum().sum() - len(feature_cols)  # Subtract diagonal\ntotal_corrs = len(feature_cols) * (len(feature_cols) - 1)\n\nprint(\"Factor Analysis Suitability:\\n\")\nprint(f\"  1. Sufficient correlations (|r| &gt; 0.3): {sufficient_corrs}/{total_corrs} ({sufficient_corrs/total_corrs:.1%})\")\n\n# 2. Check for perfect multi-collinearity (bad even for FA)\nperfect_corrs = ((correlation_matrix.abs() &gt; 0.95) & (correlation_matrix.abs() &lt; 1.0)).sum().sum() // 2\nprint(f\"  2. Perfect multi-collinearity (|r| &gt; 0.95): {perfect_corrs} pairs (should be 0)\")\n\n# 3. Bartlett's test of sphericity\nfrom factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n\nchi_square, p_value = calculate_bartlett_sphericity(features_array)\nprint(f\"  3. Bartlett's test: Ï‡Â² = {chi_square:.1f}, p = {p_value:.4f}\")\nprint(f\"     {'âœ“ Adequate correlations' if p_value &lt; 0.05 else 'âœ— Insufficient correlations'}\")\n\n# 4. KMO measure of sampling adequacy\nfrom factor_analyzer.factor_analyzer import calculate_kmo\n\nkmo_all, kmo_model = calculate_kmo(features_array)\nprint(f\"  4. KMO measure: {kmo_model:.3f}\")\nif kmo_model &gt;= 0.9:\n    print(\"     âœ“ Excellent sampling adequacy\")\nelif kmo_model &gt;= 0.8:\n    print(\"     âœ“ Good sampling adequacy\")\nelif kmo_model &gt;= 0.6:\n    print(\"     âœ“ Adequate sampling adequacy\")\nelse:\n    print(\"     âœ— Poor sampling adequacy\")\n\nFactor Analysis Suitability:\n\n  1. Sufficient correlations (|r| &gt; 0.3): 798/4422 (18.0%)\n  2. Perfect multi-collinearity (|r| &gt; 0.95): 8 pairs (should be 0)\n  3. Bartlett's test: Ï‡Â² = 17979.3, p = 0.0000\n     âœ“ Adequate correlations\n  4. KMO measure: 0.839\n     âœ“ Good sampling adequacy\n\n\nConclusion: If Bartlettâ€™s test is significant (p &lt; 0.05) and KMO &gt; 0.6, the correlation matrix is suitable for factor analysis. This confirms that multi-collinearity exists and can be leveraged for dimension reduction.\n\n\n11.6.3 Visualize: Correlation Network\n\nimport networkx as nx\n\n# Create network graph\nG = nx.Graph()\n\n# Add nodes (features)\nG.add_nodes_from(feature_cols)\n\n# Add edges for correlations above threshold\nthreshold = 0.4\nfor _, row in corr_df[corr_df['abs_corr'] &gt; threshold].iterrows():\n    G.add_edge(row['feature_1'], row['feature_2'], weight=row['correlation'])\n\n# Layout\npos = nx.spring_layout(G, k=0.5, iterations=50, seed=42)\n\n# Draw\nplt.figure(figsize=(12, 10))\nnx.draw_networkx_nodes(G, pos, node_size=100, node_color='lightblue', alpha=0.7)\nnx.draw_networkx_edges(G, pos, alpha=0.3, width=1)\nnx.draw_networkx_labels(G, pos, font_size=6)\n\nplt.title(f'Correlation Network (|r| &gt; {threshold})', fontsize=16)\nplt.axis('off')\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nNetwork statistics:\")\nprint(f\"  Nodes (features): {G.number_of_nodes()}\")\nprint(f\"  Edges (correlations &gt; {threshold}): {G.number_of_edges()}\")\nprint(f\"  Density: {nx.density(G):.3f}\")\n\n\n\n\n\n\n\nFigureÂ 11.3: Network visualization of feature correlations. Nodes are features; edges connect correlated pairs (|r| &gt; 0.4). Clusters reveal functional groupings.\n\n\n\n\n\n\nNetwork statistics:\n  Nodes (features): 67\n  Edges (correlations &gt; 0.4): 198\n  Density: 0.090\n\n\nInterpretation:\n\nDense clusters: Groups of tightly correlated features (potential factors)\nBridge nodes: Features that connect multiple clusters\nIsolated nodes: Features with few strong correlations (may not load on major factors)",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>Correlation and Multi-Collinearity</span>"
    ]
  },
  {
    "objectID": "tutorials/correlations.html#practical-applications",
    "href": "tutorials/correlations.html#practical-applications",
    "title": "11Â  Correlation and Multi-Collinearity",
    "section": "11.7 Practical Applications",
    "text": "11.7 Practical Applications\n\n11.7.1 1. Feature Selection for Classification\nProblem: Youâ€™re building a classifier to distinguish academic from journalistic writing. You have 67 Biber features, but many are highly correlated.\nSolution:\n\n# Step 1: Identify redundant features (|r| &gt; 0.8)\nredundant_pairs = corr_df[corr_df['abs_corr'] &gt; 0.8].copy()\n\n# Step 2: For each pair, keep the feature with higher variance (more information)\nfeatures_to_drop = set()\n\nfor _, row in redundant_pairs.iterrows():\n    feat1, feat2 = row['feature_1'], row['feature_2']\n    \n    # Compare variances\n    var1 = dfm_biber[feat1].to_numpy().var()\n    var2 = dfm_biber[feat2].to_numpy().var()\n    \n    # Drop lower-variance feature\n    to_drop = feat1 if var1 &lt; var2 else feat2\n    features_to_drop.add(to_drop)\n\nprint(f\"\\nFeature Selection for Classification:\")\nprint(f\"  Original features: {len(feature_cols)}\")\nprint(f\"  Features to drop (redundant): {len(features_to_drop)}\")\nprint(f\"  Retained features: {len(feature_cols) - len(features_to_drop)}\")\n\nprint(f\"\\nFeatures dropped:\")\nfor feat in sorted(features_to_drop)[:10]:\n    print(f\"  - {feat}\")\nif len(features_to_drop) &gt; 10:\n    print(f\"  ... and {len(features_to_drop) - 10} more\")\n\n\nFeature Selection for Classification:\n  Original features: 67\n  Features to drop (redundant): 1\n  Retained features: 66\n\nFeatures dropped:\n  - f_44_mean_word_length\n\n\nAlternative: Use PCA to create orthogonal components (no multi-collinearity by design).\n\n\n11.7.2 2. Understanding Functional Groupings\nProblem: Which features co-occur to serve similar communicative functions?\nSolution: Examine high-correlation clusters:\n\n# Example: Find all features strongly correlated with first-person pronouns\n# Find a feature related to first-person or pronouns\npronoun_features = [col for col in feature_cols if 'pronoun' in col.lower() or 'first' in col.lower()]\ntarget_feature = pronoun_features[0] if pronoun_features else feature_cols[0]\n\n# Get correlations with target\ntarget_corrs = correlation_matrix[target_feature].drop(target_feature)\ntarget_corrs_sorted = target_corrs.abs().sort_values(ascending=False)\n\nprint(f\"\\nFeatures most correlated with {target_feature}:\\n\")\nfor feat, corr in target_corrs_sorted.head(10).items():\n    direction = \"+\" if correlation_matrix.loc[feat, target_feature] &gt; 0 else \"-\"\n    print(f\"  {direction} {feat}: r = {correlation_matrix.loc[feat, target_feature]:.3f}\")\n\n\nFeatures most correlated with f_06_first_person_pronouns:\n\n  + f_65_clausal_coordination: r = 0.528\n  - f_44_mean_word_length: r = -0.521\n  + f_59_contractions: r = 0.486\n  + f_67_neg_analytic: r = 0.476\n  - f_16_other_nouns: r = -0.470\n  + f_42_adverbs: r = 0.454\n  + f_56_verb_private: r = 0.451\n  - f_39_prepositions: r = -0.402\n  - f_17_agentless_passives: r = -0.368\n  + f_50_discourse_particles: r = 0.356\n\n\nInterpretation: Features that correlate with first-person pronouns likely characterize involved, personal discourse. This clustering motivates the Involved Production dimension in MDA.\n\n\n11.7.3 3. Hypothesis Testing\nProblem: You hypothesize that nominalized writing (high nominalizations) uses more prepositions (due to complex noun phrases).\nSolution: Test correlation significance:\n\n# Test hypothesis\n# Use same features as above example\nfeat_x = feature_x\nfeat_y = feature_y\n\nr, p = pearsonr(dfm_biber[feat_x].to_numpy(), dfm_biber[feat_y].to_numpy())\n\nprint(f\"\\nHypothesis Test: Nominalizations ~ Prepositions\")\nprint(f\"  Hâ‚€: No correlation (r = 0)\")\nprint(f\"  Hâ‚: Positive correlation (r &gt; 0)\")\nprint(f\"  Observed r: {r:.3f}\")\nprint(f\"  P-value: {p:.6f}\")\nprint(f\"  Result: {'Reject Hâ‚€' if p &lt; 0.05 else 'Fail to reject Hâ‚€'}\")\nprint(f\"  Conclusion: {'Significant positive correlation supports hypothesis' if (p &lt; 0.05 and r &gt; 0) else 'Hypothesis not supported'}\")\n\n\nHypothesis Test: Nominalizations ~ Prepositions\n  Hâ‚€: No correlation (r = 0)\n  Hâ‚: Positive correlation (r &gt; 0)\n  Observed r: 0.610\n  P-value: 0.000000\n  Result: Reject Hâ‚€\n  Conclusion: Significant positive correlation supports hypothesis\n\n\n\n\n11.7.4 4. Comparing Correlation Across Corpora\nProblem: Do feature correlations differ across registers?\nSolution: Compute separate correlation matrices for genres:\n\n# Assume we have genre labels\n# dfm_biber = dfm_biber.with_columns(pl.lit(\"genre_column\").alias(\"text_type\"))\n\n# Example: Compare academic vs. fiction\n# academic_features = dfm_biber.filter(pl.col('text_type') == 'learned').select(feature_cols).to_pandas()\n# fiction_features = dfm_biber.filter(pl.col('text_type') == 'fiction').select(feature_cols).to_pandas()\n\n# corr_academic = academic_features.corr()\n# corr_fiction = fiction_features.corr()\n\n# # Compare specific correlation\n# feat_pair = ('f_18_first_person_pronouns', 'f_01_past_tense')\n# r_academic = corr_academic.loc[feat_pair[0], feat_pair[1]]\n# r_fiction = corr_fiction.loc[feat_pair[0], feat_pair[1]]\n\n# print(f\"Correlation: {feat_pair[0]} ~ {feat_pair[1]}\")\n# print(f\"  Academic: r = {r_academic:.3f}\")\n# print(f\"  Fiction: r = {r_fiction:.3f}\")\n# print(f\"  Difference: Î”r = {abs(r_academic - r_fiction):.3f}\")\n\nprint(\"\\n(Example codeâ€”requires genre-labeled data)\")\n\n\n(Example codeâ€”requires genre-labeled data)\n\n\nInterpretation: Different correlations across genres suggest that features serve different functions in different contexts.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>Correlation and Multi-Collinearity</span>"
    ]
  },
  {
    "objectID": "tutorials/correlations.html#methodological-considerations",
    "href": "tutorials/correlations.html#methodological-considerations",
    "title": "11Â  Correlation and Multi-Collinearity",
    "section": "11.8 Methodological Considerations",
    "text": "11.8 Methodological Considerations\n\n11.8.1 1. Sample Size\nCorrelation stability depends on sample size:\n\nn &lt; 30: Correlations unstable, prone to sampling error\nn = 100: Moderate stability, r â‰ˆ 0.25 detectable\nn = 500: Good stability, r â‰ˆ 0.12 detectable\nn &gt; 1000: Excellent stability, small correlations detectable\n\nBrown Corpus (n â‰ˆ 500) provides stable estimates for moderate-to-strong correlations.\nSignificance vs.Â meaningfulness: With large n, even tiny correlations (r = 0.05) can be statistically significant but substantively meaningless.\n\n\n11.8.2 2. Outliers\nOutliers can distort correlations:\n\nOne extreme document can artificially inflate or deflate r\nCheck scatterplots visually\nConsider robust alternatives (Spearman correlation)\n\nSolution: Examine influential points, consider removing or transforming extreme values.\n\n\n11.8.3 3. Linearity Assumption\nPearson correlation assumes linear relationships. If relationship is non-linear (curved), Pearson underestimates strength.\nCheck: Examine scatterplots for curvature.\nSolution: Use Spearman (handles monotonic non-linear relationships) or transform variables (log, square root).\n\n\n11.8.4 4. Range Restriction\nProblem: If you only analyze academic writing (restricted range on informational features), correlations may be weaker than in full population.\nExample: In a corpus of only academic papers, nominalizations and prepositions may show weak correlation (both are consistently high). In a mixed corpus (academic + fiction), correlation is stronger.\nSolution: Ensure sufficient variation in both variables for meaningful correlation estimates.\n\n\n11.8.5 5. Shared Method Variance\nProblem: Features computed from same source (same linguistic annotation) may correlate due to measurement error, not true relationship.\nExample: If POS tagger makes systematic errors, features based on POS tags may show spurious correlations.\nSolution: Validate key findings with independent annotation or alternative feature extraction methods.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>Correlation and Multi-Collinearity</span>"
    ]
  },
  {
    "objectID": "tutorials/correlations.html#connections-to-other-methods",
    "href": "tutorials/correlations.html#connections-to-other-methods",
    "title": "11Â  Correlation and Multi-Collinearity",
    "section": "11.9 Connections to Other Methods",
    "text": "11.9 Connections to Other Methods\n\n11.9.1 Correlation and Keyness\nKeyness identifies distinctive features between corpora. Correlation reveals which distinctive features co-occur.\nCombined approach:\n\nUse keyness to identify features distinguishing Genre A from Genre B\nCompute correlations among key features\nDiscover functional groupings of distinctive features\n\nExample: Academic writing is key for nominalizations, passives, and attributive adjectives. Correlation analysis reveals these all correlate (r â‰ˆ 0.5-0.7), forming an â€œinformational productionâ€ dimension.\n\n\n11.9.2 Correlation and Classification\nFeature engineering:\n\nCompute correlations among features\nCreate composite features (sum/average of correlated features)\nUse composites as classifier inputs (reduces dimensionality, improves interpretability)\n\nExample: Instead of 10 correlated pronoun features, create one â€œpronoun densityâ€ composite.\n\n\n11.9.3 Correlation and Embeddings\nContextual embeddings (from transformers) capture semantic relationships implicitly. Correlation makes them explicit.\nWorkflow:\n\nExtract embeddings for documents\nCompute pairwise document similarities (cosine)\nCheck if similarity correlates with metadata (genre, time period)\n\nExample: Do documents with high embedding similarity also have similar first-person pronoun rates? If yes, embeddings capture â€œpersonal involvementâ€ dimension.\n\n\n11.9.4 Correlation and Time Series\nDiachronic analysis: Do feature correlations change over time?\nExample: In 19th-century texts, passives and nominalizations may correlate weakly (independent choices). In modern academic writing, they correlate strongly (package deal for informational style).\nInterpretation: Evolving correlations reveal changing functional relationships in language use.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>Correlation and Multi-Collinearity</span>"
    ]
  },
  {
    "objectID": "tutorials/correlations.html#ethical-considerations",
    "href": "tutorials/correlations.html#ethical-considerations",
    "title": "11Â  Correlation and Multi-Collinearity",
    "section": "11.10 Ethical Considerations",
    "text": "11.10 Ethical Considerations\n\n11.10.1 1. Correlation Fishing\nRisk: Testing hundreds of correlations and reporting only significant ones (â€œp-hackingâ€).\nProblem: With 67 features, there are 2,211 pairwise correlations. By chance, ~110 will be â€œsignificantâ€ at p &lt; 0.05 even if no true relationships exist.\nBest practice:\n\nPre-register hypotheses: Specify predicted correlations before analysis\nBonferroni correction: Adjust Î± = 0.05 / 2,211 â‰ˆ 0.000023 for multiple comparisons\nReport all tests: Show full correlation matrix, not cherry-picked results\n\n\n\n11.10.2 2. Spurious Correlations\nRisk: Interpreting meaningful relationships from coincidental patterns.\nExample: Number of Nicholas Cage films correlates with swimming pool drownings (r = 0.67)â€”clearly spurious.\nIn text analysis: Two features may correlate in your corpus by chance (both happen to be common in Genre X) without reflecting functional relationship.\nBest practice:\n\nTheory first: Only interpret correlations with plausible linguistic motivation\nReplicate: Check if correlation holds in independent corpus\nCausal skepticism: Remember correlation â‰  causation\n\n\n\n11.10.3 3. Ecological Fallacy\nRisk: Assuming document-level correlations apply to within-document variation.\nExample: Across documents, nominalizations and prepositions correlate (academic texts have more of both). Within a single document, they might not correlate (varied sentence structures).\nBest practice: Specify level of analysis (document, paragraph, sentence) and donâ€™t generalize across levels.\n\n\n11.10.4 4. Reification of Dimensions\nRisk: Treating statistically-derived dimensions (factors, PCs) as real psychological or linguistic entities.\nExample: â€œDimension 1â€ from factor analysis is a mathematical construct explaining variance. Calling it â€œInvolved Productionâ€ is an interpretation, not a discovery of natural category.\nBest practice:\n\nPresent dimensions as analytical tools, not ontological claims\nAcknowledge alternative interpretations\nValidate dimensions against external criteria (genre labels, expert judgments)",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>Correlation and Multi-Collinearity</span>"
    ]
  },
  {
    "objectID": "tutorials/correlations.html#summary",
    "href": "tutorials/correlations.html#summary",
    "title": "11Â  Correlation and Multi-Collinearity",
    "section": "11.11 Summary",
    "text": "11.11 Summary\nCorrelation analysis reveals relationships among variables:\nKey concepts:\n\nPearson r: Measures linear relationship strength (-1 to +1)\nSignificance: p-value tests if r differs from 0\nStrength: |r| &gt; 0.5 strong, 0.3-0.5 moderate, &lt; 0.3 weak\n\nMulti-collinearity:\n\nProblem for regression/classification: Unstable coefficients, inflated variance (VIF &gt; 5-10)\nOpportunity for dimension reduction: Factor analysis and PCA leverage correlations\nDetection: Correlation matrix (|r| &gt; 0.7), VIF (&gt; 5)\n\nApplications:\n\nFeature selection: Drop redundant features for classification\nDimension reduction: Identify correlated clusters for factor analysis (MDA)\nTheory development: Discover functional groupings of linguistic features\nHypothesis testing: Test predicted relationships (e.g., nominalizations ~ prepositions)\n\nBest practices:\n\nVisualize first: Heatmaps, scatterplots reveal patterns\nConsider context: Strong correlation in one corpus may be weak in another\nCheck assumptions: Linearity, outliers, sample size\nReport transparently: Full correlation matrix, not just significant pairs\nInterpret cautiously: Correlation â‰  causation; validate theoretically\n\nConnections to MDA:\n\nMulti-collinearity among linguistic features is expected (functional groupings)\nFactor analysis identifies latent dimensions from correlation patterns\nSee MDA tutorial for how pybiber leverages correlation for dimension reduction\n\nNext steps: Apply correlation analysis to your data. Use it to inform feature selection (drop redundant features) or dimension reduction (identify correlated clusters for factor analysis). Combine with other methods (keyness, classification, MDA) for richer analysis.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>Correlation and Multi-Collinearity</span>"
    ]
  },
  {
    "objectID": "tutorials/correlations.html#further-reading",
    "href": "tutorials/correlations.html#further-reading",
    "title": "11Â  Correlation and Multi-Collinearity",
    "section": "11.12 Further Reading",
    "text": "11.12 Further Reading\nFoundational:\n\nCohen, J. (1988). Statistical Power Analysis for the Behavioral Sciences (2nd ed.). Routledge. (Correlation effect sizes)\nTabachnick, B. G., & Fidell, L. S. (2013). Using Multivariate Statistics (6th ed.). Pearson. (Chapter 4: Cleaning up your actâ€”screening data, including multi-collinearity)\n\nMulti-collinearity:\n\nDormann, C. F., et al.Â (2013). Collinearity: A review of methods to deal with it and a simulation study evaluating their performance. Ecography, 36(1), 27-46. DOI\nOâ€™Brien, R. M. (2007). A caution regarding rules of thumb for variance inflation factors. Quality & Quantity, 41(5), 673-690. DOI\n\nLinguistic applications:\n\nBiber, D. (1988). Variation Across Speech and Writing. Cambridge University Press. (Chapters 3-4: Factor analysis of correlated linguistic features)\nGries, S. T. (2013). Statistics for Linguistics with R (2nd ed.). De Gruyter Mouton. (Chapter 5: Correlations)\n\nDimension reduction:\n\nBaayen, R. H. (2008). Analyzing Linguistic Data: A Practical Introduction to Statistics Using R. Cambridge University Press. (Chapter 6: PCA and factor analysis)\nJolliffe, I. T., & Cadima, J. (2016). Principal component analysis: A review and recent developments. Philosophical Transactions of the Royal Society A, 374(2065). DOI\n\nPractical guides:\n\nScikit-learn: Feature Selection\nStatsmodels: Variance Inflation Factor\nSeaborn: Visualizing Correlation Matrices",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>11</span>Â  <span class='chapter-title'>Correlation and Multi-Collinearity</span>"
    ]
  },
  {
    "objectID": "tutorials/multi-dimensional-analysis.html",
    "href": "tutorials/multi-dimensional-analysis.html",
    "title": "12Â  Multi-Dimensional Analysis",
    "section": "",
    "text": "12.1 Introduction\nImagine comparing 15 different genres of writingâ€”news articles, academic papers, fiction, blog posts, legal documents. You might count individual features: passives, pronouns, nominalizations, hedges. But analyzing 50+ features across genres becomes overwhelming. Which features matter? How do they co-occur?\nMulti-Dimensional Analysis (MDA) solves this by revealing how linguistic features bundle together to distinguish text types. Rather than examining features in isolation, MDA identifies dimensions of variationâ€”underlying patterns that systematically differentiate registers, genres, or styles.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Multi-Dimensional Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/multi-dimensional-analysis.html#introduction",
    "href": "tutorials/multi-dimensional-analysis.html#introduction",
    "title": "12Â  Multi-Dimensional Analysis",
    "section": "",
    "text": "12.1.1 The Core Insight\nMDA, developed by Douglas Biber in the 1980s, rests on a key observation: linguistic features donâ€™t vary randomly. They co-occur in predictable patterns because they serve related communicative functions.\nFor example, involved production (conversation, personal letters) tends to have:\n\nHigh rates of first/second-person pronouns (I, you)\nPresent tense verbs (is, seems, think)\nContractions (itâ€™s, donâ€™t)\nPrivate verbs (feel, believe, want)\n\nWhile informational production (academic writing, official documents) tends to have:\n\nHigh rates of nouns and nominalizations (discussion, implementation)\nAttributive adjectives (significant, relevant)\nPrepositional phrases\nLonger words\n\nThese features donâ€™t just happen to co-occurâ€”they cluster because they all support the same communicative goal (personal interaction vs.Â information density).\n\n\n\n\n\n\nWhy â€œMulti-Dimensionalâ€?\n\n\n\nEach dimension captures a distinct pattern of co-variation. A corpus might vary along:\n\nDimension 1: Involved vs.Â informational production\nDimension 2: Narrative vs.Â non-narrative discourse\nDimension 3: Situation-dependent vs.Â elaborated reference\nDimension 4: Overt expression of persuasion\n\nTexts are scored on each dimension, creating a multi-dimensional profile. A personal blog might score high on D1 (involved) and low on D2 (non-narrative), while a historical novel scores high on D2 (narrative) but mid-range on D1.\n\n\n\n\n12.1.2 Research Questions MDA Can Answer\n\nRegister variation: How do academic writing, journalism, and fiction differ linguistically?\nHistorical change: Has scientific writing become more impersonal over time?\nGenre classification: Can we automatically distinguish mystery novels from romance based on linguistic profiles?\nCross-linguistic comparison: Do the same dimensions appear in English and Spanish?\nStyle attribution: Does Author A use more â€œnarrativeâ€ features than Author B?\n\n\n\n\n\n\n\nWhy MDA Matters for Humanities Research\n\n\n\nThe single-feature trap: You might notice that academic writing uses more passives than fiction (e.g., was observed vs.Â she observed). But is this a meaningful pattern or just one isolated feature?\nMDA reveals: Passives donâ€™t vary randomlyâ€”they cluster with nominalizations, attributive adjectives, and longer words. This bundle indicates a deeper pattern: informational production where ideas are densely packaged and agency is de-emphasized.\nImplication: Academic writing isnâ€™t just â€œmore passive.â€ It operates in a different communicative mode that requires specific linguistic resources (abstraction, nominalization, modification). Understanding this helps explain:\n\nWhy academic prose feels â€œdenseâ€ (multiple features create density together)\nHow genres constrain linguistic choices (you canâ€™t write academically with only pronouns and present tense)\nWhat features distinguish â€œacademicâ€ from â€œpopular scienceâ€ (dimensions reveal combinations, not just frequencies)\n\nMDA shifts the question from â€œwhat features differ?â€ to â€œwhat communicative modes exist, and what features realize them?â€ This is computational reasoning at its bestâ€”using statistics to discover functional patterns.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Multi-Dimensional Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/multi-dimensional-analysis.html#understanding-the-mda-workflow",
    "href": "tutorials/multi-dimensional-analysis.html#understanding-the-mda-workflow",
    "title": "12Â  Multi-Dimensional Analysis",
    "section": "12.2 Understanding the MDA Workflow",
    "text": "12.2 Understanding the MDA Workflow\n\n12.2.1 The Four Steps\n1. Identify relevant variables\nSelect linguistic features that might vary systematically across texts. These typically come from tagged corpora (POS tags, dependency parses, semantic categories).\nExample features:\n\nPast tense verbs\nModal verbs (can, should, must)\nAttributive adjectives\nType-token ratio\nAverage word length\n\n2. Extract factors from variables\nUse factor analysis to identify which variables co-occur. Variables that load on the same factor form a dimension.\nStatistical process: Factor analysis finds latent variables (factors) that explain correlations among observed variables. If passives, nominalizations, and attributive adjectives consistently co-occur across texts, theyâ€™ll load on the same factor.\n3. Functional interpretation of factors as dimensions\nExamine high-loading features and assign a functional label based on linguistic theory and the communicative purposes those features serve.\nExample interpretation: If a factor has high loadings for:\n\nPersonal pronouns (+)\nPresent tense (+)\nContractions (+)\nNominalizations (-)\nPassives (-)\n\nThis suggests an â€œinvolved vs.Â informationalâ€ dimensionâ€”one pole emphasizes personal interaction, the other informational density.\n4. Placement of categories on dimensions\nScore each text on the extracted dimensions and plot text categories (genres, registers, time periods) to see how they cluster.\nExample finding: Conversation and personal letters cluster at the â€œinvolvedâ€ pole, while academic prose and official documents cluster at the â€œinformationalâ€ pole.\n\n\n12.2.2 Why Factor Analysis?\nFactor analysis is ideal for MDA because:\n\nReduces complexity: 67 features â†’ 5 interpretable dimensions\nReveals co-occurrence patterns: Shows which features cluster together\nHandles correlations: Features are often correlatedâ€”factor analysis accounts for this\nProduces weights: Factor loadings show how strongly each feature contributes to a dimension\n\n\n\n\n\n\n\nStatistical Requirements\n\n\n\nFactor analysis requires:\n\nSufficient observations: Minimum 5-10 observations per variable (preferably 20+)\nContinuous variables: Normalized frequencies work well\nModerate correlations: Variables should correlate but not be redundant\n\nFor a corpus with 67 features, you need at least 350-670 texts (ideally 1,000+).",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Multi-Dimensional Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/multi-dimensional-analysis.html#the-biber-tagger-approach",
    "href": "tutorials/multi-dimensional-analysis.html#the-biber-tagger-approach",
    "title": "12Â  Multi-Dimensional Analysis",
    "section": "12.3 The Biber Tagger Approach",
    "text": "12.3 The Biber Tagger Approach\nDouglas Biberâ€™s foundational MDA studies used 67 lexicogrammatical features derived from grammatical tagging and parsing. The pybiber package replicates this approach using spaCy.\n\n12.3.1 Why Tagged Features?\nIn order to carry out MDA, we need at least 5 observations per variable (ideally 10-20). This generally precludes using simple word countsâ€”with 500 documents and 10,000 unique words, youâ€™d need 50,000-100,000 documents for stable factor analysis.\nInstead, we use tagged features that aggregate linguistic patterns into functional categories:\nExamples:\n\nPersonal pronouns (I, you, we)\nType-token ratio (lexical diversity)\nNominalizations (discussion, formation)\nPassives (was written, has been discussed)\n\nThe pybiber package emulates Biberâ€™s classification system, organizing 67 categories described here: https://browndw.github.io/pybiber/feature-categories.html\n\n\n12.3.2 Loading Data\nWeâ€™ll analyze the Brown Corpus (500 texts across 15 genres):\n\n# Load Brown Corpus with genre labels\nbrown_corpus = pl.read_parquet(\n    \"https://github.com/browndw/humanities_analytics/raw/refs/heads/main/data/data_tables/brown_corpus.parquet\"\n)\n\n# Select document IDs and texts\nbc = brown_corpus.select(\"doc_id\", \"text\")\nbc.head(3)\n\n\nshape: (3, 2)\n\n\n\ndoc_id\ntext\n\n\nstr\nstr\n\n\n\n\n\"A01\"\n\"The Fulton County Grand Jury sâ€¦\n\n\n\"A02\"\n\"Austin, Texas -- Committee appâ€¦\n\n\n\"A03\"\n\"Several defendants in the Summâ€¦\n\n\n\n\n\n\nThe Brown Corpus is the first computer-readable corpus of American English (1961), containing 500 text samples (~2,000 words each) across 15 genres:\n\nPress: Reportage, Editorial, Reviews\nReligion\nSkill and Hobbies\nPopular Lore\nBelles-Lettres\nLearned (academic/scientific writing)\nFiction: General, Mystery, Science Fiction, Adventure, Romance, Humor\n\nThis genre diversity makes the Brown Corpus ideal for MDAâ€”we expect systematic linguistic differences across these registers.\nReference: http://icame.uib.no/brown/bcm.html\n\n\n12.3.3 Tagging with Biber Features\n\n# Load spaCy model (disable NER for speed)\nnlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\n\n# Parse corpus with spaCy (this takes ~30 seconds with n_process=4)\ndf_spacy = pb.spacy_parse(corp=bc, nlp_model=nlp, n_process=4)\n\nPerformance: Corpus processing completed in 58.85s\n\n\nThe spacy_parse function processes each text through spaCyâ€™s pipeline, extracting POS tags and dependency relationships needed for Biber feature identification. The n_process=4 argument uses 4 CPU cores in parallel for faster processing.\n\n\n12.3.4 Aggregating Features\n\n# Aggregate into 67 Biber feature categories\ndfm_biber = pb.biber(df_spacy)\ndfm_biber.head(5)\n\n[INFO] Using MATTR for f_43_type_token (window=100)\n[INFO] All features normalized per 1000 tokens except: f_43_type_token and f_44_mean_word_length\n\n\n\nshape: (5, 68)\n\n\n\ndoc_id\nf_01_past_tense\nf_02_perfect_aspect\nf_03_present_tense\nf_04_place_adverbials\nâ€¦\nf_63_split_auxiliary\nf_64_phrasal_coordination\nf_65_clausal_coordination\nf_66_neg_synthetic\nf_67_neg_analytic\n\n\nstr\nf64\nf64\nf64\nf64\nâ€¦\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n\"A01\"\n52.475248\n5.445545\n22.277228\n0.0\nâ€¦\n1.485149\n7.425743\n4.950495\n1.485149\n4.455446\n\n\n\"A02\"\n36.39941\n4.918839\n20.659124\n0.0\nâ€¦\n0.983768\n3.935071\n1.475652\n0.491884\n3.443187\n\n\n\"A03\"\n47.40958\n4.398827\n15.151515\n0.488759\nâ€¦\n1.955034\n9.775171\n1.466276\n1.466276\n5.376344\n\n\n\"A04\"\n33.366045\n17.1737\n30.912659\n2.453386\nâ€¦\n3.43474\n6.86948\n5.397448\n2.944063\n6.378803\n\n\n\"A05\"\n39.922103\n6.815969\n32.132425\n1.94742\nâ€¦\n2.92113\n3.894839\n2.434275\n1.94742\n3.894839\n\n\n\n\n\n\nThe biber() function returns normalized frequencies (per 1,000 words) by default, making texts of different lengths comparable.\nExample features:\n\nf_01_past_tense: Count of past tense verb forms\nf_02_perfect_aspect: Have/has + past participle\nf_18_first_person_pronouns: I, me, my, we, us, our\nf_42_nominalizations: Words ending in -tion, -ment, -ness, -ity\nf_43_type_token: MATTR (moving-average type-token ratio) for lexical diversity\nf_44_mean_word_length: Average word length in characters\n\n\n\n12.3.5 Adding Metadata\n\n# Add genre labels from original corpus\ndfm_biber = dfm_biber.join(\n    brown_corpus.select(\"doc_id\", \"text_type\"), \n    on=\"doc_id\"\n)",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Multi-Dimensional Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/multi-dimensional-analysis.html#running-mda",
    "href": "tutorials/multi-dimensional-analysis.html#running-mda",
    "title": "12Â  Multi-Dimensional Analysis",
    "section": "12.4 Running MDA",
    "text": "12.4 Running MDA\n\n12.4.1 Prepare the Analyzer\n\n# Initialize BiberAnalyzer\ndf = pb.BiberAnalyzer(dfm_biber, id_column=True)\n\n\n\n12.4.2 Determine Number of Factors\nThe scree plot shows eigenvalues (variance explained) for each potential factor:\n\ndf.mdaviz_screeplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecision criteria:\n\nElbow method: Look for the bend where slope flattens (around factor 3-4)\nKaiser criterion: Extract factors with eigenvalue &gt; 1 (shown by red dashed line)\nInterpretability: Can you meaningfully interpret each factor?\n\nTypical range: 3-7 factors for most corpora\nOur decision: Extract 3 factors based on clear elbow at 3 and interpretability. While eigenvalues suggest we could extract more, we prioritize parsimony (fewer, clearer dimensions) over explanatory power (more variance explained).\n\n\n\n\n\n\nBalancing Parsimony vs.Â Power\n\n\n\nExtracting too few factors = lose important variation patterns\nExtracting too many factors = dimensions become uninterpretable\nThe goal is the smallest number of factors that still capture major register differences. 3-5 factors typically work well for genre/register studies.\n\n\n\n\n12.4.3 Extract Factors\n\ndf.mda(n_factors=3)\n\nINFO:pybiber.biber_analyzer:Dropping 2 variable(s) with max |r| &lt;= 0.20: ['f_61_stranded_preposition', 'f_62_split_infinitive']\n\n\nNote the information message: pybiber automatically drops variables with low correlations (max |r| &lt;= 0.20) because they donâ€™t contribute meaningfully to factor structure. In this case, stranded prepositions and split infinitives show minimal correlation with other features.\n\n\n12.4.4 Examine Results\nView summary statistics:\n\ndf.mda_summary\n\n\nshape: (3, 6)\n\n\n\nFactor\nF\ndf\nPR(&gt;F)\nSignif\nR2\n\n\nstr\nf64\nlist[i64]\nf64\nstr\nf64\n\n\n\n\n\"factor_1\"\n88.972607\n[14, 455]\n0.0\n\"*** p &lt; 0.001\"\n0.73245\n\n\n\"factor_2\"\n19.846179\n[14, 455]\n0.0\n\"*** p &lt; 0.001\"\n0.379133\n\n\n\"factor_3\"\n5.998623\n[14, 455]\n6.4466e-11\n\"*** p &lt; 0.001\"\n0.155814\n\n\n\n\n\n\nThis shows:\n\nVariance explained by each factor\nCumulative variance (total explained by all factors)\nEigenvalues (variance per factor)\n\nInterpretation: If 3 factors explain 60% of variance, thatâ€™s goodâ€”most linguistic variation captured by few dimensions. The remaining 40% represents unique variation not explained by common patterns.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Multi-Dimensional Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/multi-dimensional-analysis.html#interpreting-dimensions",
    "href": "tutorials/multi-dimensional-analysis.html#interpreting-dimensions",
    "title": "12Â  Multi-Dimensional Analysis",
    "section": "12.5 Interpreting Dimensions",
    "text": "12.5 Interpreting Dimensions\n\n12.5.1 Examine Factor Loadings\nFactor loadings show how strongly each linguistic feature contributes to each dimension:\n\n# Sort loadings for Factor 1 to see high positive/negative features\ndf.mda_loadings.sort(\"factor_1\")\n\n\nshape: (65, 4)\n\n\n\nfeature\nfactor_1\nfactor_2\nfactor_3\n\n\nstr\nf64\nf64\nf64\n\n\n\n\n\"f_03_present_tense\"\n-1.009852\n-0.034924\n0.942609\n\n\n\"f_40_adj_attr\"\n-0.808102\n0.150414\n0.05509\n\n\n\"f_14_nominalizations\"\n-0.777371\n0.147251\n-0.079733\n\n\n\"f_44_mean_word_length\"\n-0.747481\n-0.069632\n-0.191333\n\n\n\"f_45_conjuncts\"\n-0.679174\n0.29295\n0.157756\n\n\n\"f_39_prepositions\"\n-0.600508\n-0.070209\n-0.272134\n\n\n\"f_64_phrasal_coordination\"\n-0.522747\n-0.00872\n0.125395\n\n\n\"f_17_agentless_passives\"\n-0.462271\n-0.040817\n-0.203729\n\n\n\"f_51_demonstratives\"\n-0.456285\n0.279224\n0.110075\n\n\n\"f_53_modal_necessity\"\n-0.42862\n0.175607\n0.269923\n\n\nâ€¦\nâ€¦\nâ€¦\nâ€¦\n\n\n\"f_56_verb_private\"\n0.524676\n0.488517\n-0.067488\n\n\n\"f_11_indefinite_pronouns\"\n0.525606\n0.322819\n0.142859\n\n\n\"f_05_time_adverbials\"\n0.545754\n0.109797\n-0.048566\n\n\n\"f_65_clausal_coordination\"\n0.578759\n0.364878\n0.098526\n\n\n\"f_02_perfect_aspect\"\n0.582999\n0.331141\n-0.237912\n\n\n\"f_59_contractions\"\n0.594221\n0.017707\n0.275919\n\n\n\"f_04_place_adverbials\"\n0.633699\n-0.019547\n-0.13196\n\n\n\"f_25_present_participle\"\n0.671088\n0.02429\n-0.203594\n\n\n\"f_08_third_person_pronouns\"\n0.928793\n0.235852\n-0.277713\n\n\n\"f_01_past_tense\"\n1.229951\n0.220792\n-0.698066\n\n\n\n\n\n\nReading loadings:\n\nHigh positive (+0.7 to +1.0): Feature strongly present at positive pole of dimension\nHigh negative (-0.7 to -1.0): Feature strongly present at negative pole of dimension\nLow (close to 0): Feature doesnâ€™t contribute meaningfully to this dimension\n\nExample interpretation:\nIf Factor 1 shows:\n\nf_18_first_person_pronouns: +0.85\nf_03_present_tense: +0.78\nf_27_contractions: +0.72\nf_42_nominalizations: -0.80\nf_35_by_passives: -0.75\nf_19_attributive_adjectives: -0.70\n\nFunctional label: Involved vs.Â Informational Production\nRationale:\n\nPositive pole (+): Features of interaction and immediacy\n\nFirst-person pronouns signal personal involvement (I think, we believe)\nPresent tense creates immediacy (is, happens)\nContractions mark informal, conversational style (itâ€™s, donâ€™t)\n\nNegative pole (-): Features of informational density\n\nNominalizations pack meaning into noun phrases (the implementation of the policy)\nPassives deemphasize agency and create objectivity (was observed)\nAttributive adjectives add descriptive detail (significant findings, relevant data)\n\n\nThis aligns with Biberâ€™s Dimension 1: Involved vs.Â Informational Production, one of the most robust dimensions across English corpora.\n\n\n\n\n\n\nFunctional Interpretation Guidelines\n\n\n\n\nGroup high-loading features (both positive and negative): What communicative function do they share?\nConsult linguistic theory: What do corpus linguists say about these features? (Hallidayâ€™s functional grammar, Biberâ€™s register studies)\nExamine exemplar texts: Read texts scoring high/low on the dimensionâ€”do labels fit?\nCompare to Biberâ€™s dimensions: Do similar patterns emerge in your corpus?\n\nBiberâ€™s foundational dimensions: https://www.uni-bamberg.de/fileadmin/eng-ling/fs/Chapter_21/23DimensionsofEnglish.html\n\n\n\n\n12.5.2 Visualize Genre Differences\nPlot how genres score along each dimension:\n\ndf.mdaviz_groupmeans(factor=1, width=4, height=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nPositive scores (e.g., +15 to +30): High â€œinvolvedâ€ production\n\nFiction genres cluster here (mystery, romance, adventure) due to dialogue, character perspective, narrative immediacy\nHumor relies on conversational tone and personal engagement\n\nNegative scores (e.g., -10 to -20): High â€œinformationalâ€ production\n\nLearned/academic writing clusters here (dense information, objective stance)\nGovernment documents and reports (formal, impersonal)\nSkills/hobbies (procedural, informational)\n\nNear-zero scores (e.g., -3 to +3): Balanced or neutral\n\nPress genres (reportage, editorial, reviews) mix narrative with information\nReligion balances personal testimony with doctrinal exposition\nPopular lore mixes storytelling with factual claims\n\n\nPattern to look for: Do genres cluster as linguistic theory predicts?\n\nFiction should be high on â€œinvolvedâ€ or â€œnarrativeâ€\nAcademic writing should be high on â€œinformationalâ€ or â€œabstractâ€\nPress should occupy middle ground (narrative + information)\n\nIf patterns align, it validates both your factor interpretation and the robustness of register variation.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Multi-Dimensional Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/multi-dimensional-analysis.html#the-docuscope-approach",
    "href": "tutorials/multi-dimensional-analysis.html#the-docuscope-approach",
    "title": "12Â  Multi-Dimensional Analysis",
    "section": "12.6 The DocuScope Approach",
    "text": "12.6 The DocuScope Approach\nDocuScope provides an alternative feature set based on rhetorical functions rather than grammatical categories.\n\n12.6.1 Why Use DocuScope?\nBiber features (grammatical): Focus on syntax and grammatical choices (past tense, passives, nominalizations)\nDocuScope features (rhetorical): Focus on semantic clusters and discourse moves (reasoning, description, confidence, narrative)\nExample contrast:\n\nBiber might count past tense verbs (grammatical category)\nDocuScope identifies narrative sequences (rhetorical function of past-tense storytelling)\n\nDocuScope can reveal dimensions like:\n\nConcrete vs.Â abstract reference\nConfident vs.Â hedged assertions\nPersonal vs.Â impersonal stance\nReasoning vs.Â description\n\n\n\n12.6.2 DocuScope Categories\nDocuScope organizes over 100 rhetorical categories. Examples:\n\nReasoning: therefore, thus, consequently, it follows that\nDescription: blue, tall, enormous, ancient, modern\nCharacter: protagonist names, character mentions\nConfidence: certainly, definitely, obviously, clearly\nInformationStates: know, realize, understand, aware\n\nFull categories: https://docuscospacy.readthedocs.io/en/latest/docuscope.html#categories\n\n\n12.6.3 Processing with DocuScope\n\n# Load DocuScope model\nimport docuscospacy as ds\nnlp_ds = spacy.load(\"en_docusco_spacy\")\n\n# Parse corpus (~30 seconds with n_process=4)\nds_tokens = ds.docuscope_parse(bc, nlp_model=nlp_ds, n_process=4)\n\n# Create document-term matrix\ndfm_ds = ds.tags_dtm(ds_tokens, count_by=\"ds\")\ndfm_ds = ds.dtm_weight(dfm_ds)  # Normalize per 1,000 words\ndfm_ds = dfm_ds.drop(\"Untagged\")  # Remove untagged tokens\n\n# Add metadata\ndfm_ds = dfm_ds.join(brown_corpus.select(\"doc_id\", \"text_type\"), on=\"doc_id\")\n\ndfm_ds.head(3)\n\nPerformance: Corpus processing completed in 39.09s\n\n\n\nshape: (3, 38)\n\n\n\ndoc_id\nDescription\nCharacter\nNarrative\nAcademicTerms\nâ€¦\nResponsibility\nInformationChangeNegative\nConfidenceLow\nCitationHedged\ntext_type\n\n\nstr\nf64\nf64\nf64\nf64\nâ€¦\nf64\nf64\nf64\nf64\nstr\n\n\n\n\n\"A01\"\n0.045398\n0.101368\n0.083955\n0.04291\nâ€¦\n0.0\n0.0\n0.000622\n0.0\n\"PRESS: REPORTAGE\"\n\n\n\"A02\"\n0.048736\n0.10349\n0.0716\n0.051143\nâ€¦\n0.006017\n0.0\n0.0\n0.0\n\"PRESS: REPORTAGE\"\n\n\n\"A03\"\n0.04637\n0.090299\n0.087248\n0.058572\nâ€¦\n0.003661\n0.00122\n0.0\n0.0\n\"PRESS: REPORTAGE\"\n\n\n\n\n\n\n\n\n12.6.4 Run MDA on DocuScope Features\n\ndf_ds = pb.BiberAnalyzer(dfm_ds, id_column=True)\ndf_ds.mdaviz_screeplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf_ds.mda(n_factors=3)\ndf_ds.mda_summary\n\nINFO:pybiber.biber_analyzer:Dropping 2 variable(s) with max |r| &lt;= 0.20: ['InformationChangeNegative', 'ConfidenceLow']\n\n\n\nshape: (3, 6)\n\n\n\nFactor\nF\ndf\nPR(&gt;F)\nSignif\nR2\n\n\nstr\nf64\nlist[i64]\nf64\nstr\nf64\n\n\n\n\n\"factor_1\"\n66.570752\n[14, 455]\n0.0\n\"*** p &lt; 0.001\"\n0.671952\n\n\n\"factor_2\"\n10.974881\n[14, 455]\n0.0\n\"*** p &lt; 0.001\"\n0.252442\n\n\n\"factor_3\"\n19.913485\n[14, 455]\n0.0\n\"*** p &lt; 0.001\"\n0.379931\n\n\n\n\n\n\n\ndf_ds.mdaviz_groupmeans(factor=1, width=4, height=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n12.6.5 Compare Biber vs.Â DocuScope\nQuestion: Do similar dimensions emerge from grammatical vs.Â rhetorical features?\nPossible findings:\n\nConvergent dimensions: Both might reveal a â€œnarrative vs.Â expositoryâ€ dimension\n\nBiber: past tense, pronouns, contractions (+) vs.Â nominalizations, passives (-)\nDocuScope: Narrative, Character (+) vs.Â AcademicTerms, Reasoning (-)\nInterpretation: Robust register differenceâ€”emerges regardless of feature set\n\nDivergent dimensions: DocuScope might reveal a â€œconfident vs.Â hedgedâ€ stance dimension\n\nConfidenceHigh vs.Â ConfidenceLow, Uncertainty\nBiber features donâ€™t capture this as clearly (itâ€™s semantic, not grammatical)\nInterpretation: DocuScope adds rhetorical nuance beyond syntax\n\nComplementary dimensions: Biber might better capture â€œsyntactic complexityâ€\n\nSubordination, relative clauses, complement clauses\nDocuScope focuses on function, not structure\nInterpretation: Different taggers reveal different aspects of variation\n\n\nMethodological insight: If dimensions align across taggers, it suggests robust register variationâ€”the patterns arenâ€™t artifacts of one classification system. If they diverge, each tagger offers unique insights.\n\n\n\n\n\n\nComparative Research\n\n\n\nDejonge & Biber (2021) compared MDA using Biber features vs.Â DocuScope tags:\n\nSome dimensions converged (narrative, academic)\nDocuScope revealed additional rhetorical dimensions (confident reasoning, concrete description)\nConclusion: Different taggers complement rather than replace each other\n\nStudies in Corpus Linguistics, 109, 51-76.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Multi-Dimensional Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/multi-dimensional-analysis.html#principal-component-analysis-pca",
    "href": "tutorials/multi-dimensional-analysis.html#principal-component-analysis-pca",
    "title": "12Â  Multi-Dimensional Analysis",
    "section": "12.7 Principal Component Analysis (PCA)",
    "text": "12.7 Principal Component Analysis (PCA)\nPCA is a related dimension reduction technique, more common in machine learning and outside corpus linguistics.\n\n12.7.1 MDA vs.Â PCA: Theoretical Differences\n\n\n\n\n\n\n\n\nAspect\nMDA (Factor Analysis)\nPCA\n\n\n\n\nGoal\nFind latent factors causing observed variation\nFind variance-maximizing components\n\n\nVariance\nShared variance among variables\nTotal variance (including unique variance)\n\n\nAssumptions\nLatent factors cause observations\nNo causal assumptions\n\n\nInterpretation\nFactors have functional meanings\nComponents are mathematical constructs\n\n\nRotation\nVarimax/promax rotation for interpretability\nNo rotation (or optional)\n\n\nUse case\nInterpretive analysis (register, style)\nPredictive modeling, data compression\n\n\n\nFor text analysis:\n\nMDA preferred for interpretable dimensions (e.g., â€œinvolved vs.Â informationalâ€ has linguistic meaning)\nPCA preferred for data compression or predictive features (feeding into classification models)\n\nKey difference: MDA assumes latent factors (like â€œinvolved productionâ€) cause observed linguistic patterns (pronouns, present tense, contractions co-occur because they all serve involvement). PCA just identifies variance patterns without causal claims.\nResources:\n\nFactor analysis vs.Â PCA: https://towardsdatascience.com/factor-analysis-vs-pca-1c24a6bf2c1b\nIBM explanation: https://www.ibm.com/docs/en/spss-statistics/25.0.0?topic=detection-factor-analysis-versus-principal-components-analysis\n\n\n\n12.7.2 Running PCA\n\n# Run PCA on DocuScope data\ndf_ds.pca()\n\n\n\n12.7.3 Visualize PCA Results\n\n# Plot group means across PC1 and PC2\ndf_ds.pcaviz_groupmeans()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nPC1 (x-axis): First principal component (captures most variance)\nPC2 (y-axis): Second principal component (captures second-most variance)\nPercentages: Variance explained by each component (e.g., 18.3%, 11.8%)\n\nGenre clustering: Fiction genres (romance, mystery, adventure) cluster together on the left, while learned/government genres cluster on the right. This suggests PC1 captures a narrative vs.Â informational dimension similar to MDA Factor 1.\nPC2 separates religious texts (top) from government documents (bottom), possibly capturing a personal vs.Â impersonal or persuasive vs.Â procedural dimension.\n\n# Show variable contributions to PC1\ndf_ds.pcaviz_contrib(pc=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: Variables (DocuScope categories) with contributions above the mean threshold (red dashed line) are important for that component.\n\nPositive contributions (purple): Categories strongly associated with positive PC1 scores (e.g., AcademicTerms, InformationReportVerbs)\nNegative contributions (teal): Categories strongly associated with negative PC1 scores (e.g., Narrative, Character, Description)\n\nThis confirms PC1 as an informational vs.Â narrative dimension, aligning with MDA interpretations.\n\n\n\n\n\n\nWhen to Use PCA vs.Â MDA\n\n\n\nUse MDA when:\n\nPrimary goal is interpretation (understanding register differences functionally)\nYou want labeled dimensions (â€œinvolved vs.Â informational,â€ not â€œPC1â€)\nYouâ€™re working in corpus linguistics tradition (Biber, Halliday)\nYou need to explain findings to non-statisticians (functional labels are clearer)\n\nUse PCA when:\n\nPrimary goal is prediction or classification (feeding features into machine learning)\nYou need dimensionality reduction for computational efficiency\nYou want to preserve maximum variance with minimal assumptions\nYouâ€™re building unsupervised clustering or similarity models",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Multi-Dimensional Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/multi-dimensional-analysis.html#when-to-use-mda",
    "href": "tutorials/multi-dimensional-analysis.html#when-to-use-mda",
    "title": "12Â  Multi-Dimensional Analysis",
    "section": "12.8 When to Use MDA",
    "text": "12.8 When to Use MDA\n\n\n\n\n\n\nWell-Suited Research Questions\n\n\n\n\nRegister variation: Comparing spoken vs.Â written, formal vs.Â informal, academic vs.Â journalistic\nHistorical change: Tracking how scientific writing or legal language evolved over centuries\nCross-linguistic analysis: Testing if Biberâ€™s English dimensions generalize to Spanish, German, Chinese\nAuthorial style: Profiling individual authorsâ€™ linguistic signatures (e.g., does Austen use more â€œinvolvedâ€ features than Dickens?)\nGenre classification: Building interpretable models for automatic text categorization (what dimensions distinguish mystery from romance?)\nCorpus comparison: Do online forums differ from published writing in predictable ways?\n\n\n\n\n\n\n\n\n\nLimitations and Alternatives\n\n\n\nWhen MDA struggles:\n\nSmall corpora (&lt; 300 texts): Insufficient for stable factor extraction (need 5-10 observations per variable)\nHomogeneous texts: All one genre â†’ no variation to explain (factors require contrast)\nHighly creative language: Poetry, experimental fiction violate standard linguistic patterns\nNo tagged data: MDA requires linguistic annotation (POS tags, dependencies, or semantic tags)\nFine-grained semantics: MDA captures broad patterns, not subtle meaning differences (irony, metaphor)\n\nAlternative approaches:\n\nKeyness analysis: Simpler, works on smaller corpora, identifies distinctive features per genre (see Tutorial: Keyness)\nTopic modeling: Finds thematic rather than stylistic variation (see Tutorial: Topic Modeling)\nSupervised classification: If you only care about prediction, not interpretation (random forests, neural networks)\nSimple frequency comparisons: Sometimes a t-test or Mann-Whitney U test suffices (e.g., â€œDo academic texts use more passives than fiction?â€)\nCollocation analysis: Reveals local co-occurrence patterns (see Tutorial: Collocations)",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Multi-Dimensional Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/multi-dimensional-analysis.html#common-pitfalls",
    "href": "tutorials/multi-dimensional-analysis.html#common-pitfalls",
    "title": "12Â  Multi-Dimensional Analysis",
    "section": "12.9 Common Pitfalls",
    "text": "12.9 Common Pitfalls\n1. Over-extracting factors\nExtracting 10 factors from 500 texts produces unstable, uninterpretable dimensions. Stick to 3-7 for most corpora.\nWhy it fails: Too many factors = overfitting noise rather than capturing meaningful patterns.\n2. Forcing interpretations\nNot every factor has a clear functional interpretation. If you canâ€™t meaningfully label it after examining loadings and exemplar texts, consider reducing the number of factors.\nWarning sign: Labels like â€œFactor 3â€ or vague descriptions (â€œmiscellaneous linguistic featuresâ€).\n3. Ignoring negative loadings\nDimensions are bipolarâ€”both positive and negative poles matter. A factor isnâ€™t just â€œhigh nominalizations,â€ itâ€™s â€œnominalizations vs.Â pronounsâ€ (informational vs.Â involved).\nFix: Always report and interpret high loadings on both poles.\n4. Cherry-picking features\nReport all high-loading features (typically |loading| &gt; 0.40 or 0.50), not just the ones that fit your hypothesis.\nTransparency: Readers should see the full factor structure, including inconvenient features that complicate interpretation.\n5. Confusing correlation with causation\nMDA shows co-occurrence patterns, not causal relationships. â€œPassives correlate with nominalizationsâ€ â‰  â€œPassives cause nominalizations.â€\nCorrect framing: â€œPassives and nominalizations co-occur because they both serve informational density.â€\n6. Assuming universality\nDimensions vary across corpora, languages, and time periods. Biberâ€™s Dimension 1 might not emerge in your corpusâ€”thatâ€™s a finding, not a failure.\nExample: A corpus of 19th-century sermons might not show involved/informational variation (all texts are rhetorically involved), but might reveal a â€œdoctrinal vs.Â testimonialâ€ dimension unique to that genre.\n7. Neglecting corpus representativeness\nGarbage in, garbage out. If your â€œfictionâ€ category is all Stephen King novels, dimensions wonâ€™t generalize to all fiction.\nBest practice: Ensure balanced sampling across subcategories (multiple authors, publishers, time periods).",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Multi-Dimensional Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/multi-dimensional-analysis.html#what-to-do-after-mda",
    "href": "tutorials/multi-dimensional-analysis.html#what-to-do-after-mda",
    "title": "12Â  Multi-Dimensional Analysis",
    "section": "12.10 What to Do After MDA",
    "text": "12.10 What to Do After MDA\n\n12.10.1 Connect to Linguistic Theory\n\nDo extracted dimensions align with known register theory? (Biberâ€™s dimensions, Hallidayâ€™s functional grammar)\nDo they challenge existing assumptions about genre boundaries?\nWhat new distinctions emerge that previous research missed?\n\nExample: If a â€œconfident vs.Â hedgedâ€ dimension emerges in scientific writing, does it align with sociology of science literature on epistemic modality?\n\n\n12.10.2 Validate with Close Reading\n\nIdentify exemplar texts: Find texts scoring high/low on each dimension\nRead them: Does the dimension label fit? What nuances are missed by statistical patterns?\nRefine interpretations: Adjust labels based on actual linguistic content\n\nExample: Factor 2 loads high on past tense, third-person pronouns, and public verbs. You label it â€œnarrative,â€ but reading high-scoring texts reveals theyâ€™re historical exposition, not storytelling. Refine label to â€œhistorical recount.â€\n\n\n12.10.3 Compare Across Corpora\n\nApply the same MDA to a different corpus (different language, time period, domain)\nDo similar dimensions emerge? (suggests language-general patterns)\nWhatâ€™s corpus-specific vs.Â universal?\n\nExample: Run MDA on 18th-century and 21st-century academic writing. Does â€œinvolved vs.Â informationalâ€ still distinguish genres, or has academic writing become more conversational over time?\n\n\n12.10.4 Use for Classification\n\nTrain supervised models using dimension scores as features\nPredict genre membership for new texts\nTest how well dimensions discriminate categories (ANOVA, discriminant analysis)\n\nExample: Use 3 MDA dimensions to train a logistic regression model classifying texts as â€œfictionâ€ vs.Â â€œnon-fiction.â€ If accuracy &gt; 85%, dimensions are robust genre markers.\n\n\n12.10.5 Track Diachronic Change\n\nRun MDA on texts from different decades or centuries\nDo dimension scores shift over time? (e.g., is scientific writing becoming less formal?)\nAre certain registers becoming more/less distinct?\n\nExample: Apply MDA to New York Times articles from 1920, 1970, and 2020. Does Factor 1 (involved vs.Â informational) show journalism becoming more conversational?",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Multi-Dimensional Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/multi-dimensional-analysis.html#conclusion",
    "href": "tutorials/multi-dimensional-analysis.html#conclusion",
    "title": "12Â  Multi-Dimensional Analysis",
    "section": "12.11 Conclusion",
    "text": "12.11 Conclusion\nMDA is a powerful method for discovering and describing systematic linguistic variation. It transforms messy, correlated feature counts into interpretable dimensions that capture communicative functions.\nStrengths:\n\nReduces complexity: 67 features â†’ 3-5 interpretable dimensions\nReveals co-occurrence patterns: Shows which features cluster together and why\nProduces functional interpretations: Dimensions have linguistic meaning (â€œinvolved production,â€ not â€œPC1â€)\nEnables comparisons: Across registers, languages, time periods, authors\n\nLimitations:\n\nRequires large corpora: Minimum 300-500 texts for stable factors\nSubjective interpretation: Different researchers might label factors differently\nStatistical patterns â‰  explanations: Co-occurrence doesnâ€™t explain why features cluster (need linguistic theory)\n\nBest practices:\n\nStatistical rigor: Proper factor extraction, scree plot inspection, reporting all loadings\nLinguistic knowledge: Functional interpretation grounded in register theory (Biber, Halliday)\nQualitative validation: Close reading of exemplar texts to refine interpretations\nTransparency: Report all factors, all loadings, all decisions (donâ€™t cherry-pick)\nTriangulation: Combine MDA with other methods (keyness, topic modeling, close reading)\n\nThe best MDA workflows combine computational scale (analyzing hundreds of texts) + statistical rigor (proper factor analysis) + interpretive depth (functional linguistic interpretation) + methodological transparency (reporting everything).\n\n\n\n\n\n\nConnecting to Mini Lab 10\n\n\n\nMini Lab 10: Multi-Dimensional Analysis provides hands-on practice with the complete MDA workflow using both Biber and DocuScope features on the Brown Corpus. Youâ€™ll extract factors, interpret dimensions, visualize genre differences, and compare MDA to PCA.\nDiscussion questions in the mini lab ask you to:\n\nInterpret factor loadings functionally (what do high +/- features reveal?)\nJustify factor extraction decisions (scree plot trade-offs)\nCompare Biber vs.Â DocuScope dimensions (which better distinguishes genres?)\nAnalyze genre clustering patterns (do â€œlearnedâ€ vs.Â â€œpopularâ€ separate as predicted?)\nAssess stability across taggers (do similar dimensions emerge?)\nCompare MDA vs.Â PCA (which is more interpretable for humanities research?)",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Multi-Dimensional Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/multi-dimensional-analysis.html#see-also",
    "href": "tutorials/multi-dimensional-analysis.html#see-also",
    "title": "12Â  Multi-Dimensional Analysis",
    "section": "12.12 See Also",
    "text": "12.12 See Also\nFoundational Work:\n\nBiber, D. (1988). Variation across Speech and Writing. Cambridge University Press.\nBiber, D. (1995). Dimensions of Register Variation: A Cross-Linguistic Comparison. Cambridge University Press.\nBiber, D. (1992). The multi-dimensional approach to linguistic analyses of genre variation: An overview of methodology and findings. Computers and the Humanities, 26(5-6), 331-345.\n\nOnline Resources:\n\nBiberâ€™s Dimensions of English\npybiber Documentation\npybiber Feature Categories\nDocuScope Categories\nFactor Analysis vs.Â PCA\n\nRelated Tutorials:\n\nFrequency and Distributions: Single-feature analysis before MDA\nKeyness: Comparing registers via distinctive features (simpler alternative to MDA)\nTopic Modeling: Thematic vs.Â stylistic variation\nspaCy Basics: NLP processing pipeline for linguistic annotation\n\nRelated Mini Labs:\n\nMini Lab 3: Frequency distributions (building blocks for MDA)\nMini Lab 4: Keyness (comparing genres without factor analysis)\nMini Lab 8: spaCy processing (how Biber features are extracted)\n\nMethodological Comparisons:\n\nFriginal, E., & Hardy, J. A. (Eds.). (2013). Corpus Linguistics in Context. Routledge. (Chapters on MDA vs.Â other methods)\nDejonge, S., & Biber, D. (2021). Multidimensional analysis of DocuScope tags. Studies in Corpus Linguistics, 109, 51-76.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Multi-Dimensional Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/multi-dimensional-analysis.html#works-cited",
    "href": "tutorials/multi-dimensional-analysis.html#works-cited",
    "title": "12Â  Multi-Dimensional Analysis",
    "section": "12.13 Works Cited",
    "text": "12.13 Works Cited\nBiber, D. (1988). Variation across Speech and Writing. Cambridge University Press.\nBiber, D. (1992). The multi-dimensional approach to linguistic analyses of genre variation: An overview of methodology and findings. Computers and the Humanities, 26(5-6), 331-345.\nDejonge, S., & Biber, D. (2021). Multidimensional analysis of DocuScope tags. Studies in Corpus Linguistics, 109, 51-76.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>12</span>Â  <span class='chapter-title'>Multi-Dimensional Analysis</span>"
    ]
  },
  {
    "objectID": "tutorials/contextual-embeddings.html",
    "href": "tutorials/contextual-embeddings.html",
    "title": "13Â  Contextual Embeddings and Transformers",
    "section": "",
    "text": "13.1 Introduction\nWhen you use ChatGPT, Claude, or other AI language models, you might wonder: How does the AI â€œunderstandâ€ what Iâ€™m asking? The answer lies in contextual embeddingsâ€”the technology that allows modern language models to represent meaning as geometric relationships in high-dimensional space.\nThis tutorial introduces contextual embeddings, explains how they differ from earlier methods (like word2vec from Mini Lab 9), and demonstrates practical applications for humanities research: semantic search, clustering by meaning, and tracking how word meanings shift across contexts.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Contextual Embeddings and Transformers</span>"
    ]
  },
  {
    "objectID": "tutorials/contextual-embeddings.html#introduction",
    "href": "tutorials/contextual-embeddings.html#introduction",
    "title": "13Â  Contextual Embeddings and Transformers",
    "section": "",
    "text": "Computational Requirements\n\n\n\nRunning locally: This tutorial uses sentence-transformers, which downloads pre-trained models (~100-400 MB). First-time model download requires internet; subsequent runs are faster. The inaugural corpus examples run well on a standard laptop (8GB RAM).\nRunning on Colab: Click the â€œOpen in Colabâ€ badge above to run this tutorial in Google Colab with free GPU accessâ€”faster encoding and no local installation needed.\nCompanion lab: Mini Lab 11: Contextual Embeddings provides hands-on exercises with the same concepts.\n\n\n\n13.1.1 From Static to Contextual Representations\nStatic embeddings (word2vec, GloVe) assign each word a single vector representation:\n\nâ€œbankâ€ â†’ [0.45, -0.23, 0.89, ...] (always the same 300 numbers)\nWorks well for vocabulary analysis: finding synonyms, semantic fields, analogies\nLimitation: The word â€œbankâ€ gets the same vector whether it appears in â€œriver bankâ€ or â€œbank accountâ€\n\nContextual embeddings (BERT, RoBERTa, sentence-transformers) compute different vectors for the same word depending on its surrounding context:\n\nâ€œI deposited money at the bankâ€ â†’ [0.52, 0.18, -0.34, ...] (finance-related)\nâ€œWe sat by the river bankâ€ â†’ [-0.12, 0.67, 0.41, ...] (geography-related)\n\nThis is achieved through the attention mechanism, which allows the model to examine all words in a sentence simultaneously and adjust each wordâ€™s representation based on whatâ€™s around it.\n\n\n\n\n\n\nWhy â€œTransformersâ€?\n\n\n\nTransformer is the neural architecture that enables contextual embeddings. Introduced by Vaswani et al.Â (2017) in â€œAttention Is All You Need,â€ transformers process sequences in parallel rather than one word at a time.\nKey innovation: The self-attention mechanism lets each word â€œattend toâ€ every other word in the input, learning which words are most relevant for understanding it.\nExample: In â€œThe bank by the river was steep,â€ the model learns that â€œbankâ€ should pay high attention to â€œriverâ€ and â€œsteepâ€ (not â€œmoneyâ€ or â€œaccountâ€), producing a geography-specific representation.\n\n\n\n\n13.1.2 How This Powers Modern AI\nLarge Language Models (LLMs) like GPT-4, Claude, and Gemini are all transformer-based systems trained on massive text corpora:\nThe process:\n\nTokenization: Split input into tokens (roughly words or word pieces)\nEmbedding: Convert each token to an initial vector\nAttention layers: Multiple transformer layers refine representations by attending to context\nOutput: Contextual embeddings that capture meaning in this specific sentence\n\nWhat weâ€™re doing in this tutorial:\n\nUsing sentence-transformers, which are BERT models fine-tuned for creating high-quality sentence/document embeddings\nApplying these embeddings to research tasks: semantic similarity, search, clustering, word-sense analysis\nBuilding intuition about how AI represents language geometrically\n\n\n\n\n\n\n\nHugging Face: The AI Model Repository\n\n\n\nHugging Face (https://huggingface.co/) is the primary platform for sharing, discovering, and using AI models. Think of it as â€œGitHub for machine learning models.â€\nWhat it offers:\n\nModel Hub: 500,000+ pre-trained models (BERT, GPT, LLaMA, specialized domain models)\nDatasets: Thousands of text corpora for training/testing\nSpaces: Interactive demos to try models in your browser\nTransformers library: Python tools to download and use any model with a few lines of code\n\nFor humanities researchers: You can find models fine-tuned for specific tasks (sentiment analysis, named entity recognition, translation) or domains (historical texts, literary analysis, legal documents). All freely accessible and ready to use in Python.\nBeyond embeddings: While this lab focuses on document embeddings, Python provides powerful tools to interact with full LLMsâ€”even running smaller models (Llama 3, Mistral, Phi) locally on your computer using libraries like transformers, llama-cpp-python, or ollama. This means you can analyze sensitive texts privately, experiment without API costs, or fine-tune models on your specific corpus. We donâ€™t cover this here, but understanding embeddings is the foundation for working with any transformer-based model.\n\n\n\n\n\n\n\n\nWhy This Matters for Humanities Research\n\n\n\nBeyond keyword search: Traditional text analysis relies on exact word matches. If you search for â€œeconomic crisis,â€ you only find texts containing those wordsâ€”missing passages about â€œfinancial collapse,â€ â€œmarket downturn,â€ or â€œcommercial distress.â€\nSemantic understanding: Contextual embeddings represent meaning not vocabulary. Documents about economic crises cluster together in high-dimensional space even if they use completely different words.\nApplications:\n\nSemantic search: Find relevant passages across large archives without knowing exact terminology\n\nExample: Search thousands of 19th-century novels for â€œurban alienationâ€ even when authors write about â€œmetropolitan lonelinessâ€ or â€œcity estrangementâ€\n\nClustering by theme: Group texts by what theyâ€™re about, not which words they share\n\nExample: Discover that Civil War letters and Vietnam War letters cluster together despite different vocabulary, revealing transhistorical patterns in wartime communication\n\nHistorical semantics: Track how concepts like â€œfreedomâ€ or â€œdemocracyâ€ shift meaning across time\n\nExample: Visualize how â€œdemocracyâ€ meant different things to Founding Fathers (property-holding citizens) vs.Â Civil Rights movement (universal suffrage)\n\nCross-corpus comparison: Identify what different communities mean by the same term\n\nExample: Compare how â€œjusticeâ€ is discussed in legal documents, activist manifestos, and philosophical textsâ€”same word, different semantic neighborhoods\n\nAuthorship and influence: Detect which authors are semantically similar even when using different styles\n\nExample: Find that George Eliot and Henry James cluster together thematically (psychological realism) despite different sentence structures\n\nLLM literacy: Understanding embeddings helps you use AI tools critically and effectively\n\nExample: Recognize that ChatGPTâ€™s â€œknowledgeâ€ is really proximity in embedding spaceâ€”why it can answer some questions confidently and others with plausible-sounding nonsense\n\n\nThis is computational reasoning for the humanities: using geometry (distances in vector space) to discover semantic patterns that close reading might miss in large corpora. The goal isnâ€™t automationâ€”itâ€™s augmentation: finding patterns that generate new interpretive questions.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Contextual Embeddings and Transformers</span>"
    ]
  },
  {
    "objectID": "tutorials/contextual-embeddings.html#understanding-contextual-embeddings",
    "href": "tutorials/contextual-embeddings.html#understanding-contextual-embeddings",
    "title": "13Â  Contextual Embeddings and Transformers",
    "section": "13.2 Understanding Contextual Embeddings",
    "text": "13.2 Understanding Contextual Embeddings\n\n13.2.1 The Attention Mechanism\nThe key to contextual embeddings is self-attentionâ€”each wordâ€™s representation is influenced by every other word in the sentence.\nLetâ€™s demonstrate this with the word â€œpowerâ€ in different contexts:\n\n# Load model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n# Create sentences where \"power\" has different meanings\nsentences = [\n    \"The government has the power to pass new laws.\",  # political authority\n    \"The power plant generates electricity for the city.\",  # energy/electricity\n    \"Her speech had the power to move the audience.\",  # influence/effect\n    \"The king wielded absolute power over his subjects.\",  # political authority\n]\n\n# Get contextual embeddings\nembeddings = model.encode(sentences)\n\n# Compute similarities\nsimilarities = cosine_similarity(embeddings)\n\n# Display as DataFrame\nimport polars as pl\nsimilarity_df = pl.DataFrame(similarities, schema=[f\"S{i+1}\" for i in range(len(sentences))])\nprint(\"Cosine similarities between sentences:\\n\")\nprint(similarity_df)\n\nCosine similarities between sentences:\n\nshape: (4, 4)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ S1       â”† S2       â”† S3       â”† S4       â”‚\nâ”‚ ---      â”† ---      â”† ---      â”† ---      â”‚\nâ”‚ f32      â”† f32      â”† f32      â”† f32      â”‚\nâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ 1.0      â”† 0.224048 â”† 0.161138 â”† 0.292017 â”‚\nâ”‚ 0.224048 â”† 1.0      â”† 0.092147 â”† 0.066269 â”‚\nâ”‚ 0.161138 â”† 0.092147 â”† 1.0      â”† 0.336578 â”‚\nâ”‚ 0.292017 â”† 0.066269 â”† 0.336578 â”† 1.0      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nWhat this shows:\n\nS1-S4 (both political authority) have moderate similarity (0.29)â€”same sense but different contexts\nS3-S4 (rhetorical power â†”ï¸ political power) actually highest (0.34)â€”abstract power concepts overlap\nS2 (electricity) is quite distinct from political uses: S1-S2 = 0.22, S2-S4 = 0.07\nThe model distinguishes â€œpower plantâ€ from â€œgovernment powerâ€ based purely on surrounding words\n\nWhy this works: Human language interpretation works the same wayâ€”we understand â€œpowerâ€ by looking at surrounding words. Transformers formalize this intuition mathematically.\n\n\n13.2.2 From Words to Documents\nSentence-transformers extend this to document-level embeddings. Letâ€™s load the inaugural corpus and see this in action:\n\n# Load inaugural addresses\nurl = \"https://raw.githubusercontent.com/browndw/humanities_analytics/main/data/data_tables/inaugural.tsv\"\ninaugural = pl.read_csv(url, separator=\"\\t\")\n\nprint(f\"Loaded {len(inaugural)} presidential speeches\")\nprint(f\"Columns: {inaugural.columns}\\n\")\n\n# Show a few examples\ninaugural.select(['year', 'president']).head(5)\n\nLoaded 60 presidential speeches\nColumns: ['year', 'president', 'doc_id', 'text']\n\n\n\n\nshape: (5, 2)\n\n\n\nyear\npresident\n\n\ni64\nstr\n\n\n\n\n1789\n\"Washington\"\n\n\n1793\n\"Washington\"\n\n\n1797\n\"Adams\"\n\n\n1801\n\"Jefferson\"\n\n\n1805\n\"Jefferson\"\n\n\n\n\n\n\n\n# Encode all speeches (this takes ~30-60 seconds)\n# Suppressing output during rendering to save memory\nspeech_embeddings = model.encode(inaugural['text'].to_list(), show_progress_bar=False)\n\n\nprint(f\"Encoded {len(speech_embeddings)} speeches\")\nprint(f\"Each speech â†’ {speech_embeddings.shape[1]}-dimensional vector\")\n\nEncoded 60 speeches\nEach speech â†’ 384-dimensional vector\n\n\nNow we can:\n\nCompare documents: Cosine similarity between speech vectors\nSearch semantically: Find speeches most similar to a query concept\nCluster by meaning: Group speeches by thematic content\nTrack evolution: See how topics shift across time or communities\n\n\n\n\n\n\n\nDimensions and Model Size\n\n\n\nDifferent models produce different embedding dimensions:\n\nall-MiniLM-L6-v2: 384 dimensions (fast, lightweight, good quality)\nall-mpnet-base-v2: 768 dimensions (slower, higher quality)\nparaphrase-multilingual: 768 dimensions (works across languages)\n\nTradeoff: Higher dimensions capture more nuance but require more computation. For most humanities research, 384 dimensions provide excellent results.\nComputational cost: Encoding 1000 documents takes ~30-60 seconds on a laptop with all-MiniLM-L6-v2. This is dramatically faster than training your own word2vec model.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Contextual Embeddings and Transformers</span>"
    ]
  },
  {
    "objectID": "tutorials/contextual-embeddings.html#research-applications",
    "href": "tutorials/contextual-embeddings.html#research-applications",
    "title": "13Â  Contextual Embeddings and Transformers",
    "section": "13.3 Research Applications",
    "text": "13.3 Research Applications\n\n13.3.1 1. Semantic Search\nProblem: You want to find texts about â€œeconomic recovery after crisisâ€ in a historical corpus. Keyword search for â€œeconomic recoveryâ€ misses relevant passages using different vocabulary: â€œcommercial resurgence,â€ â€œfinancial rebound,â€ â€œrestoration of prosperity.â€\nSolution: Encode your query and all documents, then find highest cosine similarities.\nLetâ€™s search for speeches about economic crisis:\n\ndef semantic_search(query, top_k=5):\n    \"\"\"Find speeches most semantically similar to a query.\"\"\"\n    # Encode the query\n    query_embedding = model.encode([query])[0].reshape(1, -1)\n    \n    # Compute similarities\n    similarities = cosine_similarity(query_embedding, speech_embeddings)[0]\n    \n    # Get top results\n    results = inaugural.with_columns([\n        pl.Series(\"similarity\", similarities)\n    ]).sort(\"similarity\", descending=True).head(top_k)\n    \n    return results.select(['year', 'president', 'similarity'])\n\n# Try a search\nprint(\"Query: 'economic crisis and national recovery'\\n\")\nsemantic_search(\"economic crisis and national recovery\", top_k=5)\n\nQuery: 'economic crisis and national recovery'\n\n\n\n\nshape: (5, 3)\n\n\n\nyear\npresident\nsimilarity\n\n\ni64\nstr\nf32\n\n\n\n\n1901\n\"McKinley\"\n0.440758\n\n\n1933\n\"Roosevelt\"\n0.381424\n\n\n1925\n\"Coolidge\"\n0.351388\n\n\n1937\n\"Roosevelt\"\n0.317839\n\n\n2017\n\"Trump\"\n0.31327\n\n\n\n\n\n\n\n# Try another query\nprint(\"Query: 'war and national defense'\\n\")\nsemantic_search(\"war and national defense\", top_k=5)\n\nQuery: 'war and national defense'\n\n\n\n\nshape: (5, 3)\n\n\n\nyear\npresident\nsimilarity\n\n\ni64\nstr\nf32\n\n\n\n\n1973\n\"Nixon\"\n0.384878\n\n\n1925\n\"Coolidge\"\n0.383298\n\n\n1865\n\"Lincoln\"\n0.382346\n\n\n2005\n\"Bush\"\n0.374248\n\n\n1993\n\"Clinton\"\n0.320786\n\n\n\n\n\n\nWhy it works: The model learns that â€œrecovery,â€ â€œrebound,â€ and â€œresurgenceâ€ occupy nearby regions in vector space because they appear in similar contexts. Your query vector sits in this semantic neighborhood, capturing conceptually related texts regardless of vocabulary.\nResearch use cases:\n\nFinding relevant passages in massive archives (millions of documents)\nCross-linguistic search (with multilingual models)\nDiscovering unexpected connections across discourses\nBuilding custom search engines for specialized corpora\n\n\n\n\n\n\n\nSemantic Search vs.Â Topic Modeling\n\n\n\nTopic modeling (LDA, NMF) finds word co-occurrence patterns: documents sharing words like {economy, growth, market, trade} form a topic.\nSemantic search finds meaning relationships: documents about economic themes using entirely different vocabularies (19th-century â€œcommercial prosperityâ€ vs.Â modern â€œGDP growthâ€).\nWhen to use which:\n\nTopic modeling: Discover latent themes, understand vocabulary structure\nSemantic search: Find specific concepts, cross-vocabulary retrieval\nBoth together: Topic modeling reveals corpus structure, semantic search finds examples\n\n\n\n\n\n13.3.2 2. Clustering by Semantic Content\nProblem: You have 60 presidential speeches spanning 236 years. Do they cluster by historical era (19th vs.Â 20th century) or by theme (wartime, economic crisis, unity)?\nSolution: Use embeddings as features for clustering algorithms.\n\nfrom sklearn.cluster import KMeans\n\n# Cluster speeches into 4 groups\nn_clusters = 4\nkmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\nclusters = kmeans.fit_predict(speech_embeddings)\n\n# Add cluster labels to dataframe\ninaugural_clustered = inaugural.with_columns([\n    pl.Series(\"cluster\", clusters)\n])\n\n# Show distribution\nprint(\"Speeches per cluster:\\n\")\nprint(inaugural_clustered.group_by('cluster').agg(pl.count()).sort('cluster'))\n\nSpeeches per cluster:\n\nshape: (4, 2)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ cluster â”† count â”‚\nâ”‚ ---     â”† ---   â”‚\nâ”‚ i32     â”† u32   â”‚\nâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\nâ”‚ 0       â”† 9     â”‚\nâ”‚ 1       â”† 24    â”‚\nâ”‚ 2       â”† 10    â”‚\nâ”‚ 3       â”† 17    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n\n# Show samples from each cluster\nprint(\"\\nSample speeches from each cluster:\\n\")\nprint(\"=\"*60)\n\nfor cluster_id in range(n_clusters):\n    print(f\"\\nCluster {cluster_id}:\")\n    sample = inaugural_clustered.filter(pl.col('cluster') == cluster_id).sample(n=min(3, len(inaugural_clustered.filter(pl.col('cluster') == cluster_id))))\n    for row in sample.to_dicts():\n        print(f\"  {row['year']:4d} {row['president']}\")\n\n\nSample speeches from each cluster:\n\n============================================================\n\nCluster 0:\n  1977 Carter\n  1997 Clinton\n  1921 Harding\n\nCluster 1:\n  1953 Eisenhower\n  1813 Madison\n  1805 Jefferson\n\nCluster 2:\n  1917 Wilson\n  1821 Monroe\n  1933 Roosevelt\n\nCluster 3:\n  1989 Bush\n  2005 Bush\n  1857 Buchanan\n\n\nWhat you find: Examine the clusters. Do they correspond to historical periods, or do they cut across time based on themes? This reveals how rhetorical patterns persist or change.\nInsight: Speeches often cluster by communicative function not chronology. A 1941 wartime speech may resemble a 2001 wartime speech more than a 1945 peace speech.\nResearch implications:\n\nChallenges periodization assumptions (historical era â‰  rhetorical mode)\nReveals transhistorical patterns (certain situations evoke similar language)\nComplements stylistic analysis (MDA finds how texts differ, embeddings find what theyâ€™re about)\n\n\n\n\n\n\n\nInterpreting Clusters\n\n\n\nClustering results depend on:\n\nNumber of clusters (try 3-7 for interpretability)\nClustering algorithm (K-means, hierarchical, DBSCAN)\nPreprocessing choices (which texts to include)\n\nBest practice: Treat clusters as exploratory hypotheses, not objective categories. Examine sample texts, validate with close reading, test alternative cluster numbers.\n\n\n\n\n13.3.3 3. Document Similarity\nLetâ€™s find which speeches are most similar to a specific speech:\n\n# Find speeches similar to most recent\ntarget_idx = len(inaugural) - 1\ntarget_speech = inaugural[target_idx]\n\nprint(f\"Finding speeches similar to: {target_speech['year'][0]} - {target_speech['president'][0]}\\n\")\n\n# Compute similarities\ntarget_embedding = speech_embeddings[target_idx].reshape(1, -1)\nsimilarities_to_target = cosine_similarity(target_embedding, speech_embeddings)[0]\n\n# Create results dataframe\nresults = inaugural.with_columns([\n    pl.Series(\"similarity\", similarities_to_target)\n]).sort(\"similarity\", descending=True)\n\nprint(\"Most similar speeches:\\n\")\nresults.select(['year', 'president', 'similarity']).head(10)\n\nFinding speeches similar to: 2025 - Trump\n\nMost similar speeches:\n\n\n\n\nshape: (10, 3)\n\n\n\nyear\npresident\nsimilarity\n\n\ni64\nstr\nf32\n\n\n\n\n2025\n\"Trump\"\n1.0\n\n\n2021\n\"Biden\"\n0.740231\n\n\n1949\n\"Truman\"\n0.724837\n\n\n2009\n\"Obama\"\n0.66023\n\n\n2017\n\"Trump\"\n0.65667\n\n\n1933\n\"Roosevelt\"\n0.650834\n\n\n1981\n\"Reagan\"\n0.649962\n\n\n1857\n\"Buchanan\"\n0.64124\n\n\n1957\n\"Eisenhower\"\n0.628518\n\n\n2013\n\"Obama\"\n0.617471\n\n\n\n\n\n\nWhat this reveals: The most similar speeches may come from completely different eras, showing that thematic content transcends historical period.\nHistorical semantics applications:\n\nTrack how â€œdemocracy,â€ â€œliberty,â€ â€œrightsâ€ shift across constitutional debates\nCompare how different social movements invoke the same concepts\n\nIdentify when semantic change occurs (gradual drift vs.Â sudden shift)\nValidate conceptual history arguments computationally\n\n\n\n\n\n\n\nComputational Historical Semantics\n\n\n\nTraditional historical semantics relies on:\n\nClose reading of key texts\nEtymological dictionaries\nExpert knowledge of historical context\n\nComputational approach (word-in-context embeddings) adds:\n\nScale: Analyze thousands of uses automatically\nObjectivity: Find patterns without pre-existing hypotheses\nVisualization: See semantic space geometrically\nComparison: Measure similarity quantitatively\n\nBest results: Combine both. Use embeddings to find patterns at scale, then validate with close reading and historical expertise.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Contextual Embeddings and Transformers</span>"
    ]
  },
  {
    "objectID": "tutorials/contextual-embeddings.html#methodological-considerations",
    "href": "tutorials/contextual-embeddings.html#methodological-considerations",
    "title": "13Â  Contextual Embeddings and Transformers",
    "section": "13.4 Methodological Considerations",
    "text": "13.4 Methodological Considerations\n\n13.4.1 When to Use Contextual vs.Â Static Embeddings\nContextual embeddings (BERT, sentence-transformers) excel at:\n\nDocument similarity and semantic search\nTasks requiring context-dependent meaning (word sense disambiguation)\nComparing texts across vocabularies (different time periods, discourses)\nBuilding on pre-trained models (no need to train on your corpus)\n\nStatic embeddings (word2vec, GloVe) are preferable for:\n\nVocabulary analysis (semantic fields, synonym detection)\nHistorical word embedding models (training on period-specific corpora)\nComputational efficiency (smaller models, faster inference)\nInterpretability (single vector per word is easier to inspect)\n\nWhen to use both:\n\nWord2vec to map semantic fields â†’ sentence-transformers to find documents expressing those concepts\nStatic embeddings for diachronic analysis (train on decade-specific corpora) â†’ contextual for synchronic variation (how is â€œfreedomâ€ used differently in the same period)\n\n\n\n13.4.2 Limitations and Biases\nPre-training data matters: Sentence-transformers are trained on massive web corpora. This means:\n\nBias: Models inherit biases from training data (gender, race, political assumptions)\nDomain mismatch: Web text â‰  historical documents. Models may not understand archaic vocabulary or specialized discourse\nAnglophone focus: Most models are English-centric. Multilingual models exist but have lower quality\n\nExample: A model trained on 21st-century web text might misunderstand 18th-century uses of â€œvirtueâ€ (which had specific political connotations in republican discourse).\n\n\n\n\n\n\nCritical Use of Pre-trained Models\n\n\n\nQuestion to always ask: Is the modelâ€™s training data appropriate for my research questions?\nStrategies:\n\nTest model performance on sample texts from your corpus\nCompare results to expert knowledge or close reading\nConsider domain-specific models (e.g., BiomedBERT for medical texts)\nFine-tune on your corpus if you have labeled data\nAcknowledge limitations in research write-ups\n\nRemember: Embeddings find patterns in how the model was trained to represent language, not objective semantic truth. Treat them as powerful exploratory tools, not oracles.\n\n\n\n\n13.4.3 Interpreting Similarity Scores\nCosine similarity ranges from -1 to 1 (in practice, usually 0 to 1 for sentence embeddings):\n\n0.9-1.0: Nearly identical content (duplicates, paraphrases)\n0.7-0.9: High semantic overlap (same topic, similar argument)\n0.5-0.7: Moderate thematic relationship (related but distinct)\n0.3-0.5: Weak connection (tangentially related)\n&lt;0.3: Unrelated or opposite\n\nContext matters: What counts as â€œsimilarâ€ depends on your research question:\n\nComparing different editions of the same book: expect &gt;0.9\nFinding thematically related articles: 0.6-0.8 is strong\nCross-genre comparison: 0.4-0.6 might indicate meaningful connection\nHistorical comparison: 0.5 between 18th and 21st-century texts on â€œlibertyâ€ might be surprisingly high (semantic stability) or surprisingly low (conceptual shift)\n\nCalibration: Always examine sample pairs at different similarity levels to understand what scores mean in your corpus.\nHumanities perspective: Unlike quantitative sciences where statistical significance determines validity, humanities research values interpretive significance. A similarity score of 0.4 might reveal a fascinating connection worth exploring through close readingâ€”the number is a prompt for interpretation, not a verdict.\n\n\n13.4.4 Computational Costs and Scalability\nEncoding time (all-MiniLM-L6-v2 on a laptop):\n\n100 documents: ~5 seconds\n1,000 documents: ~30 seconds\n10,000 documents: ~5 minutes\n100,000 documents: ~50 minutes\n\nMemory requirements:\n\n10,000 documents Ã— 384 dimensions Ã— 4 bytes/float = ~15 MB\n100,000 documents = ~150 MB\n1 million documents = ~1.5 GB\n\nPractical implications:\n\nEncode once, save embeddings, reuse for multiple analyses\nUse batch encoding for efficiency\nFor truly massive corpora (millions of texts), consider approximate nearest neighbor libraries (FAISS, Annoy)",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Contextual Embeddings and Transformers</span>"
    ]
  },
  {
    "objectID": "tutorials/contextual-embeddings.html#connecting-to-other-methods",
    "href": "tutorials/contextual-embeddings.html#connecting-to-other-methods",
    "title": "13Â  Contextual Embeddings and Transformers",
    "section": "13.5 Connecting to Other Methods",
    "text": "13.5 Connecting to Other Methods\n\n13.5.1 Embeddings + Multi-Dimensional Analysis (MDA)\nMDA (Mini Lab 10) clusters texts by stylistic features: pronouns, passives, nominalizations, sentence length. It answers: How do texts differ in style?\nContextual embeddings cluster by semantic content: themes, topics, arguments. They answer: What are texts about?\nCombining both:\n\nUse embeddings to identify thematically similar texts\nApply MDA to see if they use similar or different styles\nExample finding: 19th and 21st century inaugural speeches about unity might share themes (high embedding similarity) but differ stylistically (different MDA profiles)\n\nInsight: Separates what is said from how itâ€™s saidâ€”crucial for understanding rhetorical choices.\n\n\n13.5.2 Embeddings + Topic Modeling\nTopic modeling (Mini Lab 9) discovers word co-occurrence patterns (latent topics).\nContextual embeddings find semantic relationships regardless of shared vocabulary.\nComplementary uses:\n\nTopic modeling: â€œWhat topics structure this corpus?â€\nEmbeddings: â€œFor each topic, find examples using different vocabulariesâ€\nExample: Topic modeling finds {economy, growth, market, trade} topic. Embeddings find 19th-century speeches expressing economic themes with entirely different words {commerce, prosperity, manufacture}.\n\n\n\n13.5.3 Embeddings + Keyness Analysis\nKeyness identifies words that distinguish one corpus from another (statistically overrepresented).\nEmbeddings identify texts that are semantically similar despite different vocabulary.\nWorkflow:\n\nUse keyness to find distinctive vocabulary (what makes Group A different from Group B)\nUse embeddings to find if Group A texts are semantically coherent (do they share themes?) or heterogeneous\nExample: Keyness shows female authors overuse â€œfelt,â€ â€œseemed,â€ â€œthought.â€ Embeddings reveal these words appear in narratives focused on interiority (not just random distribution).",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Contextual Embeddings and Transformers</span>"
    ]
  },
  {
    "objectID": "tutorials/contextual-embeddings.html#ethical-and-critical-considerations",
    "href": "tutorials/contextual-embeddings.html#ethical-and-critical-considerations",
    "title": "13Â  Contextual Embeddings and Transformers",
    "section": "13.6 Ethical and Critical Considerations",
    "text": "13.6 Ethical and Critical Considerations\n\n13.6.1 Understanding Model Biases\nTransformer models are trained on massive web corpora (Reddit, Wikipedia, news sites, books). This means they encode the biases present in that data:\nGender bias: A model might embed â€œdoctorâ€ closer to â€œheâ€ than â€œsheâ€ because of training data patterns, not reality.\nCultural bias: Concepts like â€œfamily,â€ â€œfreedom,â€ or â€œjusticeâ€ are embedded based on dominant cultural perspectives in the training data (predominantly English-language, Western sources).\nHistorical anachronism: Models trained on 21st-century text may misrepresent 18th-century concepts (â€œvirtueâ€ meant something specific in republican discourse that modern embeddings might miss).\nFor humanities researchers:\n\nAlways ask: Whose language is this model encoding? Not universal truth, but patterns from specific corpora.\nValidate computational findings with domain expertise and close reading\nConsider whether your research questions require domain-specific models or careful interpretation of general-purpose models\nAcknowledge limitations in your write-ups: â€œThese embeddings capture how concepts relate in contemporary web text, not necessarily in historical discourseâ€\n\n\n\n13.6.2 Privacy and Data Sovereignty\nWhen working with sensitive texts:\n\nCloud APIs (OpenAI, Anthropic) send your texts to their serversâ€”inappropriate for unpublished manuscripts, confidential archives, or culturally sensitive materials\nLocal models (via Hugging Face transformers or Ollama) process everything on your computerâ€”maintains privacy and data sovereignty\nConsiderations: Indigenous texts, unpublished correspondence, medical records, legal documentsâ€”all may require local processing\n\n\n\n13.6.3 Interpretive Authority\nEmbeddings are tools for discovery, not arguments in themselves:\n\nA clustering result is a pattern worth investigating, not proof of a categorical distinction\nSimilarity scores suggest connections to explore through close reading\nComputational patterns complement humanistic interpretationâ€”they donâ€™t replace it\n\nBest practice: Frame results as generative (â€œthis pattern raises questions aboutâ€¦â€) rather than conclusive (â€œthis proves thatâ€¦â€).",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Contextual Embeddings and Transformers</span>"
    ]
  },
  {
    "objectID": "tutorials/contextual-embeddings.html#practical-workflow",
    "href": "tutorials/contextual-embeddings.html#practical-workflow",
    "title": "13Â  Contextual Embeddings and Transformers",
    "section": "13.7 Practical Workflow",
    "text": "13.7 Practical Workflow\n\n13.7.1 Step 1: Choose a Model\nBrowse available models at Sentence Transformers or the Hugging Face Model Hub:\nGeneral-purpose (start here for most humanities research):\n\nall-MiniLM-L6-v2: 384 dimensions, fast, good for most tasks\n\nBest for: Getting started, exploratory analysis, large corpora (10,000+ documents)\n\nall-mpnet-base-v2: 768 dimensions, higher quality, slower\n\nBest for: Final analysis, smaller corpora where quality matters more than speed\n\n\nSpecialized (use when you have specific needs):\n\nparaphrase-multilingual-MiniLM-L12-v2: Cross-lingual embeddings\n\nBest for: Comparing French novels to English translations, multilingual archives\n\nallenai-specter: Scientific paper embeddings\n\nBest for: History of science, analyzing academic discourse\n\nmsmarco-distilbert-base-v4: Optimized for semantic search\n\nBest for: Building search engines for large text collections\n\n\nFor historical texts: Be aware that models trained on modern web text may not capture historical vocabulary well. Consider fine-tuning (advanced) or accept that results require more validation.\nfrom sentence_transformers import SentenceTransformer\n\n# Load model (downloads automatically first time from Hugging Face)\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\n\n\n13.7.2 Step 2: Encode Your Corpus\n# Prepare texts as list\ntexts = df['text'].to_list()\n\n# Encode (shows progress bar)\nembeddings = model.encode(texts, show_progress_bar=True)\n\n# Save for reuse (avoid re-encoding)\nimport numpy as np\nnp.save('corpus_embeddings.npy', embeddings)\n\n# Later, load without re-encoding\n# embeddings = np.load('corpus_embeddings.npy')\nWhy save embeddings: Encoding is the slow step (~30-60 seconds for 1000 documents). Once encoded, you can run dozens of analyses (searches, clusterings, comparisons) in seconds by loading the saved embeddings.\n\n\n\n\n\n\nVisualization Considerations\n\n\n\nWhen reducing 384-dimensional embeddings to 2D for visualization (using UMAP or t-SNE):\n\nInformation loss: 2D projections necessarily lose informationâ€”clusters that appear close in 2D might be far apart in 384D\nParameter sensitivity: UMAPâ€™s n_neighbors and min_dist affect layout significantly\nInterpretation: Use visualizations for exploration (â€œwhat patterns exist?â€), validate with quantitative analysis (actual cosine similarities)\nAlternative: For rigorous analysis, work in full 384D space and use metrics like silhouette score to evaluate clusters\n\nVisualizations are powerful for communication and hypothesis generation, but always verify patterns quantitatively.\n\n\n\n\n13.7.3 Step 3: Analyze\nSemantic search:\nquery_embedding = model.encode([\"your search query\"])[0]\nsimilarities = cosine_similarity(\n    query_embedding.reshape(1, -1), \n    embeddings\n)[0]\ntop_matches = np.argsort(similarities)[::-1][:10]\nClustering:\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=5, random_state=42)\nclusters = kmeans.fit_predict(embeddings)\nVisualization:\nfrom umap import UMAP\nimport matplotlib.pyplot as plt\n\nreducer = UMAP(n_components=2, random_state=42)\nembeddings_2d = reducer.fit_transform(embeddings)\n\nplt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n           c=clusters, cmap='viridis', alpha=0.6)\nplt.show()\n\n\n13.7.4 Step 4: Interpret and Validate\n\nExamine cluster samples: Read representative texts from each cluster\nTest edge cases: Look at low-similarity pairsâ€”why arenâ€™t they similar?\nCompare to metadata: Do clusters align with genres, time periods, authors?\nValidate with close reading: Do computational patterns hold up under careful reading?\n\n\n\n\n\n\n\nIterative Workflow\n\n\n\nEmbedding analysis is rarely one-and-done:\n\nExplore: Run initial clustering, examine results\nRefine: Adjust preprocessing (filter short texts, combine chunks)\nTest: Try different cluster numbers, alternative models\nValidate: Compare to existing categories, close reading\nInterpret: Build argument connecting patterns to research questions\n\nExpect surprises: The best insights often come from unexpected patterns that challenge your initial hypotheses.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Contextual Embeddings and Transformers</span>"
    ]
  },
  {
    "objectID": "tutorials/contextual-embeddings.html#see-also",
    "href": "tutorials/contextual-embeddings.html#see-also",
    "title": "13Â  Contextual Embeddings and Transformers",
    "section": "13.8 See Also",
    "text": "13.8 See Also\n\n13.8.1 Foundational Papers\n\nVaswani et al.Â (2017): â€œAttention Is All You Needâ€ (introduced transformers)\nDevlin et al.Â (2019): â€œBERT: Pre-training of Deep Bidirectional Transformersâ€ (BERT architecture)\nReimers & Gurevych (2019): â€œSentence-BERT: Sentence Embeddings using Siamese BERT-Networksâ€ (sentence-transformers)\n\n\n\n13.8.2 Online Resources\n\nSentence Transformers Documentation: https://www.sbert.net/\nHugging Face: https://huggingface.co/ (the primary platform for AI models and datasets)\n\nModel Hub: https://huggingface.co/models (browse 500,000+ models including specialized humanities models)\nDatasets: https://huggingface.co/datasets (thousands of text corpora)\nTransformers Documentation: https://huggingface.co/docs/transformers (library for using any model)\n\nJay Alammarâ€™s Illustrated Transformer: https://jalammar.github.io/illustrated-transformer/ (visual explanation)\nUMAP Documentation: https://umap-learn.readthedocs.io/ (dimensionality reduction)\nOllama: https://ollama.com/ (run LLMs locally on your computerâ€”simple interface for Llama, Mistral, etc.)\n\n\n\n13.8.3 Related Tutorials\n\nMini Lab 11: Contextual Embeddings (hands-on exercises with inaugural corpusâ€”start here for interactive learning)\nMini Lab 9: Vector Models (word2vec, static embeddings)\nMini Lab 10: Multi-Dimensional Analysis (stylistic dimensions)\nMini Lab 8: spaCy Basics (NLP preprocessing for embeddings)\n\nRelationship to Mini Lab 11: This tutorial provides conceptual depth, methodological guidance, and connections to other methods. Mini Lab 11 offers hands-on exercises with code you can run and modify. Use both together: read this tutorial for understanding, then work through the lab for practice.\n\n\n13.8.4 Methodological Readings\n\nUnderwood (2015): â€œThe Historical Significance of Textual Distancesâ€ (applying embeddings to literary history)\nSchmidt (2015): â€œVector Space Models for the Digital Humanitiesâ€ (overview of embedding methods)\nGarg et al.Â (2018): â€œWord Embeddings Quantify 100 Years of Gender and Ethnic Stereotypesâ€ (bias in embeddings)\nHamilton et al.Â (2016): â€œDiachronic Word Embeddings Reveal Statistical Laws of Semantic Changeâ€ (tracking meaning shifts)",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Contextual Embeddings and Transformers</span>"
    ]
  },
  {
    "objectID": "tutorials/contextual-embeddings.html#summary",
    "href": "tutorials/contextual-embeddings.html#summary",
    "title": "13Â  Contextual Embeddings and Transformers",
    "section": "13.9 Summary",
    "text": "13.9 Summary\nContextual embeddings represent the foundation of modern AI language understanding. By computing context-specific vector representations, they enable:\n\nSemantic search: Find meaning, not keywords\nThematic clustering: Group by content, not vocabulary\nWord-sense analysis: Track how concepts shift across contexts\nLLM literacy: Understand how ChatGPT â€œknowsâ€ language\n\nKey insights:\n\nContext matters: Same word in different contexts gets different representations\nGeometry is meaning: Semantic relationships = distances in high-dimensional space\n\nPre-training enables transfer: Models trained on web text work (mostly) on historical/literary corpora\nAlways validate: Computational patterns are hypotheses, not facts\n\nNext steps: Apply these methods to your own research questions. What semantic patterns exist in your corpus that close reading might miss at scale? How do concepts evolve across your texts? What unexpected connections emerge from semantic clustering?\nThe goal isnâ€™t to replace humanistic interpretationâ€”itâ€™s to discover patterns that inspire new interpretative questions.",
    "crumbs": [
      "Advanced Analysis",
      "<span class='chapter-number'>13</span>Â  <span class='chapter-title'>Contextual Embeddings and Transformers</span>"
    ]
  },
  {
    "objectID": "tutorials/classification-federalist-papers.html",
    "href": "tutorials/classification-federalist-papers.html",
    "title": "14Â  Classification & Authorship Attribution",
    "section": "",
    "text": "14.1 Introduction to Classification\nClassification is a supervised machine learning task where we train a model to predict categorical labels (classes) based on features. In text analysis, common classification tasks include:\nKey concepts:\nimport polars as pl\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Set style\nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10, 6)",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>Classification & Authorship Attribution</span>"
    ]
  },
  {
    "objectID": "tutorials/classification-federalist-papers.html#introduction-to-classification",
    "href": "tutorials/classification-federalist-papers.html#introduction-to-classification",
    "title": "14Â  Classification & Authorship Attribution",
    "section": "",
    "text": "Authorship attribution: Who wrote this text?\nSentiment analysis: Is this review positive or negative?\nGenre classification: Is this a novel, poem, or essay?\nTopic classification: What subject does this article discuss?\n\n\n\nFeatures: Measurable properties (e.g., word frequencies)\nLabels: Categories we want to predict (e.g., author names)\nTraining data: Labeled examples used to build the model\nValidation data: Held-out data to test model performance\nTest data: New data where labels are unknown (what we want to predict)",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>Classification & Authorship Attribution</span>"
    ]
  },
  {
    "objectID": "tutorials/classification-federalist-papers.html#mosteller-wallaces-variables",
    "href": "tutorials/classification-federalist-papers.html#mosteller-wallaces-variables",
    "title": "14Â  Classification & Authorship Attribution",
    "section": "14.2 Mosteller & Wallaceâ€™s Variables",
    "text": "14.2 Mosteller & Wallaceâ€™s Variables\nBecause of computational limits in the 1960s, Mosteller & Wallace needed to identify potentially productive variables ahead of building their model. This is not how we would approach it now, but it was a constraint at the time. They created 3 groups of candidate words.\nTheir first group contains 70 common function words:\n\nmw_group1 = [\n    \"a\", \"all\", \"also\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\",\n    \"been\", \"but\", \"by\", \"can\", \"do\", \"down\", \"even\", \"every\", \"for\",\n    \"from\", \"had\", \"has\", \"have\", \"her\", \"his\", \"if\", \"in\", \"into\",\n    \"is\", \"it\", \"its\", \"may\", \"more\", \"must\", \"my\", \"no\", \"not\",\n    \"now\", \"of\", \"on\", \"one\", \"only\", \"or\", \"our\", \"shall\", \"should\",\n    \"so\", \"some\", \"such\", \"than\", \"that\", \"the\", \"their\", \"then\",\n    \"there\", \"things\", \"this\", \"to\", \"up\", \"upon\", \"was\", \"were\",\n    \"what\", \"when\", \"which\", \"who\", \"will\", \"with\", \"would\", \"your\"\n]\n\nTheir second group adds 47 more words:\n\nmw_group2 = [\n    \"affect\", \"again\", \"although\", \"among\", \"another\", \"because\",\n    \"between\", \"both\", \"city\", \"commonly\", \"consequently\",\n    \"considerable\", \"contribute\", \"defensive\", \"destruction\", \"did\",\n    \"direction\", \"disgracing\", \"either\", \"enough\", \"fortune\",\n    \"function\", \"himself\", \"innovation\", \"join\", \"language\", \"most\",\n    \"nor\", \"offensive\", \"often\", \"pass\", \"perhaps\", \"rapid\", \"same\",\n    \"second\", \"still\", \"those\", \"throughout\", \"under\", \"vigor\",\n    \"violate\", \"violence\", \"voice\", \"where\", \"whether\", \"while\",\n    \"whilst\"\n]\n\nTheir third group includes 48 more content-bearing words (some originally lemmatized):\n\nmw_group3 = [\n    \"about\", \"according\", \"adversaries\", \"after\", \"aid\", \"always\",\n    \"apt\", \"asserted\", \"before\", \"being\", \"better\", \"care\", \"choice\",\n    \"common\", \"danger\", \"decide\", \"decides\", \"decided\", \"deciding\",\n    \"degree\", \"during\", \"expense\", \"expenses\", \"extent\", \"follow\",\n    \"follows\", \"followed\", \"following\", \"i\", \"imagine\", \"imagined\",\n    \"intrust\", \"intrusted\", \"intrusting\", \"kind\", \"large\", \"likely\",\n    \"matter\", \"matters\", \"moreover\", \"necessary\", \"necessity\",\n    \"necessities\", \"others\", \"particularly\", \"principle\",\n    \"probability\", \"proper\", \"propriety\", \"provision\", \"provisions\",\n    \"requisite\", \"substance\", \"they\", \"though\", \"truth\", \"truths\",\n    \"us\", \"usage\", \"usages\", \"we\", \"work\", \"works\"\n]\n\nAll together: 165 candidate variables (180 unlemmatized tokens).\n\n# Combine all groups\nmw_all = sorted(mw_group1 + mw_group2 + mw_group3)\nprint(f\"Total variables: {len(mw_all)}\")\n\nTotal variables: 180",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>Classification & Authorship Attribution</span>"
    ]
  },
  {
    "objectID": "tutorials/classification-federalist-papers.html#the-federalist-papers",
    "href": "tutorials/classification-federalist-papers.html#the-federalist-papers",
    "title": "14Â  Classification & Authorship Attribution",
    "section": "14.3 The Federalist Papers",
    "text": "14.3 The Federalist Papers\nThe Federalist Papers are 85 articles written by Alexander Hamilton, James Madison, and John Jay under the pseudonym â€œPubliusâ€ to promote ratification of the U.S. Constitution.\nAuthorship breakdown:\n\nHamilton: 51 papers\nMadison: 14 papers\nJay: 5 papers\nJoint (Hamilton & Madison): 3 papers\nDisputed (Hamilton or Madison?): 12 papers\n\nAuthorship has been debated since publication. Weâ€™ll use the generally accepted attributions for known papers, then predict the disputed ones.\n\n14.3.1 Load the Data\n\n# Load metadata\nmeta_url = \"https://github.com/browndw/humanities_analytics/raw/refs/heads/main/data/data_meta/federalist_meta.csv\"\nfed_meta = pl.read_csv(meta_url)\n\nprint(\"Authorship breakdown:\")\nprint(fed_meta.group_by('author_id').agg(pl.count()).sort('count', descending=True))\n\nAuthorship breakdown:\nshape: (5, 2)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ author_id        â”† count â”‚\nâ”‚ ---              â”† ---   â”‚\nâ”‚ str              â”† u32   â”‚\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\nâ”‚ Hamilton         â”† 51    â”‚\nâ”‚ Madison          â”† 14    â”‚\nâ”‚ Disputed         â”† 12    â”‚\nâ”‚ Jay              â”† 5     â”‚\nâ”‚ Hamilton_Madison â”† 3     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\n/tmp/ipykernel_3420/3030490263.py:6: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n(Deprecated in version 0.20.5)\n  print(fed_meta.group_by('author_id').agg(pl.count()).sort('count', descending=True))\n\n\n\n# Load text data\ndata_url = \"https://github.com/browndw/humanities_analytics/raw/refs/heads/main/data/data_tables/federalist_papers.csv\"\nfed_text = pl.read_csv(data_url)\n\nprint(f\"Loaded {len(fed_text)} papers\")\n\nLoaded 85 papers",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>Classification & Authorship Attribution</span>"
    ]
  },
  {
    "objectID": "tutorials/classification-federalist-papers.html#data-preparation",
    "href": "tutorials/classification-federalist-papers.html#data-preparation",
    "title": "14Â  Classification & Authorship Attribution",
    "section": "14.4 Data Preparation",
    "text": "14.4 Data Preparation\nWe need to convert the raw text into a document-term matrix with word proportions. Why proportions? Paper length shouldnâ€™t bias our modelâ€”a long paper will naturally have more occurrences of every word.\n\n# Join metadata with text\nfed_data = fed_text.join(fed_meta.select(['doc_id', 'author_id']), on='doc_id', how='left')\n\n\n14.4.1 Create Document-Term Matrix\nWeâ€™ll tokenize the text and count Mosteller & Wallaceâ€™s candidate words, converting counts to proportions:\n\ndef create_dtm(text, vocabulary):\n    \"\"\"Create document-term matrix with proportions for given vocabulary\"\"\"\n    tokens = text.lower().split()\n    total_tokens = len(tokens)\n    \n    word_counts = {}\n    for word in vocabulary:\n        count = tokens.count(word)\n        word_counts[word] = count / total_tokens if total_tokens &gt; 0 else 0\n    \n    return word_counts\n\n# Apply to all documents\ndtm_data = []\nfor row in fed_data.iter_rows(named=True):\n    word_props = create_dtm(row['text'], mw_all)\n    word_props['doc_id'] = row['doc_id']\n    word_props['author_id'] = row['author_id']\n    dtm_data.append(word_props)\n\n# Convert to polars DataFrame\nfed_data_prop = pl.DataFrame(dtm_data)\nword_cols = [col for col in fed_data_prop.columns if col not in ['doc_id', 'author_id']]\nfed_data_prop = fed_data_prop.select(['doc_id', 'author_id'] + word_cols)\n\nprint(f\"Document-term matrix: {fed_data_prop.shape}\")\nprint(\"\\nFirst few rows (sample):\")\nprint(fed_data_prop.select(['doc_id', 'author_id'] + word_cols[:5]).head())\n\nDocument-term matrix: (85, 182)\n\nFirst few rows (sample):\nshape: (5, 7)\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ doc_id        â”† author_id â”† a        â”† about â”† according â”† adversaries â”† affect   â”‚\nâ”‚ ---           â”† ---       â”† ---      â”† ---   â”† ---       â”† ---         â”† ---      â”‚\nâ”‚ str           â”† str       â”† f64      â”† f64   â”† f64       â”† f64         â”† f64      â”‚\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•¡\nâ”‚ FEDERALIST_01 â”† Hamilton  â”† 0.01547  â”† 0.0   â”† 0.0       â”† 0.0         â”† 0.000619 â”‚\nâ”‚ FEDERALIST_02 â”† Jay       â”† 0.017344 â”† 0.0   â”† 0.0       â”† 0.0         â”† 0.0      â”‚\nâ”‚ FEDERALIST_03 â”† Jay       â”† 0.008978 â”† 0.0   â”† 0.0       â”† 0.0         â”† 0.001381 â”‚\nâ”‚ FEDERALIST_04 â”† Jay       â”† 0.00978  â”† 0.0   â”† 0.0       â”† 0.0         â”† 0.001222 â”‚\nâ”‚ FEDERALIST_05 â”† Jay       â”† 0.006682 â”† 0.0   â”† 0.0       â”† 0.0         â”† 0.0      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nWhatâ€™s happening here?\n\nTokenization: Split text into words (lowercased)\nCount: For each M&W word, count occurrences\nNormalize: Divide by total tokens to get proportion\nResult: Each row is a paper, each column is a wordâ€™s proportion",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>Classification & Authorship Attribution</span>"
    ]
  },
  {
    "objectID": "tutorials/classification-federalist-papers.html#traintest-split",
    "href": "tutorials/classification-federalist-papers.html#traintest-split",
    "title": "14Â  Classification & Authorship Attribution",
    "section": "14.5 Train/Test Split",
    "text": "14.5 Train/Test Split\nWeâ€™ll separate papers into:\n\nTraining: Hamilton and Madison papers (known authors) for building the model\nTest: Disputed papers (what we want to predict)\n\n\n# Training data: known authors (Hamilton & Madison only)\ntrain_data = fed_data_prop.filter(\n    (pl.col('author_id') == 'Hamilton') | (pl.col('author_id') == 'Madison')\n)\n\n# Test data: disputed papers\ntest_data = fed_data_prop.filter(pl.col('author_id') == 'Disputed')\n\nprint(f\"Training data: {len(train_data)} papers\")\nprint(f\"Test data: {len(test_data)} papers\")\n\nTraining data: 65 papers\nTest data: 12 papers",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>Classification & Authorship Attribution</span>"
    ]
  },
  {
    "objectID": "tutorials/classification-federalist-papers.html#random-forest-classification",
    "href": "tutorials/classification-federalist-papers.html#random-forest-classification",
    "title": "14Â  Classification & Authorship Attribution",
    "section": "14.6 Random Forest Classification",
    "text": "14.6 Random Forest Classification\nWhy Random Forest?\n\nEnsemble method: Combines multiple decision trees\nRobust: Handles many features well\nInterpretable: Provides feature importance rankings\nPopular: Widely used in practice\n\nHow it works:\n\nBuild many decision trees (100 in our case)\nEach tree votes for a class (Hamilton or Madison)\nFinal prediction is the majority vote\n\nLetâ€™s start with just Mosteller & Wallaceâ€™s first group (70 words):\n\n# Select Group 1 variables\navailable_mw1 = [word for word in mw_group1 if word in train_data.columns]\ntrain_g1 = train_data.select(['doc_id', 'author_id'] + available_mw1)\n\n# Convert to numpy for sklearn\nX_train_g1 = train_g1.select(available_mw1).to_numpy()\ny_train_g1 = train_g1.select('author_id').to_numpy().flatten()\n\n# Split into train/validation (80/20)\n# stratify=y_train_g1 maintains class balance (51 Hamilton vs 14 Madison)\nX_tr, X_val, y_tr, y_val = train_test_split(\n    X_train_g1, y_train_g1, test_size=0.2, random_state=123, stratify=y_train_g1\n)\n\nprint(f\"Training set: {len(X_tr)} papers\")\nprint(f\"Validation set: {len(X_val)} papers\")\n\nTraining set: 52 papers\nValidation set: 13 papers\n\n\n\n14.6.1 Train the Model\n\n# Random Forest with 100 trees\nrf_model = RandomForestClassifier(\n    n_estimators=100,\n    random_state=123,\n    max_depth=10,\n    min_samples_split=5\n)\n\nrf_model.fit(X_tr, y_tr)\n\nprint(f\"Training accuracy: {rf_model.score(X_tr, y_tr):.2%}\")\n\nTraining accuracy: 100.00%\n\n\n\n\n14.6.2 Feature Importance\nOne advantage of Random Forest: we can see which features (words) matter most:\n\nfeature_importance = pd.DataFrame({\n    'feature': available_mw1,\n    'importance': rf_model.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"Top 20 most important features:\")\nprint(feature_importance.head(20))\n\nTop 20 most important features:\n   feature  importance\n59    upon    0.197918\n39      on    0.103972\n54   there    0.078205\n12      by    0.077218\n57      to    0.049214\n4      and    0.048835\n64   which    0.024699\n68   would    0.022795\n38      of    0.019039\n0        a    0.018555\n26      in    0.018129\n35      no    0.017874\n56    this    0.017812\n8       at    0.017394\n17   every    0.017315\n45  should    0.016665\n27    into    0.015672\n46      so    0.014967\n28      is    0.013733\n20     had    0.012681\n\n\nInterpretation: Importance measures how much each word reduces classification uncertainty (Gini impurity). Higher values = better at distinguishing Hamilton from Madison. The Random Forest averages importance across all 100 trees.\n\n\n14.6.3 Validation\nLetâ€™s test on the held-out validation set:\n\ny_pred_val = rf_model.predict(X_val)\n\nprint(f\"Validation accuracy: {accuracy_score(y_val, y_pred_val):.2%}\\n\")\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_val, y_pred_val))\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_val, y_pred_val))\n\nValidation accuracy: 100.00%\n\nConfusion Matrix:\n[[10  0]\n [ 0  3]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n    Hamilton       1.00      1.00      1.00        10\n     Madison       1.00      1.00      1.00         3\n\n    accuracy                           1.00        13\n   macro avg       1.00      1.00      1.00        13\nweighted avg       1.00      1.00      1.00        13\n\n\n\nReading the confusion matrix:\n             Predicted\n             H    M\nActual  H   [10   0]\n        M   [ 0   3]\nPerfect classification! All Hamiltons correctly identified as Hamilton, all Madisons as Madison.\nClassification report metrics:\n\nPrecision: Of papers predicted as author X, what % were actually by X?\nRecall: Of papers truly by author X, what % did we correctly identify?\nF1-score: Harmonic mean balancing precision and recall\nSupport: Number of papers by each author in validation set\n\nThese metrics matter especially with imbalanced classes (51 Hamilton vs 14 Madison papers).",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>Classification & Authorship Attribution</span>"
    ]
  },
  {
    "objectID": "tutorials/classification-federalist-papers.html#full-model-all-mw-variables",
    "href": "tutorials/classification-federalist-papers.html#full-model-all-mw-variables",
    "title": "14Â  Classification & Authorship Attribution",
    "section": "14.7 Full Model: All M&W Variables",
    "text": "14.7 Full Model: All M&W Variables\nNow letâ€™s use all 180 variables:\n\n# Select all M&W variables\navailable_mw_all = [word for word in mw_all if word in train_data.columns]\ntrain_all = train_data.select(['doc_id', 'author_id'] + available_mw_all)\n\n# Convert to numpy\nX_train_all = train_all.select(available_mw_all).to_numpy()\ny_train_all = train_all.select('author_id').to_numpy().flatten()\n\n# Split\nX_tr2, X_val2, y_tr2, y_val2 = train_test_split(\n    X_train_all, y_train_all, test_size=0.2, random_state=123, stratify=y_train_all\n)\n\nprint(f\"Feature matrix: {X_train_all.shape}\")\n\nFeature matrix: (65, 180)\n\n\n\n# Train Random Forest\nrf_model2 = RandomForestClassifier(\n    n_estimators=100,\n    random_state=123,\n    max_depth=10,\n    min_samples_split=5\n)\n\nrf_model2.fit(X_tr2, y_tr2)\n\nprint(f\"Training accuracy: {rf_model2.score(X_tr2, y_tr2):.2%}\")\n\nTraining accuracy: 100.00%\n\n\n\n# Feature importance\nfeature_importance2 = pd.DataFrame({\n    'feature': available_mw_all,\n    'importance': rf_model2.feature_importances_\n}).sort_values('importance', ascending=False)\n\nprint(\"Top 20 most important features:\")\nprint(feature_importance2.head(20))\n\nTop 20 most important features:\n          feature  importance\n112            on    0.123860\n155          upon    0.078644\n150            to    0.056658\n143         there    0.051814\n31             by    0.047179\n172        whilst    0.032703\n162         voice    0.026959\n10       although    0.025712\n169       whether    0.025421\n178         would    0.023410\n147         those    0.021650\n14            and    0.018237\n80             in    0.017668\n38   consequently    0.017125\n161      violence    0.017055\n148        though    0.014737\n12          among    0.014324\n11         always    0.014092\n129          same    0.013577\n102     necessary    0.013213\n\n\n\n# Validate\ny_pred_val2 = rf_model2.predict(X_val2)\n\nprint(f\"Validation accuracy: {accuracy_score(y_val2, y_pred_val2):.2%}\")\n\nValidation accuracy: 100.00%",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>Classification & Authorship Attribution</span>"
    ]
  },
  {
    "objectID": "tutorials/classification-federalist-papers.html#predict-disputed-papers",
    "href": "tutorials/classification-federalist-papers.html#predict-disputed-papers",
    "title": "14Â  Classification & Authorship Attribution",
    "section": "14.8 Predict Disputed Papers",
    "text": "14.8 Predict Disputed Papers\nNow for the main event: Who wrote the 12 disputed papers?\n\n# Train final model on ALL Hamilton/Madison papers (no validation split)\nfinal_model = RandomForestClassifier(\n    n_estimators=100,\n    random_state=123,\n    max_depth=10,\n    min_samples_split=5\n)\n\nfinal_model.fit(X_train_all, y_train_all)\n\nprint(f\"Final model training accuracy: {final_model.score(X_train_all, y_train_all):.2%}\")\n\nFinal model training accuracy: 100.00%\n\n\n\n# Prepare test data\ntest_subset = test_data.select(['doc_id', 'author_id'] + available_mw_all)\nX_test = test_subset.select(available_mw_all).to_numpy()\ntest_doc_ids = test_subset.select('doc_id').to_numpy().flatten()\n\n# Predict\ntest_probs = final_model.predict_proba(X_test)\ntest_predictions = final_model.predict(X_test)\n\n# Results\nresults = pd.DataFrame({\n    'Paper': test_doc_ids,\n    'Prob_Hamilton': test_probs[:, 0],\n    'Prob_Madison': test_probs[:, 1],\n    'Predicted_Author': test_predictions\n})\n\nprint(\"\\nPredictions for Disputed Papers:\")\nprint(\"=\" * 60)\nprint(results.to_string(index=False))\n\n\nPredictions for Disputed Papers:\n============================================================\n        Paper  Prob_Hamilton  Prob_Madison Predicted_Author\nFEDERALIST_49       0.392952      0.607048          Madison\nFEDERALIST_50       0.375254      0.624746          Madison\nFEDERALIST_51       0.339167      0.660833          Madison\nFEDERALIST_52       0.421310      0.578690          Madison\nFEDERALIST_53       0.478548      0.521452          Madison\nFEDERALIST_54       0.391643      0.608357          Madison\nFEDERALIST_55       0.653333      0.346667         Hamilton\nFEDERALIST_56       0.407595      0.592405          Madison\nFEDERALIST_57       0.412873      0.587127          Madison\nFEDERALIST_58       0.417310      0.582690          Madison\nFEDERALIST_62       0.354738      0.645262          Madison\nFEDERALIST_63       0.484167      0.515833          Madison\n\n\n\n14.8.1 Visualization\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\npapers = results['Paper'].values\nx_pos = np.arange(len(papers))\n\n# Stacked bar chart\nax.barh(x_pos, results['Prob_Hamilton'], label='Hamilton', color='#d62728')\nax.barh(x_pos, results['Prob_Madison'], left=results['Prob_Hamilton'], \n        label='Madison', color='#1f77b4')\n\n# Decision boundary\nax.axvline(x=0.5, color='black', linestyle='--', linewidth=1, alpha=0.5)\n\nax.set_yticks(x_pos)\nax.set_yticklabels(papers)\nax.set_xlabel('Probability', fontsize=12)\nax.set_ylabel('Federalist Paper', fontsize=12)\nax.set_title('Authorship Attribution: Disputed Federalist Papers', \n             fontsize=14, fontweight='bold')\nax.legend(loc='upper right')\nax.set_xlim(0, 1)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\nFigureÂ 14.1: Authorship probabilities for disputed Federalist Papers. Most show strong evidence for Madison, while Paper 55 leans toward Hamilton.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>Classification & Authorship Attribution</span>"
    ]
  },
  {
    "objectID": "tutorials/classification-federalist-papers.html#interpretation",
    "href": "tutorials/classification-federalist-papers.html#interpretation",
    "title": "14Â  Classification & Authorship Attribution",
    "section": "14.9 Interpretation",
    "text": "14.9 Interpretation\nOur findings:\n\n11 papers predicted as Madison (varying confidence 52-66%)\nPaper 55 predicted as Hamilton (65% confidence)\n\nComparison to Mosteller & Wallace (1963):\n\nThey concluded Madison wrote all disputed papers except possibly Paper 55\nPaper 55 had weak evidenceâ€”consistent with our low confidence\nLater studies using different methods have also suggested Hamilton for Paper 55\n\nKey discriminating features:\nThe Random Forest identified function words as most importantâ€”subtle stylistic markers like â€œuponâ€ (Hamilton), â€œwhilstâ€ (Madison), â€œthere,â€ â€œof.â€ These are words humans might not consciously notice, but algorithms detect reliably.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>Classification & Authorship Attribution</span>"
    ]
  },
  {
    "objectID": "tutorials/classification-federalist-papers.html#what-this-demonstrates",
    "href": "tutorials/classification-federalist-papers.html#what-this-demonstrates",
    "title": "14Â  Classification & Authorship Attribution",
    "section": "14.10 What This Demonstrates",
    "text": "14.10 What This Demonstrates\n\nStylometry works: Function word frequencies distinguish authors\nAutomation enables scale: What took M&W months takes us minutes\nUncertainty remains: Some texts (Paper 55) are genuinely ambiguous\nValidation matters: Testing on known authors first builds confidence",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>Classification & Authorship Attribution</span>"
    ]
  },
  {
    "objectID": "tutorials/classification-federalist-papers.html#methodological-considerations",
    "href": "tutorials/classification-federalist-papers.html#methodological-considerations",
    "title": "14Â  Classification & Authorship Attribution",
    "section": "14.11 Methodological Considerations",
    "text": "14.11 Methodological Considerations\n\n14.11.1 Why Function Words?\nFunction words (articles, prepositions, conjunctions) are:\n\nFrequent: Occur in every text\nUnconscious: Writers donâ€™t think about them\nStable: Donâ€™t vary with topic\nDistinctive: Reflect individual style\n\nContent words (nouns, verbs about topics) are:\n\nTopic-dependent: â€œgovernment,â€ â€œlibertyâ€ appear in political writing\nConscious: Authors choose them deliberately\nShared: Multiple authors discussing Constitution use similar vocabulary\n\n\n\n14.11.2 Model Choice: Random Forest vs.Â Alternatives\nRandom Forest advantages:\n\nNo complex hyperparameter tuning needed\nHandles nonlinear relationships\nRobust to outliers\nFeature importance built-in\n\nAlternatives you might try:\n\nLogistic Regression: Simpler, more interpretable coefficients\nSupport Vector Machines (SVM): Effective for high-dimensional data\nNaive Bayes: Fast, probabilistic, works well for text\nNeural Networks: Powerful but requires more data\n\n\n\n14.11.3 Sample Size Considerations\nWe have:\n\n51 Hamilton papers: Plenty for training\n14 Madison papers: Smaller, but sufficient\n12 disputed papers: Reasonable test set\n\nImbalance problem: 51 Hamilton vs.Â 14 Madison. Random Forest handles this reasonably well, but you could try:\n\nClass weighting\nResampling techniques (SMOTE, undersampling)\nStratified cross-validation (which we used)",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>Classification & Authorship Attribution</span>"
    ]
  },
  {
    "objectID": "tutorials/classification-federalist-papers.html#connections-to-other-methods",
    "href": "tutorials/classification-federalist-papers.html#connections-to-other-methods",
    "title": "14Â  Classification & Authorship Attribution",
    "section": "14.12 Connections to Other Methods",
    "text": "14.12 Connections to Other Methods\n\n14.12.1 Multi-Dimensional Analysis (MDA)\nMini Lab 10 examined stylistic dimensions. Classification is:\n\nSupervised: We know the authors\nBinary: Hamilton or Madison?\nPredictive: Goal is accurate classification\n\nMDA is:\n\nUnsupervised: Discovers patterns without labels\nContinuous: Multiple dimensions simultaneously\nExploratory: Goal is understanding variation\n\n\n\n14.12.2 Contextual Embeddings\nMini Lab 11 used semantic similarity. You could combine approaches:\n\nExtract contextual embeddings for each paper\nUse embeddings as features in Random Forest\nCompare performance to function words alone\n\n\n\n14.12.3 Keyness Analysis\nKeyness identifies distinctive words. Feature importance from Random Forest serves a similar purposeâ€”which words best distinguish authors?",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>Classification & Authorship Attribution</span>"
    ]
  },
  {
    "objectID": "tutorials/classification-federalist-papers.html#extensions-applications",
    "href": "tutorials/classification-federalist-papers.html#extensions-applications",
    "title": "14Â  Classification & Authorship Attribution",
    "section": "14.13 Extensions & Applications",
    "text": "14.13 Extensions & Applications\n\n14.13.1 Historical Questions\nAuthorship attribution can answer:\n\nAnonymous texts: Who wrote this unsigned medieval chronicle?\nDisputed works: Did Shakespeare write the collaborative plays?\nGhostwriting: Did this politician write their own speeches?\nForgeries: Is this â€œnewly discoveredâ€ document authentic?\n\n\n\n14.13.2 Modern Applications\n\nSocial media: Identifying sock puppet accounts\nForensic linguistics: Author profiling in legal cases\nPlagiarism detection: Did two students collaborate inappropriately?\nLiterary studies: Tracking stylistic evolution across an authorâ€™s career\n\n\n\n14.13.3 Ethical Considerations\nPrivacy concerns:\n\nStylometric techniques can de-anonymize authors\nEven brief texts may reveal identity\nâ€œAnonymousâ€ writing may not stay anonymous\n\nPotential misuse:\n\nSurveillance and profiling\nViolating confidentiality\nIncorrect attributions with serious consequences\n\nBest practices:\n\nObtain consent when possible\nBe transparent about methods\nAcknowledge uncertainty (like Paper 55)\nConsider ethical implications of findings",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>Classification & Authorship Attribution</span>"
    ]
  },
  {
    "objectID": "tutorials/classification-federalist-papers.html#summary",
    "href": "tutorials/classification-federalist-papers.html#summary",
    "title": "14Â  Classification & Authorship Attribution",
    "section": "14.14 Summary",
    "text": "14.14 Summary\nWhat we accomplished:\n\nReplicated Mosteller & Wallaceâ€™s famous 1963 study\nUsed Random Forest for robust classification\nAchieved high accuracy on validation data\nPredicted authorship of disputed Federalist Papers\n\nKey insights:\n\nFunction words reveal authorial style\nEnsemble methods provide robust predictions\nSome texts resist confident classification\nComputational methods illuminate historical questions\n\nNext steps:\nApply these techniques to your own research questions. The combination of careful feature selection, appropriate modeling, and rigorous validation opens doors to answering questions that seemed unanswerable.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>Classification & Authorship Attribution</span>"
    ]
  },
  {
    "objectID": "tutorials/classification-federalist-papers.html#further-reading",
    "href": "tutorials/classification-federalist-papers.html#further-reading",
    "title": "14Â  Classification & Authorship Attribution",
    "section": "14.15 Further Reading",
    "text": "14.15 Further Reading\n\nMosteller, F., & Wallace, D. L. (1963). Inference and disputed authorship: The Federalist. Addison-Wesley.\nJockers, M. L., & Witten, D. M. (2010). A comparative study of machine learning methods for authorship attribution. Literary and Linguistic Computing, 25(2), 215-223.\nKoppel, M., Schler, J., & Argamon, S. (2009). Computational methods in authorship attribution. Journal of the American Society for Information Science and Technology, 60(1), 9-26.\nJuola, P. (2008). Authorship attribution. Foundations and Trends in Information Retrieval, 1(3), 233-334.\n\n\n\n\n\nMosteller, Frederick, and David L Wallace. 1963. â€œInference in an Authorship Problem: A Comparative Study of Discrimination Methods Applied to the Authorship of the Disputed Federalist Papers.â€ Journal Article. Journal of the American Statistical Association 58 (302): 275â€“309. https://doi.org/10.1080/01621459.1963.10500849.",
    "crumbs": [
      "Applications",
      "<span class='chapter-number'>14</span>Â  <span class='chapter-title'>Classification & Authorship Attribution</span>"
    ]
  },
  {
    "objectID": "reports/cbe_01.html",
    "href": "reports/cbe_01.html",
    "title": "Coffee Break Experiment #1: From Curiosity to Visualization",
    "section": "",
    "text": "The Assignment\nWhat youâ€™ll create: One slide with a visualization and a claim\nWhere to explore: BYU English Language Corpora\nWhere to share: Class Google Slides (add your slide to the deck)\nKey requirement: Include your name somewhere on the slide!",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #1: From Curiosity to Visualization"
    ]
  },
  {
    "objectID": "reports/cbe_01.html#part-1-finding-something-interesting",
    "href": "reports/cbe_01.html#part-1-finding-something-interesting",
    "title": "Coffee Break Experiment #1: From Curiosity to Visualization",
    "section": "Part 1: Finding Something Interesting",
    "text": "Part 1: Finding Something Interesting\n\nBrowsing the BYU Corpora\nThe BYU corpora give you access to massive language datasets. Here are some options:\nCorpus of Contemporary American English (COCA): - 1 billion words (1990-2019) - Multiple genres: spoken, fiction, magazines, newspapers, academic - Good for: tracking recent changes, comparing genres\nCorpus of Historical American English (COHA): - 400+ million words (1820s-2000s) - Good for: tracking change over time, historical patterns\nGoogle Books Ngram Viewer: - Massive corpus across centuries - Good for: quick exploration, long-term trends\n\n\n\n\n\n\nStart with Curiosity, Not a Hypothesis\n\n\n\nDonâ€™t feel like you need a research question. Instead, ask: - â€œI wonder if people use X differently than Y?â€ - â€œHas anyone noticed how often Z appears in news vs.Â fiction?â€ - â€œWhat happened to that old-fashioned word W?â€\nExplore first, explain later.\n\n\n\n\nExploration Strategies\nFollow your interests: - Love science fiction? Compare â€œalienâ€ vs â€œrobotâ€ over time - Into politics? Track â€œdemocracyâ€ vs â€œfreedomâ€ across decades - Language nerd? Compare â€œliterallyâ€ usage across genres\nLook for patterns: - Change over time: Is a word increasing or declining? - Genre differences: Does a word appear more in spoken vs.Â academic texts? - Collocations: What words appear near your target word? - Variants: â€œemailâ€ vs â€œe-mail,â€ â€œOKâ€ vs â€œokayâ€\nQuick tips for the BYU interface:\n\nSearch: Type a word in the search box\nChart: Click â€œChartâ€ to see frequency over time\nCompare: Use the comparison feature for multiple words\nDownload: Export data as CSV for plotting\n\n\n\n\n\n\n\nYou Donâ€™t Need Big Claims\n\n\n\nGood CBE findings: - â€œHuh, â€˜literallyâ€™ spiked in the 2010sâ€ - â€œInterestingâ€”â€˜emailâ€™ overtook â€˜e-mailâ€™ around 2005â€ - â€œâ€˜Robotâ€™ appears way more in fiction than academic writingâ€\nThese observations are enough!",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #1: From Curiosity to Visualization"
    ]
  },
  {
    "objectID": "reports/cbe_01.html#part-2-creating-your-visualization",
    "href": "reports/cbe_01.html#part-2-creating-your-visualization",
    "title": "Coffee Break Experiment #1: From Curiosity to Visualization",
    "section": "Part 2: Creating Your Visualization",
    "text": "Part 2: Creating Your Visualization\n\nThe Assertion-Evidence Model\nYour slide should have two parts:\n\nAssertion (headline): A clear claim about what you found\nEvidence (visualization): A graph that supports the claim\n\nWatch this explanation (from 4:45): https://youtu.be/kbdO7adBRFE?t=280\nExamples of assertion headlines:\nâœ… Good (specific claim): - â€œâ€˜Literallyâ€™ has tripled in frequency since 2000â€ - â€œâ€˜E-mailâ€™ gave way to â€˜emailâ€™ in the mid-2000sâ€ - â€œFiction uses â€˜suddenlyâ€™ 5Ã— more than academic writingâ€\nâŒ Too vague: - â€œWord frequency over timeâ€ - â€œInteresting patterns in COCAâ€ - â€œLanguage changeâ€\n\n\nChoosing Your Plot Type\nMatch your plot to your data:\nTime series (change over time): - Line chart: Shows trends clearly - Use when: tracking a word across decades\nComparison (differences between categories): - Bar chart: Compares discrete categories - Use when: comparing genres, word variants, or short time periods\nRelationship (correlation between variables): - Scatter plot: Shows if two things co-vary - Use when: exploring if two words rise/fall together\n\n\n\n\n\n\nPlot Types to Avoid for Beginners\n\n\n\n\nPie charts: Hard to compare precisely\n3D charts: Distort perception\nDual-axis charts: Confusing and easily misleading\n\nStick with simple line or bar charts for this exercise.\n\n\n\n\nDesign Principles: Less is More\nFollow these quick rules for clear visualizations:\n1. Clear labels - Axes: What are you measuring? Include units (e.g., â€œFrequency per million wordsâ€) - Title/Assertion: Your claim goes here, not â€œFigure 1â€ - Legend: If comparing multiple items, label them clearly\n2. Reduce non-data ink - Remove unnecessary gridlines - Eliminate chart borders if they donâ€™t add information - Use minimal colors (2-3 max)\n3. Make it readable - Large enough font (18pt minimum for presentation slides) - High contrast (dark text on light background or vice versa) - Donâ€™t crowd the slideâ€”white space is good\nDesign resource: https://stat545.com/effective-graphs.html\n\n\nGetting Your Data Ready\n\n\n\n\n\n\nDonâ€™t Use Screen Grabs!\n\n\n\nNever take a screenshot of BYU corpus output and paste it into your slide. This creates a blurry, unprofessional image that canâ€™t be edited.\nAlways work with the actual data to create your own chart.\n\n\nCopying data from BYU corpora:\nWhen you copy frequency data from BYU corpus results, it often includes hidden hyperlinks that will cause problems in Excel or Google Sheets.\nSolution 1: Paste into text editor first 1. Copy data from BYU corpus 2. Paste into Notepad (Windows) or TextEdit (Mac) 3. Copy again from text editor 4. Now paste into Excel/Sheetsâ€”links removed!\nSolution 2: Use Paste Special 1. Copy data from BYU corpus 2. In Excel/Sheets: Edit â†’ Paste Special â†’ Values only (or Ctrl/Cmd + Shift + V) 3. This pastes just the text, not the links\n\n\nCreating Your Plot\nUsing Google Sheets (recommended for beginners):\n\nEnter your data: Type or paste (see tips above about removing links)\nYear    Frequency\n1990    2.3\n2000    3.1\n2010    5.2\n2020    6.8\nSelect your data: Highlight both columns including headers\nInsert chart: Insert â†’ Chart\nChoose chart type:\n\nChart editor â†’ Setup â†’ Chart type\nPick Line chart or Column chart\n\nClean up the default design (this is important!):\n\nChart editor â†’ Customize tab\nChart style: Uncheck â€œBorder colorâ€ (removes box around chart)\nChart & axis titles:\n\nChart title: Enter your assertion (e.g., â€œâ€˜Literallyâ€™ has tripled since 2000â€)\nHorizontal axis: Enter what X represents (e.g., â€œYearâ€)\nVertical axis: Enter what Y represents with units (e.g., â€œFrequency per million wordsâ€)\n\nGridlines:\n\nMajor gridlines â†’ Uncheck (or set to light gray if you need them)\n\nLegend:\n\nIf only one line/bar, uncheck â€œLegendâ€ entirely (unnecessary clutter)\n\nSeries:\n\nIncrease line thickness (2-3pt) or make bars wider for better visibility\n\n\nCopy to Google Slides:\n\nClick the three dots (â‹®) on chart â†’ Copy chart\nIn Google Slides: Ctrl/Cmd + V\nWhen prompted, choose â€œLink to spreadsheetâ€ or â€œPaste unlinkedâ€ (unlinked is fine for CBE)\n\n\n\n\n\n\n\n\nGoogle Sheets Default Designs Need Help\n\n\n\nThe default chart has too much clutter: - âŒ Heavy gridlines that compete with your data - âŒ Unnecessary border box - âŒ Legend when you only have one data series - âŒ Thin lines that are hard to see\nSpend 2 minutes cleaning these upâ€”it makes a huge difference!\n\n\nUsing Excel:\n\nEnter your data: Type or paste (use Paste Special â†’ Values if copying from BYU)\nSelect data: Highlight both columns\nInsert chart: Insert tab â†’ Chart type (Line or Column)\nClean up the design:\n\nRemove gridlines: Click chart â†’ Plus icon (+) â†’ Gridlines â†’ Uncheck\nRemove border: Click chart border â†’ Delete or Format â†’ Border â†’ No line\nAdd axis titles: Plus icon (+) â†’ Axis Titles â†’ Check Primary Horizontal and Primary Vertical\n\nClick each title to edit the text\n\nRemove legend (if only one series): Plus icon (+) â†’ Legend â†’ Uncheck\nMake lines thicker:\n\nClick the line â†’ Format Data Series â†’ Line â†’ Width (increase to 2-3pt)\n\nChart title: Click â€œChart Titleâ€ and type your assertion\n\nCopy to Google Slides:\n\nRight-click chart â†’ Copy\nIn Google Slides â†’ Paste\n\n\n\n\n\n\n\n\nWhy Remove Gridlines and Borders?\n\n\n\nEvery element that isnâ€™t your actual data is â€œnon-data ink.â€ Gridlines, borders, and heavy axis lines compete for attention with your main message. Your eye should go straight to the line or barsâ€”everything else should fade into the background.\nCompare: - Default Excel chart: Heavy grid, border, thick axes, small data - Cleaned chart: Just the data line, simple axes, clear labels\nThe second one is easier to read and more professional.\n\n\nUsing Python (if youâ€™re comfortable):\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\n# Your data\nyears = [1990, 2000, 2010, 2020]\nfrequency = [2.3, 3.1, 5.2, 6.8]\n\n# Create plot\nplt.figure(figsize=(8, 5))\nplt.plot(years, frequency, marker='o', linewidth=2)\nplt.xlabel('Year', fontsize=14)\nplt.ylabel('Frequency per million words', fontsize=14)\nplt.title(\"'Literally' has tripled since 2000\", fontsize=16, weight='bold')\nplt.grid(False)\nplt.savefig('literally_trend.png', dpi=300, bbox_inches='tight')\n\n\n\n\n\n\nGood Enough Is Good Enough\n\n\n\nYour first visualization doesnâ€™t need to be perfect: - Colors not quite right? Thatâ€™s okay - Legend slightly off? Donâ€™t worry about it - Wish the line was thicker? Move on\nFocus on: clear assertion + clean visualization that supports it",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #1: From Curiosity to Visualization"
    ]
  },
  {
    "objectID": "reports/cbe_01.html#part-3-building-your-slide",
    "href": "reports/cbe_01.html#part-3-building-your-slide",
    "title": "Coffee Break Experiment #1: From Curiosity to Visualization",
    "section": "Part 3: Building Your Slide",
    "text": "Part 3: Building Your Slide\n\nSlide Layout\nTop half: Your assertion (headline) - Font: 36-44pt, bold - Keep it to one line if possible (two max)\nBottom half: Your visualization - Make it largeâ€”fill most of the space - Leave some white space around edges\nCorner: Your name - Font: 18-24pt - Top right or bottom right corner\nOptional: Brief note about data source - Font: 14pt - Bottom of slide: â€œData: COCA 1990-2020â€\n\n\nExample Slide Structure\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ 'Literally' has tripled in frequency since 2000â”‚\nâ”‚                                                  â”‚\nâ”‚         [LARGE LINE CHART HERE]                  â”‚\nâ”‚                                                  â”‚\nâ”‚                                                  â”‚\nâ”‚ Data: COCA 1990-2020              Your Name     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\n\nAdding Your Slide to the Deck\n\nOpen the shared Google Slides: [Class presentation link]\nAdd a new slide:\n\nGo to the end of the deck\nSlide â†’ New Slide (or Ctrl/Cmd + M)\n\nChoose blank layout: Right-click â†’ Apply Layout â†’ Blank\nAdd your assertion: Insert â†’ Text Box\nAdd your visualization:\n\nFrom Excel: Copy chart â†’ Paste\nFrom file: Insert â†’ Image â†’ Upload from computer\n\nAdd your name: Insert â†’ Text Box â†’ Type your name\nCheck it: Present mode to see how it looks full-screen\n\n\n\n\n\n\n\nPresenting Your Slide\n\n\n\nBe ready to say in ~30 seconds: - â€œI noticed X in the BYU corpusâ€ - â€œThe chart shows Y patternâ€ - â€œI think this might mean Zâ€ (or â€œIâ€™m not sure what it means yet!â€)\nThe goal is to share what you found and hear what others discovered.",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #1: From Curiosity to Visualization"
    ]
  },
  {
    "objectID": "reports/cbe_01.html#part-4-from-visualization-to-interpretation",
    "href": "reports/cbe_01.html#part-4-from-visualization-to-interpretation",
    "title": "Coffee Break Experiment #1: From Curiosity to Visualization",
    "section": "Part 4: From Visualization to Interpretation",
    "text": "Part 4: From Visualization to Interpretation\n\nWhat Did You Find?\nAfter creating your visualization, reflect:\nDescribe the pattern: - What does the visualization show? - Is there an increase, decrease, or difference? - How big is the change or difference?\nSpeculate about why: - What might explain this pattern? - Cultural shifts? Technology? Genre conventions? - Donâ€™t worry about being certainâ€”brainstorm possibilities\nWonder whatâ€™s next: - What would you look at if you had more time? - Other words to compare? Different time periods? More genres?\n\n\n\n\n\n\nEmbrace â€œI Donâ€™t Knowâ€\n\n\n\nItâ€™s completely fine if you: - Found something weird and donâ€™t know why - Canâ€™t explain the pattern - Have more questions than answers\nCBEs are about exploration, not having all the answers. Share what puzzles you!\n\n\n\n\nExample Reflection\n\nWhat I found: â€œEmailâ€ overtook â€œe-mailâ€ around 2005\nWhy it might matter: Shows how technology terms stabilizeâ€”the hyphen disappeared as email became commonplace\nWhat Iâ€™d do next: Check when other tech terms lost hyphens (e-commerce? e-book?)\nWhat puzzles me: Why 2005 specifically? Was there an influential style guide change?",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #1: From Curiosity to Visualization"
    ]
  },
  {
    "objectID": "reports/cbe_01.html#reflection-questions",
    "href": "reports/cbe_01.html#reflection-questions",
    "title": "Coffee Break Experiment #1: From Curiosity to Visualization",
    "section": "Reflection Questions",
    "text": "Reflection Questions\nAfter completing CBE #1:\n\nDiscovery: What surprised you most when exploring the corpus? Did you find what you expected?\nVisualization: What was hardest about creating the chartâ€”the technical steps, design decisions, or deciding what to show?\nCommunication: Does your assertion + visualization combination tell a clear story? What would make it clearer?\nCuriosity: What new questions emerged from this exploration? Where would you go next?\nTool comfort: How comfortable did you feel with the BYU interface and Excel/plotting tools? What would help you improve?",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #1: From Curiosity to Visualization"
    ]
  },
  {
    "objectID": "reports/cbe_01.html#quick-reference-checklist",
    "href": "reports/cbe_01.html#quick-reference-checklist",
    "title": "Coffee Break Experiment #1: From Curiosity to Visualization",
    "section": "Quick Reference Checklist",
    "text": "Quick Reference Checklist\nBefore adding your slide to the deck:\n\nAssertion headline is clear and specific\nVisualization supports the assertion\nAxes are labeled with units\nChart is simple and readable (minimal clutter)\nYour name appears on the slide\nSlide looks good in present/full-screen mode\nYou can explain your finding in 30 seconds\n\n\n\n\n\n\n\nRemember: Coffee Break = Quick & Interesting\n\n\n\nThis exercise should take an hour or two, not days. Pick something that catches your eye, make a clear chart, share it. The goal is practice, not perfection.",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #1: From Curiosity to Visualization"
    ]
  },
  {
    "objectID": "reports/cbe_01.html#works-cited",
    "href": "reports/cbe_01.html#works-cited",
    "title": "Coffee Break Experiment #1: From Curiosity to Visualization",
    "section": "Works Cited",
    "text": "Works Cited",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #1: From Curiosity to Visualization"
    ]
  },
  {
    "objectID": "reports/cbe_02.html",
    "href": "reports/cbe_02.html",
    "title": "Coffee Break Experiment #2: From Results to Research",
    "section": "",
    "text": "Part 1: Getting Started with Overleaf",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #2: From Results to Research"
    ]
  },
  {
    "objectID": "reports/cbe_02.html#part-1-getting-started-with-overleaf",
    "href": "reports/cbe_02.html#part-1-getting-started-with-overleaf",
    "title": "Coffee Break Experiment #2: From Results to Research",
    "section": "",
    "text": "Understanding the Template Structure\nThe Coffee Break Experiment template provides a basic academic paper structure. Hereâ€™s what you need to know:\nMain Files:\n\nmain.tex - The primary document (this is where you write)\nmain.bib - Bibliography database\ntables/ - Put your table files here\nfigures/ - Put your images here\n\nEssential LaTeX Commands:\n\nSections: \\section{Title} creates a new section\nReferences: Table~\\ref{tbl:composition} links to your table\nCitations: \\cite{authorYEAR} cites a source\nIncluding files: \\input{tables/filename} brings in a table\n\n\n\n\n\n\n\nDonâ€™t Worry About LaTeX Perfection\n\n\n\nIf something doesnâ€™t compile perfectly, focus on getting your ideas down. You can fix formatting later (or leave it imperfect for this exploratory exercise).\n\n\n\n\n\n\n\n\nCollaborative Editing\n\n\n\nMultiple team members can edit simultaneously in Overleaf. Youâ€™ll see each otherâ€™s cursors in real-time. Use the comment feature (add comment icon) to discuss changes without editing the text directly.\n\n\n\n\nSetting Up Your Team Report\nStep-by-Step Setup:\n\nManager creates the project:\n\nClick â€œCopy Projectâ€ on the template\nRename: â€œCBE 2: [Your Topic]â€ (e.g., â€œCBE 2: ChatGPT vs Human Academic Writingâ€)\n\nShare with team:\n\nClick â€œShareâ€ button (top right)\nAdd team membersâ€™ email addresses\nSet permissions to â€œCan Editâ€\n\nCustomize the front matter:\n\nUpdate \\title{CBE Title} with your actual title\nReplace Author One, Author Two with your names\nUpdate department affiliations if needed\n\nCreate your first file:\n\nGo to tables/ folder\nClick three dots â†’ New File\nName it descriptively: hape_composition.tex",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #2: From Results to Research"
    ]
  },
  {
    "objectID": "reports/cbe_02.html#part-2-integrating-results-from-mini-lab-5",
    "href": "reports/cbe_02.html#part-2-integrating-results-from-mini-lab-5",
    "title": "Coffee Break Experiment #2: From Results to Research",
    "section": "Part 2: Integrating Results from Mini Lab 5",
    "text": "Part 2: Integrating Results from Mini Lab 5\n\nAdding Your Corpus Composition Table\nFrom Mini Lab 5, you generated LaTeX code using great_tables. Hereâ€™s how to integrate it:\nIn your Colab notebook:\n# This generates LaTeX code\n(\n    GT(pl.concat([mini_total, grand_summary_row]))\n    .fmt_integer(columns=pl.exclude(\"Author\"))\n).as_latex()\nSteps to add the table:\n\nCopy the LaTeX output from the notebook cell\nPaste into new table file (tables/hape_composition.tex)\nAdd caption and label:\n\\begin{table}[!t]\n\\caption{Composition of the HAP-E mini corpus by author type.}\n\\label{tbl:hape-composition}\n% ...rest of your table code...\n\\end{table}\nReference in main.tex:\n\nFind the Data section\nReplace \\input{tables/corpus_composition} with \\input{tables/hape_composition}\nUpdate the reference: Table~\\ref{tbl:hape-composition} shows...\n\n\n\n\n\n\n\n\nCommon LaTeX Table Issues\n\n\n\n\nOverfull boxes: If your table is too wide, the compiler will warn you. Consider reducing font size with \\footnotesize before \\begin{tabular}\nColumn alignment: l = left, r = right, c = center\nSpecial characters: Use \\% for percent signs, \\_ for underscores\n\n\n\n\n\nAdding Your Visualization\nPreparing your figure:\n\nSave from Colab with appropriate filename:\nfig.savefig('gpt_vs_human_keyness.png', \n            bbox_inches='tight', \n            dpi=300)  # High resolution for publication\nDownload from Google Drive\nUpload to Overleaf:\n\nNavigate to figures/ folder\nClick â€œUploadâ€ icon\nSelect your PNG file\n\nAdd to document:\n\\begin{figure}[!t]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{figures/gpt_vs_human_keyness.png}\n    \\caption{DocuScope categories showing highest effect sizes \n    (LR &gt; 0.5) distinguishing ChatGPT-generated text from \n    human-written text in the HAP-E corpus.}\n    \\label{fig:keyness}\n\\end{figure}\nReference the figure: As shown in Figure~\\ref{fig:keyness}, the Positive category...",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #2: From Results to Research"
    ]
  },
  {
    "objectID": "reports/cbe_02.html#part-3-from-data-to-interpretation",
    "href": "reports/cbe_02.html#part-3-from-data-to-interpretation",
    "title": "Coffee Break Experiment #2: From Results to Research",
    "section": "Part 3: From Data to Interpretation",
    "text": "Part 3: From Data to Interpretation\n\nWriting the Results Section\nThe Results section reports what you foundâ€”keep it brief and descriptive:\nWhat to include:\n\nDirect description of patterns:\n\nâ€œChatGPT-generated texts used Positive language 5.2 times per 100 tokens compared to 3.1 in human texts (LR = 0.67, LL = 423.5, p &lt; 0.001)â€\nReport actual numbers from your tables\nInclude effect sizes (LR) and significance (LL, p-values)\n\nReference your visualizations:\n\nâ€œFigure 1 shows the five DocuScope categories with the largest effect sizesâ€¦â€\nGuide readers through what theyâ€™re seeing\n\nAvoid interpretation (save that for Discussion):\n\nâŒ â€œThis suggests ChatGPT is overly optimisticâ€\nâœ… â€œChatGPT texts contain 67% more Positive languageâ€\n\n\nQuick example (yours doesnâ€™t need to be this polished):\n\nWe analyzed 150 texts from the HAP-E corpus (see Table 1). Figure 2 shows the five DocuScope categories that most strongly distinguish ChatGPT from human writing (LR &gt; 0.5). The Positive category shows the largest difference: ChatGPT uses it 5.2 times per 100 tokens versus 3.1 in human texts (LR = 0.67, p &lt; 0.001).\n\n\n\nDeveloping Your Discussion\nThis is the exploratory heart of the CBE. The Discussion is where you: - Speculate about what the patterns might mean - Propose provisional explanations (even if you canâ€™t fully test them) - Sketch what youâ€™d investigate next\n\n\n\n\n\n\nThink Out Loud\n\n\n\nDonâ€™t worry about having a complete argument. Coffee break experiments are about exploration: - What caught your attention in the data? - What might explain it? - What questions does it raise? - If you had more time, what would you do next?\n\n\nQuick framework for provisional interpretation:\n\nWhat did you notice?\n\nLook at your KWIC concordancesâ€”what patterns jumped out?\nExample: â€œresilience,â€ â€œopportunity,â€ â€œbeneficialâ€ cluster together\n\nWhy might this be happening?\n\nBrainstorm possibilities (training data? instruction-following? genre conventions?)\nYou donâ€™t need definitive answersâ€”provisional explanations are fine\n\nWhat would you do next?\n\nIf this were a full project, what would you investigate?\nMore data? Different comparisons? Qualitative analysis?\n\nWhat didnâ€™t work?\n\nItâ€™s okay if your initial approach didnâ€™t pan out\nExplain what you tried and why it didnâ€™t reveal what you expected\n\n\nExample Discussion paragraph (exploratory tone):\n\nWhat caught our attention was how often ChatGPT uses words like â€œresilience,â€ â€œopportunity,â€ and â€œbeneficialâ€â€”always in optimistic contexts. Looking at the concordances, ChatGPT seems to avoid hedging or acknowledging limitations, which is common in human academic writing. Why? Maybe itâ€™s trained on motivational content, or maybe when you prompt it to â€œexplainâ€ something, it defaults to positive framing. Weâ€™re not sure, but it makes us wonder about using AI for academic writingâ€”students might need to consciously add critical perspective. If we had more time, weâ€™d compare different prompts (â€œcritiqueâ€ vs.Â â€œexplainâ€) to see if instruction affects tone.\n\n\n\nSketching Your Exploratory Path\nYour CBE should tell a story of discovery, not present a finished argument:\nIntroduction â†’ What made you curious?\nData â†’ What did you look at?\nMethods â†’ What did you try?\nResults â†’ What did you find?\nDiscussion â†’ What might it mean? Whatâ€™s next?\nExample discovery path:\n\nCuriosity: We wondered if AI writing sounds different from human writing\nData: Used HAP-E corpus to compare\nMethod: Tried keyness analysis to find distinctive features\nFinding: ChatGPT uses way more â€œPositiveâ€ language\nSpeculation: Maybe itâ€™s too optimistic? Lacks critical stance?\nNext steps: Would test different prompts, look at other AI models, read more about training data\n\n\n\n\n\n\n\nEmbrace the Provisional\n\n\n\nItâ€™s okay to say: - â€œWeâ€™re not sure, butâ€¦â€\n- â€œThis might suggestâ€¦â€\n- â€œIf we had more time, weâ€™dâ€¦â€\n- â€œThis didnâ€™t work as expected becauseâ€¦â€\nCBEs are about the process of exploration, not the final answer.",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #2: From Results to Research"
    ]
  },
  {
    "objectID": "reports/cbe_02.html#part-4-citation-and-academic-integrity",
    "href": "reports/cbe_02.html#part-4-citation-and-academic-integrity",
    "title": "Coffee Break Experiment #2: From Results to Research",
    "section": "Part 4: Citation and Academic Integrity",
    "text": "Part 4: Citation and Academic Integrity\n\nAdding Sources to Your Bibliography\nThe template includes main.bib with sample citations. Add your own sources:\nBibTeX format:\n@article{uniquekey2024,\n  title={Article Title},\n  author={Last, First and Last, First},\n  journal={Journal Name},\n  volume={10},\n  number={2},\n  pages={100--125},\n  year={2024},\n  publisher={Publisher Name}\n}\nFinding BibTeX citations:\n\nGoogle Scholar: Click â€œCiteâ€ â†’ â€œBibTeXâ€\nZotero: Right-click item â†’ Export â†’ BibTeX\n\nCiting in text:\n\n\\cite{uniquekey2024} â†’ â€œ(Last, 2024)â€\n\\citet{uniquekey2024} â†’ â€œLast (2024)â€ (with natbib package)\n\n\n\nUsing Generative AI Responsibly\nThe template includes an Acknowledgments section for disclosing AI use. Be specific:\nAcceptable disclosure: &gt; â€œWe used ChatGPT-4 (OpenAI) to generate initial drafts of the Introduction and Methods sections, which we then substantially revised for accuracy and clarity. We also used it to suggest alternative phrasings for complex statistical descriptions in the Results section. We found it helpful for overcoming initial drafting inertia, but the AI-generated prose often lacked discipline-specific precision and required significant editing.â€\nWhat to disclose:\n\nWhich AI tool(s) you used (name and version)\nWhat specific tasks (drafting, revising, paraphrasing)\nHow you modified the output\nYour evaluation of its usefulness",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #2: From Results to Research"
    ]
  },
  {
    "objectID": "reports/cbe_02.html#part-5-team-workflow-and-collaboration",
    "href": "reports/cbe_02.html#part-5-team-workflow-and-collaboration",
    "title": "Coffee Break Experiment #2: From Results to Research",
    "section": "Part 5: Team Workflow and Collaboration",
    "text": "Part 5: Team Workflow and Collaboration\n\nDivision of Labor\nAssign clear responsibilities:\nManager: - Creates and shares project - Manages final compilation - Ensures all sections connect\nData Analyst: - Runs Mini Lab 5 analyses - Generates tables and figures - Writes Methods section\nWriter 1: - Introduction and Data sections - Bibliography management\nWriter 2: - Results and Discussion sections - Caption writing\n\n\n\n\n\n\nEveryone Reviews Everything\n\n\n\nEven with assigned sections, all team members should read and comment on the complete draft. Fresh eyes catch inconsistencies and unclear explanations.\n\n\n\n\nOverleaf Collaboration Features\nTrack Changes: - Review â†’ Track Changes â†’ On - All edits appear highlighted - Accept/reject individual changes\nComments: - Highlight text â†’ Comment icon - Use for questions, suggestions, citations to add\nChat: - Bottom left icon - For quick coordination - Not preserved in final document\nVersion History: - Clock icon (top right) - See all changes over time - Revert to earlier versions if needed\n\n\nQuick Checklist\nBefore submitting (donâ€™t obsess over perfection):\n\nNames and title updated\nYour table and figure are included\nPlaceholder â€œLorem ipsumâ€ text replaced with your writing\nPDF compiles and downloads\nAll team members have read it\n\n\n\n\n\n\n\nGood Enough Is Good Enough\n\n\n\nIf a table is slightly too wide, or you have a citation warning, or your figure caption could be betterâ€”thatâ€™s fine. Focus your limited time on thinking through your interpretation, not perfecting LaTeX formatting.",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #2: From Results to Research"
    ]
  },
  {
    "objectID": "reports/cbe_02.html#reflection-questions",
    "href": "reports/cbe_02.html#reflection-questions",
    "title": "Coffee Break Experiment #2: From Results to Research",
    "section": "Reflection Questions",
    "text": "Reflection Questions\nAfter completing CBE #2, consider:\n\nDiscovery: What surprised you most in your data? Did your initial expectations match what you found?\nInterpretation: How did you move from â€œChatGPT uses more Positive languageâ€ to â€œthis might mean Xâ€? What made that leap difficult or easy?\nNext steps: If you had another week, what would you investigate next? What questions remain unanswered?\nLimitations: What didnâ€™t work as planned? What would you change about your approach?\nCollaborative exploration: How did working as a team help (or hinder) the exploratory process? Did you generate ideas together or divide and conquer?",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #2: From Results to Research"
    ]
  },
  {
    "objectID": "reports/cbe_02.html#works-cited",
    "href": "reports/cbe_02.html#works-cited",
    "title": "Coffee Break Experiment #2: From Results to Research",
    "section": "Works Cited",
    "text": "Works Cited",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #2: From Results to Research"
    ]
  },
  {
    "objectID": "reports/cbe_03.html",
    "href": "reports/cbe_03.html",
    "title": "Coffee Break Experiment #3: Final Project Pilot Study",
    "section": "",
    "text": "Part 1: Approaching CBE #3 as a Pilot Study",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #3: Final Project Pilot Study"
    ]
  },
  {
    "objectID": "reports/cbe_03.html#part-1-approaching-cbe-3-as-a-pilot-study",
    "href": "reports/cbe_03.html#part-1-approaching-cbe-3-as-a-pilot-study",
    "title": "Coffee Break Experiment #3: Final Project Pilot Study",
    "section": "",
    "text": "Two Strategic Approaches\nOption 1: Preliminary Exploration of Final Project Data\nAnalyze a subset of your final project corpus using simpler methods:\nExample: - Final project: Topic modeling 500 congressional speeches (1990-2020) to track political discourse evolution - CBE #3 pilot: Analyze 30 speeches (5 per decade) using basic frequency analysis and keyness to identify distinctive vocabulary by era, confirming your corpus shows temporal variation before investing in full topic modeling\nWhy this works: Tests data collection, validates that patterns exist, identifies preprocessing needs\n\nOption 2: Full Pipeline on Small Sample\nTest your complete analytical workflow on a small corpus before scaling up:\nExample: - Final project: Multi-Dimensional Analysis of 1,000 19th-century novels to identify register differences across publishers - CBE #3 pilot: Process 10 novels (2 per publisher) through spaCy â†’ pybiber â†’ MDA to test pipeline, check processing time, ensure features extract correctly, see if preliminary patterns emerge\nWhy this works: Debugs technical workflow, estimates computational requirements, reveals whether your approach is viable\n\n\n\n\n\n\n\nChoose Based on Your Uncertainty\n\n\n\nUncertain about data quality or patterns? â†’ Option 1 (simple methods on small sample)\nUncertain about technical pipeline? â†’ Option 2 (full workflow on tiny corpus)\nUncertain about both? â†’ Option 1, then scale to Option 2 for final project\n\n\n\n\nWhat Makes a Good Pilot Study?\nGood pilots answer specific questions:\nâœ… â€œDoes my corpus actually show the variation I expect?â€ (test with keyness, frequency comparisons)\nâœ… â€œCan I successfully process texts with this tool?â€ (test pipeline on 5-10 documents)\nâœ… â€œDo preliminary patterns suggest my hypothesis is worth pursuing?â€ (exploratory visualization)\nâœ… â€œHow long will processing take for the full corpus?â€ (time 10 documents, extrapolate to 1000)\nPoor pilots try to do everything:\nâŒ Attempting definitive analysis with insufficient data (3 texts canâ€™t support topic modeling)\nâŒ Using complex methods before verifying basic patterns (MDA on data you havenâ€™t checked for variation)\nâŒ Perfect polished prose instead of learning from failures\n\n\nScoping Your Pilot Appropriately\nCorpus size guidelines:\n\n\n\nMethod\nMinimum for CBE #3\nIdeal for Final Project\n\n\n\n\nFrequency/Distributions\n10-20 texts\n100+ texts\n\n\nKeyness\n15+ per group (30 total)\n50+ per group\n\n\nCollocations\n20-30 texts\n100+ texts\n\n\nSentiment/Syuzhet\n5-10 narratives\n50+ narratives\n\n\nTopic Modeling\n50+ texts (bare minimum)\n200+ texts\n\n\nMDA (Factor Analysis)\n30+ texts (risky)\n100+ texts\n\n\nClassification\n20+ per class (40 total)\n100+ per class\n\n\nClustering\n20-30 texts\n100+ texts\n\n\nTime Series\n15+ time points\n30+ time points\n\n\nVector Models\n30+ texts\n200+ texts\n\n\n\nFor CBE #3: Use the minimum or even smaller (to test feasibility). For final project, aim for ideal range.",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #3: Final Project Pilot Study"
    ]
  },
  {
    "objectID": "reports/cbe_03.html#part-2-connecting-to-course-materials",
    "href": "reports/cbe_03.html#part-2-connecting-to-course-materials",
    "title": "Coffee Break Experiment #3: Final Project Pilot Study",
    "section": "Part 2: Connecting to Course Materials",
    "text": "Part 2: Connecting to Course Materials\nYou now have access to the full repertoire of tutorials and mini labs. Hereâ€™s how to leverage them for CBE #3:\n\nCore Analysis Methods\nBasic Text Statistics (Tutorial: Corpus Basics) - Good for: Initial corpus exploration, validating data structure - Pilot use: Check text lengths, vocabulary sizes, basic distributions before committing to complex analysis - Mini Lab 2: Practice with Brown corpus subsetting\nFrequency and Distributions (Tutorial: Frequency) - Good for: Identifying salient features, comparing genres/authors - Pilot use: See if hypothesized differences actually exist in data - Mini Lab 3: Frequency calculations, dispersion measures\nKeyness Analysis (Tutorial: Keyness) - Good for: Finding distinctive features between corpora, hypothesis generation - Pilot use: Quick test of whether two categories differ linguistically - Mini Labs 4-5: Log-likelihood, effect sizes, visualization\nCollocations (Tutorial: Collocations) - Good for: Local co-occurrence patterns, phrase identification - Pilot use: Test if target keywords have interesting collocational patterns worth pursuing - Mini Lab 6: MI scores, t-scores, cluster extraction\nTime Series (Tutorial: Time Series) - Good for: Diachronic change, periodization - Pilot use: Check if temporal variation exists before committing to full historical analysis - Mini Lab 7: Change-point detection, VNC clustering\nspaCy Processing (Tutorial: spaCy Basics) - Good for: Linguistic annotation (POS, dependencies, NER) - Pilot use: Test processing pipeline, check annotation quality on sample before scaling - Mini Lab 8: Custom pipelines, token filtering\nSentiment and Narrative Analysis (Tutorial: Sentiment/Syuzhet) - Good for: Emotional arcs, plot structures - Pilot use: Test if narrative shapes emerge in small sample - Mini Labs 1, 11-12: Sentiment lexicons, syuzhet curves\n\n\nAdvanced Methods\nTopic Modeling (Tutorial: Topic Modeling) - Good for: Discovering thematic structure in large corpora - Pilot use: Risky for CBE #3 unless you have 50+ texts; consider deferring to final project - Mini Lab 9: LDA, coherence, interpretation\nVector Models (Word2Vec, Embeddings) (Tutorial: Vector Models, Contextual Embeddings) - Good for: Semantic similarity, meaning in context - Pilot use: Test if semantic clustering reveals patterns; requires moderate corpus size - Mini Labs 9, 11: word2vec, sentence transformers, similarity\nMulti-Dimensional Analysis (Tutorial: MDA) - Good for: Identifying register dimensions, functional variation - Pilot use: Risky for CBE #3 with &lt; 30 texts; test feature extraction pipeline instead - Mini Lab 10: Factor analysis, dimension interpretation\nClassification/Authorship (Tutorial: Classification) - Good for: Genre prediction, authorship attribution, stylistics - Pilot use: Test if features distinguish categories before building full classifier - Mini Lab 12: Random Forest, feature importance\n\n\nSupporting Concepts\nClustering (Tutorial: Clustering) - Hierarchical clustering for exploratory grouping - K-means for thematic discovery - Pilot use: See if texts naturally cluster by hypothesized categories\nCorrelation and Multi-Collinearity (Tutorial: Correlations) - Understanding feature relationships - Feature selection for classification/regression - Pilot use: Check which features co-occur before MDA or classification\nANOVA and RÂ² (Tutorial: ANOVA) - Testing if groups differ on dimensions - Quantifying variance explained - Pilot use: Validate that extracted dimensions distinguish your categories\n\n\nChoosing Your Method(s)\nDecision tree for CBE #3:\nDo you have clear categories to compare?\nâ”œâ”€ YES â†’ Keyness (quick) or Classification (if testing pipeline)\nâ””â”€ NO â†’ Do you expect temporal patterns?\n    â”œâ”€ YES â†’ Time Series or Frequency over time\n    â””â”€ NO â†’ Do you want to discover themes?\n        â”œâ”€ YES â†’ Topic Modeling (if n &gt; 50) or Clustering\n        â””â”€ NO â†’ Do you want semantic relationships?\n            â”œâ”€ YES â†’ Vector Models or Embeddings\n            â””â”€ NO â†’ Start with basic Frequency/Distributions\n\n\n\n\n\n\nStart Simple, Add Complexity\n\n\n\nEven if your final project will use MDA or Topic Modeling, consider starting CBE #3 with: 1. Corpus exploration (distributions, word counts) 2. Basic comparison (keyness if two groups, frequency if one) 3. One targeted method (the one most central to your final project)\nThis ensures you learn something even if the advanced method doesnâ€™t work perfectly.",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #3: Final Project Pilot Study"
    ]
  },
  {
    "objectID": "reports/cbe_03.html#part-3-data-collection-and-processing",
    "href": "reports/cbe_03.html#part-3-data-collection-and-processing",
    "title": "Coffee Break Experiment #3: Final Project Pilot Study",
    "section": "Part 3: Data Collection and Processing",
    "text": "Part 3: Data Collection and Processing\n\nFinding and Preparing Your Data\nData sources (see Data Resources):\n\nProject Gutenberg: Historical literature (out of copyright)\nArchive.org: Digitized texts, speeches, periodicals\nCorpora: Brown, inaugural addresses, MICUSP (academic writing)\nWeb scraping: News articles, blogs, social media (with ethical considerations)\nCustom collection: Interviews, student writing, archival materials\n\nProcessing pipeline checklist:\n\nLoad data into polars DataFrame\nimport polars as pl\ncorpus = pl.DataFrame({\n    'doc_id': [...],\n    'text': [...],\n    'metadata': [...]  # genre, date, author, etc.\n})\nClean text (if needed)\n\nRemove headers/footers (Project Gutenberg boilerplate)\nHandle encoding issues\nSee Corpus Basics tutorial\n\nAnnotate (if using spaCy/pybiber)\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n# See Mini Lab 8, spaCy tutorial\nCreate document-feature matrix\n\nWord counts, POS tags, DocuScope categories, or Biber features\nSee relevant tutorial for your chosen method\n\n\n\n\n\n\n\n\nStart Small, Verify Quality\n\n\n\nProcess 2-3 texts first, manually inspect output: - Do POS tags look correct? - Are sentences segmented properly? - Is metadata aligned with text?\nCatching errors early saves hours of debugging later.\n\n\n\n\nDocumenting Your Pipeline\nIn your Methods section, explain:\n\nData source: â€œWe collected 15 Victorian novels from Project Gutenberg (1850-1900)â€\nSelection criteria: â€œWe selected novels by women authors with &gt; 50,000 wordsâ€\nProcessing steps: â€œTexts were parsed with spaCy en_core_web_sm, extracting POS tags and dependency relationsâ€\nFeature extraction: â€œWe computed normalized frequencies for 67 Biber features using pybiber v0.5â€\nQuality checks: â€œWe manually verified POS tagging accuracy on 3 sample textsâ€\n\nWhy documentation matters: If something doesnâ€™t work, you (and your team) can trace where it went wrong.",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #3: Final Project Pilot Study"
    ]
  },
  {
    "objectID": "reports/cbe_03.html#part-4-exploratory-analysis-and-iteration",
    "href": "reports/cbe_03.html#part-4-exploratory-analysis-and-iteration",
    "title": "Coffee Break Experiment #3: Final Project Pilot Study",
    "section": "Part 4: Exploratory Analysis and Iteration",
    "text": "Part 4: Exploratory Analysis and Iteration\n\nEmbrace the Pilot Mindset\nCBE #3 is not about perfectionâ€”itâ€™s about learning what works:\nâœ… â€œWe tried clustering 10 novels, but they didnâ€™t group by author as expected. Might need more texts or different features.â€\nâœ… â€œKeyness revealed that Author A uses significantly more abstract nounsâ€”worth investigating with MDA in final project.â€\nâœ… â€œProcessing 10 texts took 2 hours. Scaling to 500 will require parallelization.â€\nâŒ â€œWe got perfect results and everything worked flawlessly.â€ (Suspiciously polished for a pilot)\n\n\nIteration Examples\nIteration 1: Initial hypothesis fails - Tried: Topic modeling 30 blog posts - Result: Incoherent topics (too few documents) - Pivot: Switched to keyness comparison (political blogs vs.Â personal blogs)â€”worked better - Learning: Small corpus â†’ simpler methods\nIteration 2: Data quality issues - Tried: Sentiment analysis on scraped tweets - Result: Many non-English tweets corrupted results - Pivot: Added language detection, filtered to English only - Learning: Always validate data quality assumptions\nIteration 3: Pipeline too slow - Tried: Processing 50 novels through spaCy + pybiber - Result: Took 6 hours (not scalable to 500) - Pivot: Tested n_process=4 (parallel), reduced to 1.5 hours - Learning: Test computational requirements early\n\n\n\n\n\n\nDocument What Didnâ€™t Work\n\n\n\nYour Discussion section should honestly report: - What you tried that failed - Why you think it failed - What you learned - How youâ€™d adjust for the final project\nFailed experiments teach more than successful ones.\n\n\n\n\nGenerating Provisional Findings\nEven with a small pilot, you can learn something:\nFrom 15 texts (too small for topic modeling): - âœ… Keyness shows distinctive vocabulary - âœ… Frequency distributions reveal stylistic differences - âœ… Correlation analysis shows which features co-occur - âŒ Topic modeling (too few documents) - âŒ MDA (underpowered)\nFrom 30 texts (borderline for complex methods): - âœ… Clustering might work (exploratory) - âœ… Sentiment trends visible - âœ… Classification possible (if 15+ per class) - âš ï¸ Topic modeling (risky, check coherence) - âš ï¸ MDA (check KMO, Bartlettâ€™s test)\nFrom 50+ texts (most methods viable): - âœ… Topic modeling - âœ… MDA (if features are appropriate) - âœ… Robust classification - âœ… Vector models (if corpus is cohesive)",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #3: Final Project Pilot Study"
    ]
  },
  {
    "objectID": "reports/cbe_03.html#part-5-from-pilot-to-final-project",
    "href": "reports/cbe_03.html#part-5-from-pilot-to-final-project",
    "title": "Coffee Break Experiment #3: Final Project Pilot Study",
    "section": "Part 5: From Pilot to Final Project",
    "text": "Part 5: From Pilot to Final Project\n\nUsing CBE #3 to Refine Your Research Question\nBefore CBE #3: â€œI want to analyze gender differences in 19th-century novelsâ€\nAfter CBE #3: â€œPilot showed female authors use more emotion words (keyness LR &gt; 0.6). Final project will test if this holds across 200 novels using MDA to identify broader stylistic dimensions beyond single features.â€\nHow the pilot refines the question:\n\nValidates basic premise (yes, gender differences exist in this corpus)\nIdentifies specific patterns (emotion words, not just abstract â€œstyleâ€)\nSuggests appropriate methods (MDA better than simple frequency for multidimensional patterns)\nScopes the project (200 novels feasible, 1000 not necessary)\n\n\n\nTranslating Pilot Findings to Final Project Plan\nIn your CBE #3 Discussion, sketch the path forward:\nExample 1: Scaling up &gt; â€œOur pilot analyzed 20 news articles (10 print, 10 online) using keyness. We found online news uses 40% more interactive language (questions, second-person pronouns). For the final project, weâ€™ll expand to 200 articles (100 per medium) and use MDA to test if â€˜interactive vs.Â informationalâ€™ forms a coherent dimension beyond these individual features. Weâ€™ll also add a temporal component (2000, 2010, 2020) to see if convergence is occurring.â€\nExample 2: Pivoting method &gt; â€œWe attempted topic modeling on 30 congressional speeches but topics were incoherent (perplexity = 850, coherence = 0.32). For the final project, weâ€™ll collect 150 speeches and use clustering + keyness instead: cluster speeches by similarity, then identify distinctive vocabulary for each cluster. This better suits our corpus size and provides more interpretable results.â€\nExample 3: Refining focus &gt; â€œWe extracted 67 Biber features for 15 academic papers across 3 disciplines. Factor analysis failed (KMO = 0.51, insufficient correlations). Examining individual features, we noticed â€˜nominalizationsâ€™ and â€˜passivesâ€™ alone distinguish STEM from humanities (LR &gt; 0.5). Final project will focus on these 10 key features with targeted analysis (ANOVA, regression) rather than full MDA, using 100 papers per discipline.â€\n\n\nTechnical Lessons Learned\nDocument in your Discussion:\nData collection: - â€œScraping took longer than expected due to rate limitsâ€”build in buffer timeâ€ - â€œ10% of texts had encoding errorsâ€”add validation stepâ€ - â€œMetadata was inconsistentâ€”need standardized schemaâ€\nProcessing: - â€œspaCy sentence segmentation failed on dialogâ€”consider custom rulesâ€ - â€œpybiber dropped 12 features due to low correlationâ€”expected, not problematicâ€ - â€œProcessing 30 texts took 45 minutesâ€”final 200 will take ~5 hours (acceptable)â€\nAnalysis: - â€œTopic coherence improved when we increased n_topics from 5 to 8â€ - â€œClustering worked better with TF-IDF than raw countsâ€ - â€œDimension 1 explained 35% varianceâ€”strong enough to pursueâ€\n\n\n\n\n\n\nThink of CBE #3 as a Feasibility Study\n\n\n\nYour Discussion should answer: - Is this project doable? (data accessible, methods viable) - Is it interesting? (patterns exist, not just noise) - What adjustments are needed? (more data, different method, refined question) - Whatâ€™s the timeline? (realistic estimate based on pilot)",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #3: Final Project Pilot Study"
    ]
  },
  {
    "objectID": "reports/cbe_03.html#part-6-writing-the-report",
    "href": "reports/cbe_03.html#part-6-writing-the-report",
    "title": "Coffee Break Experiment #3: Final Project Pilot Study",
    "section": "Part 6: Writing the Report",
    "text": "Part 6: Writing the Report\n\nStructuring CBE #3 for Maximum Learning\nIntroduction (1-2 paragraphs): - Research question: What do you want to explore? - Rationale: Why is this interesting/important? - Pilot scope: What are you testing in CBE #3 specifically?\nExample: &gt; We investigate whether computational metaphor identification can distinguish academic from popular science writing. Prior work suggests experts use more abstract metaphors (Lakoff & Johnson, 1980), but computational validation is limited. This pilot tests whether spaCyâ€™s dependency parsing can reliably extract metaphor candidates, using 10 texts (5 academic, 5 popular) to assess feasibility before scaling to 200 texts for the final project.\n\nData (1 paragraph + table): - Source: Where did data come from? - Selection: How did you choose texts? - Composition: Table showing corpus breakdown\nExample: &gt; We collected 15 Victorian novels from Project Gutenberg, selecting 5 each by Austen, BrontÃ«, and Eliot to test author differentiation. Table 1 shows composition by author and decade. All novels exceeded 100,000 tokens.\n\nMethods (1-2 paragraphs): - Processing pipeline: spaCy â†’ feature extraction - Features/Analysis: Which features, which statistical tests - Tools: Polars, pybiber, sklearn, etc.\nExample: &gt; Texts were parsed with spaCy en_core_web_sm. We extracted 67 Biber features using pybiber v0.5, normalized per 1,000 words. Due to small sample size (n=15), we used keyness analysis (log-likelihood, effect sizes) to compare authors rather than MDA, which requires 30+ texts for stable factors.\n\nResults (1-2 paragraphs + 1-2 visualizations): - What did you find? (report numbers, cite tables/figures) - No interpretation yetâ€”save for Discussion\nExample: &gt; Figure 1 shows the 10 linguistic features with largest effect sizes distinguishing Austen from BrontÃ« (LR &gt; 0.4). Austen uses significantly more first-person pronouns (5.2 vs.Â 3.1 per 100 tokens, LR = 0.52, p &lt; 0.001) and present tense verbs (8.7 vs.Â 6.1, LR = 0.48, p &lt; 0.001). BrontÃ« shows higher rates of past tense (12.3 vs.Â 9.4, LR = 0.44, p &lt; 0.01).\n\nDiscussion (2-3 paragraphsâ€”the heart of CBE #3):\n\nInterpretation of pilot findings:\n\nWhat might these patterns mean?\nProvisional explanations (itâ€™s okay to speculate)\n\nLimitations and lessons learned:\n\nWhat didnâ€™t work as expected?\nTechnical challenges encountered\nWhat would you do differently?\n\nPath to final project:\n\nHow will you scale up?\nWhat adjustments to methods/questions?\nWhat additional analyses will you add?\n\n\nExample (combining all three): &gt; The pilot revealed that Austenâ€™s high use of first-person pronouns and present tense likely reflects her narrative styleâ€”free indirect discourse bringing readers into charactersâ€™ immediate consciousness (Moretti, 2013). BrontÃ«â€™s past-tense dominance suggests more traditional retrospective narration. However, our sample of 5 novels per author is too small to confidently generalize. For the final project, weâ€™ll expand to 50 novels across 10 Victorian authors and use Multi-Dimensional Analysis to test if â€œnarrative immediacyâ€ (pronouns + present tense) forms a coherent dimension distinguishing authors beyond these individual features. Weâ€™ll also add temporal metadata to test if narrative style evolved across the century. Technically, we learned that pybiber processing is efficient (15 novels in 20 minutes), but we need to manually verify proper noun vs.Â common noun distinctions, as spaCy occasionally misclassifies character names.\n\nConclusion (optional for CBE, but useful): - Brief summary of pilot outcomes - Reaffirm feasibility and next steps\n\nAcknowledgments: - Generative AI use (if anyâ€”be specific) - Team member contributions - Data sources\n\n\nTables and Figures\nMinimum required:\n\nTable 1: Corpus composition (from Mini Lab 5 approach)\nFigure 1: Main result visualization (keyness plot, frequency comparison, etc.)\n\nOptional additions:\n\nTable 2: Descriptive statistics (means, SDs by group)\nFigure 2: Secondary finding (correlation heatmap, cluster dendrogram, etc.)\n\n\n\n\n\n\n\nQuality Over Quantity\n\n\n\nTwo well-explained visualizations &gt; five poorly labeled ones.\nEach figure/table should: - Have a descriptive caption - Be referenced in text - Add substantive information (not decorative)\n\n\n\n\nCitations and Bibliography\nMinimum citations:\n\nMethods: Cite software packages (spaCy, polars, pybiber)\nTheory: 1-2 sources establishing context (prior research, linguistic theory)\nData: If using published corpus, cite it\n\nFinding relevant citations:\n\nTutorials: Check â€œFurther Readingâ€ sections for foundational sources\nPackages: Most have citation guidance (e.g., pybiber documentation)\nGoogle Scholar: Search your topic + â€œcorpus linguisticsâ€ or â€œcomputational text analysisâ€\n\nBibTeX from course materials:\n@article{biber1988variation,\n  title={Variation across speech and writing},\n  author={Biber, Douglas},\n  year={1988},\n  publisher={Cambridge University Press}\n}",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #3: Final Project Pilot Study"
    ]
  },
  {
    "objectID": "reports/cbe_03.html#part-7-team-collaboration-strategies",
    "href": "reports/cbe_03.html#part-7-team-collaboration-strategies",
    "title": "Coffee Break Experiment #3: Final Project Pilot Study",
    "section": "Part 7: Team Collaboration Strategies",
    "text": "Part 7: Team Collaboration Strategies\n\nDividing Pilot Study Work\nWeek 1: Setup and data collection - Manager: Creates Overleaf project, assigns roles - Data collector: Gathers texts, creates corpus DataFrame - Technical lead: Tests processing pipeline on 2-3 sample texts\nWeek 2: Analysis and writing - Analyst 1: Runs main analysis, generates tables/figures - Analyst 2: Runs secondary analysis or validates results - Writer 1: Drafts Introduction and Methods - Writer 2: Drafts Results and Discussion\nWeek 3: Revision and finalization - Everyone: Reviews complete draft, adds comments - Manager: Integrates revisions, ensures coherence - All: Final proofreading and compilation\n\n\n\n\n\n\nPair Programming for Pilots\n\n\n\nConsider working in pairs for technical components: - One person writes code, other reviews in real-time - Switch roles every 30 minutes - Catches errors early, shares knowledge\n\n\n\n\nManaging Iteration and Pivots\nIf your initial plan isnâ€™t working:\n\nTeam meeting (virtual or in-person): Whatâ€™s the problem?\nQuick brainstorm (15 minutes): What are alternatives?\nDecision: Pick one pivot, commit for 24 hours\nReassess: Did the pivot work?\n\nExample pivot: - Original plan: Topic modeling 30 speeches - Problem: Topics incoherent (perplexity too high) - Pivot: Switch to clustering + keyness - Decision point: 24 hours later, check if clusters are interpretable - Outcome: Clusters work, proceed with this method\nDocument the pivot in your Discussionâ€”itâ€™s valuable learning, not failure.",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #3: Final Project Pilot Study"
    ]
  },
  {
    "objectID": "reports/cbe_03.html#part-8-from-cbe-3-to-final-project-proposal",
    "href": "reports/cbe_03.html#part-8-from-cbe-3-to-final-project-proposal",
    "title": "Coffee Break Experiment #3: Final Project Pilot Study",
    "section": "Part 8: From CBE #3 to Final Project Proposal",
    "text": "Part 8: From CBE #3 to Final Project Proposal\n\nUsing CBE #3 to Write Your Proposal\nYour final project proposal should build directly on CBE #3 findings:\nResearch Question (refined): - Before CBE #3: â€œI want to study gender in novelsâ€ - After CBE #3: â€œBuilding on pilot findings that female authors use 40% more emotion words (LR = 0.58), I will test if this pattern holds across 200 Victorian novels and whether it correlates with critical reception.â€\nMethods (validated): - Before: â€œIâ€™ll probably use topic modeling or somethingâ€ - After: â€œPilot showed MDA is computationally feasible (30 novels in 45 minutes) and reveals interpretable dimensions. Iâ€™ll use MDA with 67 Biber features, validated by pilotâ€™s successful extraction and preliminary factor structure (KMO = 0.72).â€\nFeasibility (demonstrated): - Before: â€œI assume I can find the data somewhereâ€ - After: â€œPilot confirmed Project Gutenberg has sufficient texts (identified 250 candidate novels). Processing pipeline tested and working. Estimated timeline: data collection (2 weeks), processing (3 days), analysis (1 week).â€\nExpected Outcomes (grounded): - Before: â€œI expect to find differencesâ€ - After: â€œPilot suggested Dimension 1 (emotional vs.Â restrained) will distinguish female from male authors (Î·Â² â‰ˆ 0.25 in pilot). Full project will test this with adequate power (200 novels) and add temporal analysis (1800-1900) to see if gender patterns evolved.â€\n\n\nTimeline for Final Project\nBased on CBE #3 pilot lessons:\nWeek 1-2: Data collection (adjusted based on pilot challenges) Week 3: Processing (using validated pipeline from pilot) Week 4: Analysis (main method tested in pilot, plus extensions) Week 5: Writing (structure similar to CBE #3, but more polished) Week 6: Revision and finalization\n\n\n\n\n\n\nBuild Buffer Time\n\n\n\nIf pilot processing took 2 hours for 30 texts, estimate 20 hours for 300 texts (not 20 hours exactlyâ€”add 25% buffer for unexpected issues).\nIf pilot analysis revealed data quality problems in 10% of texts, expect 10% of full corpus will need manual cleaning.",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #3: Final Project Pilot Study"
    ]
  },
  {
    "objectID": "reports/cbe_03.html#reflection-questions",
    "href": "reports/cbe_03.html#reflection-questions",
    "title": "Coffee Break Experiment #3: Final Project Pilot Study",
    "section": "Reflection Questions",
    "text": "Reflection Questions\nAfter completing CBE #3, consider:\n\nFeasibility: Is your final project viable given what you learned? Do you need to adjust scope or methods?\nSurprises: What did you learn from the pilot that you didnâ€™t expect? (Data quality issues? Patterns stronger/weaker than anticipated?)\nMethods: Did your chosen analytical method work well for your research question? Would a different approach be better?\nIteration: How many times did you pivot or adjust your approach? What did each iteration teach you?\nCollaboration: How did team dynamics shape the pilot? What worked well? What needs adjustment for the final project?\nTimeline: How accurate were your time estimates? (Data collection, processing, analysis) What will you adjust for the final project?\nFindings: Even if preliminary, did the pilot reveal anything interesting enough to pursue further?",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #3: Final Project Pilot Study"
    ]
  },
  {
    "objectID": "reports/cbe_03.html#final-project-preview-whats-next",
    "href": "reports/cbe_03.html#final-project-preview-whats-next",
    "title": "Coffee Break Experiment #3: Final Project Pilot Study",
    "section": "Final Project Preview: Whatâ€™s Next?",
    "text": "Final Project Preview: Whatâ€™s Next?\nAfter CBE #3, you should have:\nâœ… A tested data collection and processing pipeline\nâœ… Preliminary findings suggesting your research question is viable\nâœ… Realistic understanding of computational requirements\nâœ… Identified technical challenges and solutions\nâœ… Refined research question based on empirical evidence\nâœ… Clear plan for scaling up to full project\nYour final project will:\n\nExpand corpus to sufficient size (based on method requirements)\nAdd depth to analysis (secondary methods, validation, robustness checks)\nProduce polished visualizations and tables\nProvide thorough interpretation grounded in theory\nDemonstrate scholarly rigor in methods and reporting\n\nCBE #3 is your safety netâ€”better to discover problems with 30 texts than 300 texts.",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #3: Final Project Pilot Study"
    ]
  },
  {
    "objectID": "reports/cbe_03.html#works-cited",
    "href": "reports/cbe_03.html#works-cited",
    "title": "Coffee Break Experiment #3: Final Project Pilot Study",
    "section": "Works Cited",
    "text": "Works Cited",
    "crumbs": [
      "Reports",
      "Coffee Break Experiment #3: Final Project Pilot Study"
    ]
  },
  {
    "objectID": "reports/final-project.html",
    "href": "reports/final-project.html",
    "title": "Final Project: Independent Corpus Analysis",
    "section": "",
    "text": "Overview: From Pilot to Publication-Quality\nThe final project represents the culmination of your work this semester. Unlike the exploratory Coffee Break Experiments, this project should demonstrate:\nâœ… Scholarly rigor: Grounded in relevant literature, theoretically motivated\nâœ… Methodological sophistication: Appropriate methods for your question, validated by pilot\nâœ… Sufficient scale: Corpus size adequate for chosen methods (informed by CBE #3)\nâœ… Polished presentation: Publication-quality visualizations, clear writing, professional formatting\nâœ… Critical reflection: Honest assessment of limitations, implications, future directions",
    "crumbs": [
      "Reports",
      "Final Project: Independent Corpus Analysis"
    ]
  },
  {
    "objectID": "reports/final-project.html#overview-from-pilot-to-publication-quality",
    "href": "reports/final-project.html#overview-from-pilot-to-publication-quality",
    "title": "Final Project: Independent Corpus Analysis",
    "section": "",
    "text": "Key Differences: CBE #3 vs.Â Final Project\n\n\n\n\n\n\n\n\nAspect\nCBE #3 (Pilot)\nFinal Project\n\n\n\n\nCorpus size\nMinimum viable (10-50 texts)\nSufficient for method (100-500+ texts)\n\n\nAnalysis depth\nSingle method, exploratory\nMultiple complementary methods\n\n\nLiterature\n1-2 citations for context\n10-15+ sources (grad students more)\n\n\nVisualization\n1-2 quick plots\n3-5 polished, publication-ready figures\n\n\nInterpretation\nProvisional, speculative\nGrounded, nuanced, caveated\n\n\nTone\nâ€œWe tried this andâ€¦â€\nâ€œOur findings suggestâ€¦â€\n\n\nPolish\nGood enough is good enough\nProfessional, submission-ready",
    "crumbs": [
      "Reports",
      "Final Project: Independent Corpus Analysis"
    ]
  },
  {
    "objectID": "reports/final-project.html#part-1-scaling-up-from-cbe-3",
    "href": "reports/final-project.html#part-1-scaling-up-from-cbe-3",
    "title": "Final Project: Independent Corpus Analysis",
    "section": "Part 1: Scaling Up from CBE #3",
    "text": "Part 1: Scaling Up from CBE #3\n\nUsing Your Pilot to Inform the Final Project\nIf you completed CBE #3, you have a blueprint for the final project. Hereâ€™s how to leverage it:\n1. Expand corpus to sufficient size\nYour pilot tested feasibility with a small sample. Now scale to the corpus size your method requires:\n\nCBE #3: 20 texts (10 per genre) tested keyness\nFinal: 200 texts (100 per genre) for robust statistical power, add classification to validate keyness patterns\n\n2. Add complementary methods\nPilot typically used one analytical approach. Final project might use multiple methods that triangulate:\n\nCBE #3: Keyness identified distinctive vocabulary\nFinal: Add (1) classification to test predictive power of key features, (2) collocations to examine how keywords are used contextually, (3) time series if temporal dimension exists\n\n3. Deepen theoretical grounding\nPilot had minimal literature review. Final project needs scholarly context:\n\nCBE #3: â€œPrior work suggests genre differences existâ€\nFinal: 2-3 paragraph literature review citing 10+ sources establishing theoretical framework, gaps in knowledge, how your study contributes\n\n4. Polish everything\nPilot had quick visualizations and provisional prose. Final project needs publication quality:\n\nCBE #3: Default matplotlib plot with minimal labels\nFinal: Seaborn/plotly styled figure with informative title, axis labels with units, legend, caption explaining key takeaway, referenced in text\n\n\n\nCommon Scaling Paths\nPath 1: Exploratory â†’ Confirmatory\n\nPilot: Explored 30 speeches with keyness, noticed temporal patterns\nFinal: Collected 200 speeches, formalized hypotheses based on pilot patterns, tested with time series + ANOVA\n\nPath 2: Single Method â†’ Multi-Method\n\nPilot: Topic modeling 50 news articles\nFinal: 300 articles, added keyness (validate topics distinguish genres), time series (track topic prevalence), classification (test topic-genre association)\n\nPath 3: Subsample â†’ Full Corpus\n\nPilot: Tested processing pipeline on 10 novels\nFinal: Scaled to 200 novels, added MDA beyond basic features tested in pilot\n\nPath 4: Method Pivot (if pilot revealed issues)\n\nPilot: Attempted MDA with 20 texts (KMO too low, factors unstable)\nFinal: Collected 120 texts (sufficient for MDA), OR pivoted to clustering + keyness (if corpus expansion not feasible)\n\n\n\nWhat If Youâ€™re Not Building from CBE #3?\nIf starting fresh:\n\nDefine research question using framework from Part 2 below\nChoose appropriate methods using decision tree from CBE #3 guide\nCollect sufficient data from the start (donâ€™t pilot at this stageâ€”just proceed to full scale)\nBudget extra time since you wonâ€™t have pilot lessons to draw on",
    "crumbs": [
      "Reports",
      "Final Project: Independent Corpus Analysis"
    ]
  },
  {
    "objectID": "reports/final-project.html#part-2-research-questions-and-literature",
    "href": "reports/final-project.html#part-2-research-questions-and-literature",
    "title": "Final Project: Independent Corpus Analysis",
    "section": "Part 2: Research Questions and Literature",
    "text": "Part 2: Research Questions and Literature\n\nCrafting a Strong Research Question\nStrong research questions for computational text analysis are:\nâœ… Specific: â€œHow do emotion words differ between academic and popular science writing?â€ (not â€œWhat are differences in writing styles?â€)\nâœ… Testable: Can be addressed with corpus methods + available data\nâœ… Motivated: Builds on prior scholarship or addresses practical problem\nâœ… Scoped: Answerable in 10-20 pages with your corpus size\n\n\n\n\n\n\nUsing Course Methods\n\n\n\nAll examples in this guide use methods covered in tutorials and mini labs (keyness, sentiment, topic modeling, MDA, classification, etc.). You can build an excellent final project entirely from course materialsâ€”extensions beyond what weâ€™ve covered are welcome but not expected.\n\n\nQuestion development process:\n\nBroad interest: â€œIâ€™m interested in academic writing differencesâ€\nNarrowing: â€œSpecifically, how STEM and humanities writing differ stylisticallyâ€\nTestable form: â€œDo Biology and English papers differ in linguistic complexity and personal engagement?â€\nFurther scoped: â€œTesting complexity and stance differences in MICUSP corpus (80 Biology vs 80 English papers) using Biber features and pronoun analysisâ€\n\n\n\nLiterature Review: Building Scholarly Context\nYour introduction should establish why your question matters through engagement with prior work.\nUndergraduate literature review (1-2 pages, 8-12 sources):\n\nBackground: What do we know about this phenomenon? (2-3 sources)\nGap: Whatâ€™s missing or underexplored? (1-2 sources)\nMethods: How have others studied this computationally? (2-3 sources)\nYour contribution: How does your project address the gap?\n\nGraduate literature review (3-4 pages, 15-25 sources):\n\nTheoretical framework: What theories explain this phenomenon? (4-6 sources)\nEmpirical findings: What have prior studies found? (5-8 sources)\nMethodological approaches: Evolution of computational methods in this area (3-5 sources)\nSynthesis: How your study extends, challenges, or complements prior work\n\nFinding relevant literature:\n\nStart with course readings: Many tutorials include â€œFurther Readingâ€ sections\nGoogle Scholar: Search â€œcorpus linguistics + [your topic]â€ or â€œcomputational text analysis + [your topic]â€\nFollow citations: Read abstracts, note which sources are cited repeatedly\nRecent work: Prioritize last 5-10 years, but include foundational sources\n\nOrganizing literature:\nBy theme (best for most projects):\nIntroduction\n  - Theme 1: Disciplinary writing differences (Hyland 2004, Biber 2006)\n  - Theme 2: Register variation in academic writing (Gray 2015, Hardy 2018)\n  - Theme 3: Computational approaches to style (Lee 2020, Brown 2022)\n  - Your study: Combines themes 1-3 with larger corpus than prior work\nChronologically (if showing evolution):\nIntroduction\n  - Early approaches: Manual coding (Smith 1995, Jones 2000)\n  - Computational turn: Automated detection (Lee 2010, Brown 2015)\n  - Recent advances: Neural models (Davis 2022, Wilson 2023)\n  - Your study: Applies recent methods to understudied corpus\n\n\nHypothesis Development (Optional but Recommended)\nBased on literature and/or pilot findings, you may formulate explicit hypotheses:\nExample: &gt; Based on prior work showing that humanities writing uses more interpretive and personal stance (Hyland, 2005) while STEM writing prioritizes objectivity and information density (Biber, 2006), we hypothesize that: &gt; - H1: English papers contain more first-person pronouns and hedging features than Biology papers &gt; - H2: Personal pronoun frequency correlates negatively with nominal density (measured by Biber features)\nHypotheses guide analysis and make interpretation clearer (confirmed/disconfirmed/partially supported).",
    "crumbs": [
      "Reports",
      "Final Project: Independent Corpus Analysis"
    ]
  },
  {
    "objectID": "reports/final-project.html#part-3-methodsdemonstrating-intentionality",
    "href": "reports/final-project.html#part-3-methodsdemonstrating-intentionality",
    "title": "Final Project: Independent Corpus Analysis",
    "section": "Part 3: Methodsâ€”Demonstrating Intentionality",
    "text": "Part 3: Methodsâ€”Demonstrating Intentionality\nThe rubric emphasizes analytical intentionality: why you chose specific methods, what alternatives you considered, how your choices connect to your research question.\n\nCorpus Design and Description\nData section checklist:\nâœ… Source: Where did texts come from? (Project Gutenberg, web scraping, institutional corpus, etc.)\nâœ… Selection criteria: How did you choose texts? (time period, author demographics, genre, length thresholds)\nâœ… Size and composition: How many texts? What categories? (present in clear table)\nâœ… Preprocessing: What cleaning steps? (removed headers, handled encoding, segmented)\nâœ… Justification: Why is this corpus appropriate for your question?\nExample: &gt; We collected 160 final-year student papers from the Michigan Corpus of Upper-level Student Papers (MICUSP; RÃ¶mer & Oâ€™Donnell, 2011). We selected 80 Biology papers and 80 English papers, all receiving grades of A or A-. We excluded papers shorter than 2,000 words (n=12 removed) to ensure sufficient linguistic features for analysis. Table 1 shows distribution by paper type and discipline. This corpus is appropriate because MICUSP represents successful student writing at the same institutional level, allowing controlled comparison of disciplinary conventions without confounding variables of student ability or institutional context (Hardy & RÃ¶mer, 2013).\nTable 1: Corpus Composition\n# Use great_tables (from Mini Lab 5) to create professional table\nimport polars as pl\nfrom great_tables import GT\n\ncorpus_summary = pl.DataFrame({\n    'Paper_Type': ['Argumentative Essay', 'Research Paper', 'Response Paper', 'Report'],\n    'Biology': [25, 30, 15, 10],\n    'English': [30, 20, 25, 5],\n    'Total': [55, 50, 40, 15]\n})\n\n(GT(corpus_summary)\n .tab_header(title=\"MICUSP Papers by Type and Discipline\")\n .fmt_number(columns=['Biology', 'English', 'Total'], decimals=0)\n .as_latex())  # Export to Overleaf\n\n\nMethods Section: Explaining Your Analytical Choices\nThis is where intentionality is most crucial. For each method, explain:\n\nWhat: Which analysis did you perform?\nWhy: Why is this appropriate for your question?\nHow: What specific implementation? (tools, parameters, validation)\nAlternatives: What other methods did you consider? Why not those?\n\nTemplate for each method:\n\nMethod 1: Keyness Analysis\nWhat: We computed log-likelihood keyness (Rayson & Garside, 2000) comparing Biology vs.Â English papers.\nWhy: Keyness identifies words that distinguish corpora, directly addressing our question about disciplinary linguistic differences. Log-likelihood accounts for corpus size balance (80 vs.Â 80 papers) and provides effect sizes interpretable across studies.\nHow: Using spaCy tokenization and custom keyness function (from Mini Lab 4), we: - Computed word frequencies per corpus - Calculated log-likelihood for each word - Applied Bonferroni correction (Î± = 0.05/10,000 words) - Filtered for effect size &gt; 0.2 (small-to-medium effect)\nAlternatives considered: We considered TF-IDF, but keyness provides statistical significance testing and is standard in corpus linguistics (Gabrielatos, 2018), making results comparable to prior work on disciplinary variation (Hyland, 2008).\n\nRepeat this structure for each method (2-3 methods typical for undergrads, 3-5 for grad students).\n\n\n\nConnecting Methods to Research Questions\nExplicitly link each analysis to your research question:\n\nRQ1: Do Biology and English papers differ in personal stance markers?\nâ†’ Method: Frequency analysis of first-person pronouns and hedging devices\nâ†’ Validation: T-test and effect size (Cohenâ€™s d) for frequency differences\n\n\nRQ2: Do disciplines differ on Biberâ€™s dimensions of academic writing?\nâ†’ Method: Multi-dimensional analysis using pybiber (67 features â†’ factor analysis)\nâ†’ Visualization: Figure 2 showing disciplines plotted on Dimensions 1-2\n\n\nRQ3: Can linguistic features predict discipline?\nâ†’ Method: Random Forest classification with 10-fold cross-validation\nâ†’ Interpretation: Feature importance analysis showing which features best distinguish disciplines\n\nThis structure shows reviewers (instructor, peers) that every analytical choice serves your research goals.\n\n\nAddressing Caveats and Limitations Proactively\nThe rubric rewards noting problems and their potential effects. Address limitations in Methods (technical) and Discussion (interpretive).\nMethods limitations (acknowledge technical constraints):\n\nGrade-level restriction: â€œOur corpus includes only A/A- papers, representing successful student writing. Linguistic patterns may differ in lower-achieving papers, limiting generalizability to all student writing (Hardy, 2013).â€\nCorpus coverage: â€œOur corpus includes only two disciplines (Biology and English). Findings may not generalize to all STEM vs.Â humanities writingâ€”disciplines like Psychology or History may show hybrid patterns (Hyland, 2004).â€\nGenre variation: â€œMICUSP includes multiple paper types (essays, reports, research papers). Observed disciplinary differences may partly reflect genre distribution differences between Biology and English (see Discussion).â€\n\nWhy this matters: Demonstrates critical thinking, prevents reviewers from seeing these as oversights.",
    "crumbs": [
      "Reports",
      "Final Project: Independent Corpus Analysis"
    ]
  },
  {
    "objectID": "reports/final-project.html#part-4-resultspresenting-findings-clearly",
    "href": "reports/final-project.html#part-4-resultspresenting-findings-clearly",
    "title": "Final Project: Independent Corpus Analysis",
    "section": "Part 4: Resultsâ€”Presenting Findings Clearly",
    "text": "Part 4: Resultsâ€”Presenting Findings Clearly\n\nOrganizing Results for Maximum Clarity\nResults section structure (report findings without interpretation):\n\nPrimary findings: Main patterns addressing RQ1\nSecondary findings: Additional patterns addressing RQ2-3\nRobustness checks: Validation that findings are stable (if applicable)\n\nEach finding should include:\n\nQuantitative summary: Specific numbers (means, effect sizes, p-values)\nFigure reference: â€œFigure 1 showsâ€¦â€\nDescriptive caption: What readers should notice\nNo interpretation yet: Save â€œwhyâ€ for Discussion\n\n\n\nCreating Publication-Quality Visualizations\nFigure checklist (from rubric):\nâœ… Clear and simple: One main point per figure, not cluttered\nâœ… Legible: Font size readable (12pt minimum for labels)\nâœ… Attractive: Professional styling (seaborn themes, consistent colors)\nâœ… Axes labeled: Include units (â€œFrequency per 1,000 wordsâ€)\nâœ… Legend: If multiple groups/categories\nâœ… Caption: Descriptive (not just â€œKeyness plotâ€), explains key takeaway\nâœ… Well-chosen type: Bar for comparisons, line for trends, scatter for relationships, etc.\nExample caption: &gt; Figure 1. Keyness of Top 20 Words Distinguishing Democratic and Republican Speeches. Positive log-likelihood values (blue) indicate words more frequent in Democratic speeches; negative values (red) indicate Republican-frequent words. Effect sizes (bars) show magnitude of difference. Words like â€œhealthcareâ€ and â€œequalityâ€ are Democratic markers, while â€œfreedomâ€ and â€œdefenseâ€ are Republican markers (all LL &gt; 0.4, p &lt; 0.001).\nCommon visualization types:\n\n\n\nResearch Goal\nVisualization\nTutorial Reference\n\n\n\n\nCompare groups\nKeyness plot, boxplots, grouped bars\nKeyness\n\n\nShow trends\nLine plots, change-point plots\nTime Series\n\n\nRelationships\nScatterplots, correlation heatmaps\nCorrelation\n\n\nDistributions\nHistograms, density plots\nFrequency\n\n\nClustering\nDendrograms, PCA scatter\nClustering\n\n\nClassification\nConfusion matrix, ROC curves\nMini Lab 12\n\n\nTopic composition\nStacked bars, heatmaps\nMini Lab 9\n\n\n\n\n\nWriting Results Prose\nGood results prose is:\n\nSpecific: â€œEnglish papers used 45% more first-person pronouns (M = 8.2 per 1,000 words) than Biology papers (M = 5.6), t(158) = 4.3, p &lt; 0.001, d = 0.68â€\nReferenced: â€œFigure 1 shows the disciplinary comparisonâ€¦â€\nOrdered logically: Primary findings first, secondary next\nDescriptive not interpretive: Report what you found, not why\n\nExample paragraph: &gt; Keyness analysis revealed significant disciplinary differences in vocabulary (Figure 1). The top 20 keywords included 13 Biology-frequent and 7 English-frequent terms (all LL &gt; 0.4). Biology papers overused technical terminology: â€œcellsâ€ (LL = 1.5, 4.2Ã— more frequent), â€œproteinâ€ (LL = 1.3, 3.8Ã— more frequent), and â€œhypothesisâ€ (LL = 0.9, 2.6Ã— more frequent). English papers overused interpretive and evaluative language: â€œsuggestsâ€ (LL = 1.1, 3.2Ã— more frequent), â€œreaderâ€ (LL = 0.8, 2.4Ã— more frequent), and â€œthemeâ€ (LL = 0.6, 2.1Ã— more frequent). Effect sizes ranged from 0.4 (small-medium) to 1.5 (large).\nNotice: Numbers, figure reference, no speculation about why these patterns exist (thatâ€™s Discussion).",
    "crumbs": [
      "Reports",
      "Final Project: Independent Corpus Analysis"
    ]
  },
  {
    "objectID": "reports/final-project.html#part-5-discussioninterpreting-findings",
    "href": "reports/final-project.html#part-5-discussioninterpreting-findings",
    "title": "Final Project: Independent Corpus Analysis",
    "section": "Part 5: Discussionâ€”Interpreting Findings",
    "text": "Part 5: Discussionâ€”Interpreting Findings\n\nMoving from Results to Interpretation\nDiscussion should:\n\nRestate main findings (briefly, 1-2 sentences)\nInterpret patterns (what do they mean?)\nConnect to literature (align with or challenge prior work?)\nAddress limitations (what canâ€™t you conclude?)\nSuggest implications (so what? whatâ€™s next?)\n\nExample structure:\n\nFinding 1: Disciplinary Vocabulary Differences\nRestatement: â€œBiology papers showed 60% higher frequency of technical terminology and passive constructions, while English papers emphasized interpretive and evaluative language with more first-person stance markers.â€\nInterpretation: â€œThis aligns with Hylandâ€™s (2005) framework for disciplinary epistemology, wherein STEM fields prioritize empirical objectivity and replicability, while humanities prioritize interpretive authority and argumentation. The prominence of technical nouns and passive voice in Biology reflects the â€˜rhetoric of objectivityâ€™ (Bazerman, 1988), while Englishâ€™s use of â€˜suggests,â€™ â€˜argues,â€™ and first-person pronouns reflects the â€˜rhetoric of interpretationâ€™ (Geisler, 1994).â€\nLiterature connection: â€œOur findings corroborate Biberâ€™s (2006) multi-dimensional analysis of academic writing, extending the STEM/humanities divide to undergraduate student writing. However, our effect sizes (LL = 0.4-1.5) are comparable to Biberâ€™s professional academic writing, suggesting disciplinary socialization occurs early in undergraduate education.â€\nLimitation: â€œWe cannot determine whether differences reflect genuine epistemological commitments or strategic genre performance. Interview data with students about their writing choices would clarify this (future work).â€\nImplication: â€œDisciplinary linguistic differences may create challenges for interdisciplinary writing (Carter, 2007), suggesting writing instruction should make disciplinary conventions explicit rather than assuming transferability.â€\n\n\n\nAddressing the Rubric Criteria\nâ€œCaveats and problems are noted and their potential effect on results explainedâ€\nBe proactive about limitations:\nData limitations: - â€œOur corpus includes only A/A- papers from one institution (University of Michigan), which may not represent typical undergraduate writing. Effect sizes might differ at other institutions or achievement levels.â€ - â€œWe analyzed only two disciplines, but disciplinary writing exists on a continuum. Psychology or Linguistics might show intermediate patterns between Biology and English (see Hardy, 2013).â€\nMethod limitations: - â€œPybiber features were developed on professional academic writing and may not fully capture undergraduate writing patterns. However, prior validation studies (Biber, 2006) suggest features transfer across proficiency levels.â€ - â€œK-means clustering requires pre-specifying cluster number. We chose k=5 based on silhouette score, but alternative k values might reveal different thematic structures.â€\nInterpretation limitations: - â€œWe infer disciplinary epistemologies from language patterns, but cannot confirm student awareness of these conventions. Differences may be tacit rather than conscious (Prior, 1998).â€\nEffect on results: - â€œGrade-level limitation suggests generalizability is narrowâ€”findings apply to successful student writing, not all undergraduate writing.â€ - â€œTwo-discipline design suggests disciplinary differences may be maximizedâ€”broader sampling (4-6 disciplines) would show more nuanced variation.â€\n\n\nBalancing Confidence and Humility\nToo confident: â€œOur results prove Biology is more objective than English.â€\nAppropriately caveated: â€œOur results suggest Biology writing employs more objectivity markers (passive voice, nominalizations), consistent with STEM epistemological norms prioritizing empirical replicability (Bazerman, 1988). However, linguistic objectivity â‰  epistemic objectivityâ€”both disciplines make knowledge claims, but through different rhetorical strategies.â€\nToo hedged: â€œWe canâ€™t really say anything definitive about disciplinary differences.â€\nAppropriately confident: â€œDisciplinary vocabulary differences were robust (large effect sizes, consistent with prior research), strongly suggesting distinct rhetorical conventions grounded in epistemological commitments (Hyland, 2004).â€",
    "crumbs": [
      "Reports",
      "Final Project: Independent Corpus Analysis"
    ]
  },
  {
    "objectID": "reports/final-project.html#part-6-argument-organization-and-flow",
    "href": "reports/final-project.html#part-6-argument-organization-and-flow",
    "title": "Final Project: Independent Corpus Analysis",
    "section": "Part 6: Argument Organization and Flow",
    "text": "Part 6: Argument Organization and Flow\nThe rubric emphasizes effective organization and reduced interpretive burden. Help readers follow your logic.\n\nSignposting and Transitions\nIntroduce each section with a roadmap:\n\nâ€œThe next section presents three sets of findings. First, we report keyness analysis identifying partisan vocabulary differences (RQ1). Second, we examine temporal trends in these differences (RQ2). Third, we test whether keywords predict speech ideology using classification (RQ3).â€\n\nTransition between sections:\n\nâ€œHaving established that partisan differences exist (keyness), we now examine whether these differences have changed over time.â€\n\n\nâ€œResults showed clear vocabulary distinctions. We next interpret these patterns in light of moral foundations theory.â€\n\n\n\nConnecting Figures to Narrative\nPoor connection: &gt; â€œFigure 1 shows keyness. Democratic speeches use different words.â€\nStrong connection: &gt; â€œFigure 1 reveals stark partisan vocabulary differences. The 20 most distinctive words cluster into two thematic categories: Democratic speeches emphasize social welfare (healthcare, education, equality), while Republican speeches emphasize security and economics (freedom, defense, tax). This pattern suggests parties activate distinct moral frames (Lakoff, 2002), a point we return to in the Discussion.â€\nIn caption, reinforce the point: &gt; Figure 1. Partisan Vocabulary Differences in State of the Union Addresses (1900-2020). Democratic-frequent words (blue bars) cluster around social welfare themes, while Republican-frequent words (red bars) emphasize security and economics. Effect sizes (log-likelihood &gt; 0.4) indicate substantively meaningful differences beyond statistical significance.\n\n\nReducing Interpretive Burden\nHigh interpretive burden (reader must infer connections): &gt; â€œWe used keyness. Table 1 shows corpus composition. Figure 2 shows results. We also did time series.â€\nLow interpretive burden (explicit connections): &gt; â€œTo test whether partisan rhetoric differs (RQ1), we computed keyness comparing Democratic and Republican speeches (see Table 1 for corpus composition). Figure 2 shows that Democratic speeches overuse social welfare vocabulary while Republican speeches overuse security vocabulary (all LL &gt; 0.4, p &lt; 0.001). This confirms our hypothesis that parties employ distinct moral frames. We next examine whether this difference has changed over time (RQ2), using time series analysis (Figure 3).â€",
    "crumbs": [
      "Reports",
      "Final Project: Independent Corpus Analysis"
    ]
  },
  {
    "objectID": "reports/final-project.html#part-7-professional-polish",
    "href": "reports/final-project.html#part-7-professional-polish",
    "title": "Final Project: Independent Corpus Analysis",
    "section": "Part 7: Professional Polish",
    "text": "Part 7: Professional Polish\n\nWriting Style\nConcise, appropriate to audience:\n\nAcademic readers: Assume familiarity with basic corpus methods, but explain technical choices\nActive voice (when appropriate): â€œWe computed keynessâ€ (not â€œKeyness was computedâ€)\nPrecise language: â€œ35% more frequentâ€ (not â€œa lot moreâ€)\nAvoid jargon: Define technical terms on first use\n\nExample: &gt; We computed log-likelihood (LL), a statistical measure comparing word frequencies between corpora that accounts for corpus size differences (Rayson & Garside, 2000). LL values &gt; 3.84 indicate significant differences (p &lt; 0.05); values &gt; 0.4 are considered medium effect sizes.\n\n\nFormatting and Legibility\nOverleaf formatting checklist:\nâœ… Consistent headings: Use \\section{}, \\subsection{}, \\subsubsection{}\nâœ… Figure placement: After first mention in text (use \\begin{figure}[h] for â€œhereâ€)\nâœ… Table formatting: Use \\toprule, \\midrule, \\bottomrule for professional look\nâœ… Citations: BibTeX format, consistent style (APA, MLA, Chicago)\nâœ… Page numbers: Include\nâœ… Line spacing: 1.5 or double (check assignment requirements)\nâœ… Margins: 1-inch all sides\nLaTeX tips (from CBE #2):\n% Figure with caption\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=0.8\\textwidth]{figures/keyness_plot.png}\n    \\caption{Partisan Vocabulary Differences (1900-2020)}\n    \\label{fig:keyness}\n\\end{figure}\n\n% Reference in text\nFigure~\\ref{fig:keyness} shows...\n\n% Table with great_tables output\n\\input{tables/corpus_composition.tex}\n\n% Citation\nPrior work has shown... \\citep{Wilson2019}\n\n\nCitations and References\nAll sources must be cited:\n\nTheories: â€œMoral foundations theory (Haidt, 2012) positsâ€¦â€\nPrior findings: â€œWilson (2019) found partisan differences in campaign speechesâ€¦â€\nMethods: â€œLog-likelihood keyness (Rayson & Garside, 2000)â€¦â€\nSoftware: â€œWe used spaCy 3.5 , polars , and scikit-learn â€\nData: â€œState of the Union addresses from the American Presidency Project (Peters & Woolley, 2023)â€\n\nAI disclosure (if applicable):\n\nAcknowledgments: We used ChatGPT-4 (OpenAI, 2023) to debug spaCy dependency parsing code (lines 45-67 in process_corpus.py). All conceptual work, analysis design, and interpretation are our own. We verified AI-generated code for correctness before use.",
    "crumbs": [
      "Reports",
      "Final Project: Independent Corpus Analysis"
    ]
  },
  {
    "objectID": "reports/final-project.html#part-8-methodological-showcases",
    "href": "reports/final-project.html#part-8-methodological-showcases",
    "title": "Final Project: Independent Corpus Analysis",
    "section": "Part 8: Methodological Showcases",
    "text": "Part 8: Methodological Showcases\nHere are common project archetypes with recommended method combinations:\n\nArchetype 1: Genre/Register Differentiation\nQuestion: How does academic writing differ from journalistic writing?\nMethods: 1. Keyness (identify distinctive vocabulary) 2. MDA (pybiber 67 features â†’ factor analysis â†’ dimensions) 3. ANOVA (test if dimensions distinguish genres, compute Î·Â²) 4. Visualization: Biplot showing genres in 2D dimension space\nTutorials: Keyness, MDA, ANOVA\n\n\n\nArchetype 2: Temporal Change\nQuestion: How has scientific writing evolved (1900-2020)?\nMethods: 1. Time series (track features over time: complexity, hedging, first-person) 2. Change-point detection (identify moments of shift) 3. Regression (test if trends are significant) 4. Clustering (VNC to identify periodization)\nTutorials: Time Series, Clustering\n\n\n\nArchetype 3: Authorship/Stylometry\nQuestion: Can we distinguish Author A from Author B?\nMethods: 1. Frequency (compare function word rates, POS distributions) 2. Keyness (most distinctive features per author) 3. Classification (Random Forest with cross-validation) 4. Feature importance (which features best predict authorship?)\nTutorials: Frequency, Mini Lab 12\n\n\n\nArchetype 4: Thematic Discovery\nQuestion: What themes emerge in 500 blog posts?\nMethods: 1. Topic modeling (LDA for thematic structure) 2. Coherence (validate topic quality) 3. Keyness (validate topics distinguish by metadata: author, date, etc.) 4. Time series (track topic prevalence over time)\nTutorials: Mini Lab 9, Time Series\n\n\n\nArchetype 5: Narrative/Sentiment\nQuestion: Do tragic novels have different emotional arcs than comedies?\nMethods: 1. Sentiment analysis (syuzhet, NRC lexicon) 2. Clustering (group novels by arc shape) 3. Comparison (test if clusters align with genre labels) 4. Visualization: Emotional arcs superimposed by genre\nTutorials: Sentiment/Syuzhet, Mini Labs 1, 11-12\n\n\n\nArchetype 6: Semantic Relationships\nQuestion: How does meaning of â€œfreedomâ€ vary by political context?\nMethods: 1. Word2vec (train embeddings on political corpus) 2. Nearest neighbors (find words similar to â€œfreedomâ€ in different subcorpora) 3. Vector arithmetic (semantic shifts: freedom[2020] - freedom[1960]) 4. Visualization: t-SNE plot showing semantic space\nTutorials: Vector Models, Mini Lab 9\n\n\n\n\n\n\n\nExtending Beyond Course Methods\n\n\n\nWhile all examples above use methods covered in tutorials and mini labs, youâ€™re welcome to explore additional techniques (e.g., metaphor detection, neural models, network analysis) if you have the background or interest. However, this is not expectedâ€”successful projects can be built entirely from course materials. If extending, budget extra time for learning and troubleshooting.",
    "crumbs": [
      "Reports",
      "Final Project: Independent Corpus Analysis"
    ]
  },
  {
    "objectID": "reports/final-project.html#part-9-final-checklist",
    "href": "reports/final-project.html#part-9-final-checklist",
    "title": "Final Project: Independent Corpus Analysis",
    "section": "Part 9: Final Checklist",
    "text": "Part 9: Final Checklist\nBefore submission, verify you have:\n\nContent Completeness\nâœ… Introduction (2-3 pages): - Research question clearly stated - Literature review (undergrad: 8-12 sources; grad: 15-25 sources) - Theoretical framework - Hypotheses (if applicable)\nâœ… Data (1-2 pages): - Source and selection criteria explained - Corpus composition table (by relevant categories) - Preprocessing steps documented - Justification for corpus appropriateness\nâœ… Methods (2-3 pages): - Each method explained: what, why, how, alternatives - Explicit connection to research questions - Technical details (tools, parameters, validation) - Caveats and limitations noted\nâœ… Results (2-4 pages): - Findings organized logically (primary â†’ secondary) - Specific quantitative summaries (means, effect sizes, p-values) - 3-5 figures with descriptive captions - All figures referenced in text - No interpretation (save for Discussion)\nâœ… Discussion (2-4 pages): - Interpretation of findings - Connection to literature (confirm/challenge prior work) - Limitations and their effects on conclusions - Implications and future directions\nâœ… Conclusion (1 page): - Summary of main findings - Broader significance - Limitations restated briefly - Next steps for research\nâœ… References: - All sources cited in text appear in bibliography - Consistent citation style - Software, data, and AI tools cited (if used)\n\n\nMethodological Rigor\nâœ… Analytical intentionality demonstrated: Clear rationale for each method choice\nâœ… Alternatives considered: Explained why other methods were not chosen\nâœ… Sufficient scale: Corpus size matches method requirements (see CBE #3 guide)\nâœ… Validation: Results checked for robustness (cross-validation, multiple metrics, etc.)\nâœ… Caveats proactive: Limitations acknowledged with potential effects explained\n\n\nPresentation Quality\nâœ… Figures: - Clear, simple, legible, attractive - Axes labeled with units - Legends included - Well-chosen types for intended points - Descriptive captions\nâœ… Tables: - Professional formatting - Clear headers - Referenced in text\nâœ… Writing: - Concise, appropriate style - Transitions between sections - Low interpretive burden (explicit connections) - Proofread (no typos, grammar errors)\nâœ… Formatting: - Consistent heading levels - Page numbers - Proper margins and spacing - Professional appearance\n\n\nCollaboration (if team project)\nâœ… Division of labor documented: Who did what?\nâœ… All members contributed: Evidence in Overleaf version history\nâœ… Integrated coherently: Single authorial voice, not patchwork",
    "crumbs": [
      "Reports",
      "Final Project: Independent Corpus Analysis"
    ]
  },
  {
    "objectID": "reports/final-project.html#part-10-from-final-project-to-future-work",
    "href": "reports/final-project.html#part-10-from-final-project-to-future-work",
    "title": "Final Project: Independent Corpus Analysis",
    "section": "Part 10: From Final Project to Future Work",
    "text": "Part 10: From Final Project to Future Work\nThe best final projects donâ€™t end with the semesterâ€”they become:\n\nConference presentations: Many undergraduate conferences accept computational text analysis projects\nPublications: Strong projects can be expanded into journal submissions (grad students especially)\nPortfolio pieces: Demonstrate technical and analytical skills to employers/graduate programs\nSpringboards: Foundation for thesis, dissertation, or future research\n\nIf you want to continue this work, consider:\n\nExpand corpus: Scale from 200 to 2,000 texts for more robust findings\nAdd methods: Incorporate techniques not covered in class (network analysis, neural models, multilingual)\nDeepen theory: Engage more extensively with disciplinary literature\nCollaborate: Co-author with faculty or peers from other departments\nPresent: Submit to conferences (DH, ICAME, Corpus Linguistics)\n\nYour final project is not the endâ€”itâ€™s the beginning of computational text analysis as part of your scholarly toolkit.\n\nFinal Project Rubric:\nChosen analysis is clearly connected to substantive question\n20 to &gt;18.0 pts Excellent 18 to &gt;15.0 pts Satisfactory 15 to &gt;0 pts Satisfactory\n20 pts Text clearly explains why this analysis was chosen over alternatives\n20 to &gt;18.0 pts Excellent 18 to &gt;15.0 pts Satisfactory 15 to &gt;0 pts Satisfactory\n20 pts Caveats and problems are noted and their potential effect on results explained\n20 to &gt;18.0 pts Excellent 18 to &gt;15.0 pts Satisfactory 15 to &gt;0 pts Satisfactory\n20 pts The Reportâ€™s argument is effectively organized. Data and their interpretation are presented in an appropriate order and connected explicitly with transitions or signaling. The most effective Reports reduce the interpretive burden on the reader.\n20 to &gt;18.0 pts Excellent 18 to &gt;15.0 pts Satisfactory 15 to &gt;0 pts Satisfactory\n20 pts Introduction sets out questions to be answered and their importance\n10 to &gt;9.0 pts Excellent 9 to &gt;7.5 pts Satisfactory 7.5 to &gt;0 pts Satisfactory\n10 pts The source of the data is described and its relevance to the problem summarized. Corpus data is presented and summarized in a clear table broken out into relevant categories.\n10 to &gt;9.0 pts Excellent 9 to &gt;7.5 pts Satisfactory 7.5 to &gt;0 pts Satisfactory\n10 pts Connection between figures and narrative is clear from text and caption\n10 to &gt;9.0 pts Excellent 9 to &gt;7.5 pts Satisfactory 7.5 to &gt;0 pts Satisfactory\n10 pts Figures are clear, simple, legible, and attractive\n10 to &gt;9.0 pts Excellent 9 to &gt;7.5 pts Satisfactory 7.5 to &gt;0 pts Satisfactory\n10 pts All legends and axes are clearly labeled, including units\n10 to &gt;9.0 pts Excellent 9 to &gt;7.5 pts Satisfactory 7.5 to &gt;0 pts Satisfactory\n10 pts Types of figures are well-chosen, illustrating the intended points clearly\n10 to &gt;9.0 pts Excellent 9 to &gt;7.5 pts Satisfactory 7.5 to &gt;0 pts Satisfactory\n10 pts Figures are used whenever the text needs them\n10 to &gt;9.0 pts Excellent 9 to &gt;7.5 pts Satisfactory 7.5 to &gt;0 pts Satisfactory\n10 pts Summarizes conclusions presented in report\n10 to &gt;9.0 pts Excellent 9 to &gt;7.5 pts Satisfactory 7.5 to &gt;0 pts Satisfactory\n10 pts Conclusion notes any limitations on conclusions and what could be done to address these\n10 to &gt;9.0 pts Excellent 9 to &gt;7.5 pts Satisfactory 7.5 to &gt;0 pts Satisfactory\n10 pts Has a concise style that is appropriate to the audience\n10 to &gt;9.0 pts Excellent 9 to &gt;7.5 pts Satisfactory 7.5 to &gt;0 pts Satisfactory\n10 pts Formatting is clear and legible\n10 to &gt;9.0 pts Excellent 9 to &gt;7.5 pts Satisfactory 7.5 to &gt;0 pts Satisfactory\n10 pts References are clearly given to information from outside sources; quotations clearly mark any verbatim text from\n10 to &gt;9.0 pts Excellent 9 to &gt;7.5 pts Satisfactory 7.5 to &gt;0 pts Satisfactory\n10 pts",
    "crumbs": [
      "Reports",
      "Final Project: Independent Corpus Analysis"
    ]
  },
  {
    "objectID": "reports/final-project.html#works-cited",
    "href": "reports/final-project.html#works-cited",
    "title": "Final Project: Independent Corpus Analysis",
    "section": "Works Cited",
    "text": "Works Cited",
    "crumbs": [
      "Reports",
      "Final Project: Independent Corpus Analysis"
    ]
  },
  {
    "objectID": "reports/statistical-reporting-guide.html",
    "href": "reports/statistical-reporting-guide.html",
    "title": "Statistical Reporting Guide",
    "section": "",
    "text": "General Principles",
    "crumbs": [
      "Reports",
      "Statistical Reporting Guide"
    ]
  },
  {
    "objectID": "reports/statistical-reporting-guide.html#general-principles",
    "href": "reports/statistical-reporting-guide.html#general-principles",
    "title": "Statistical Reporting Guide",
    "section": "",
    "text": "What to Include\nEvery statistical claim should include:\n\nDescriptive statistics: Means (M), standard deviations (SD), counts (n)\nTest statistic: The specific value (t, F, Ï‡Â², LL, etc.)\nEffect size: Magnitude of difference (d, Î·Â², LL effect size, etc.)\nSignificance: p-value or confidence interval\nInterpretation: What it means in plain language\n\nExample: &gt; Biology papers used significantly more passive constructions (M = 12.3 per 1,000 words, SD = 3.1) than English papers (M = 7.8, SD = 2.4), t(158) = 8.2, p &lt; 0.001, d = 1.62. This large effect suggests passive voice is a core feature of STEM scientific writing.\n\n\nWhat NOT to Do\nâŒ Screenshots of code or output: Never include screenshots from Jupyter notebooks or Colab\nâŒ Raw Python output: Donâ€™t paste unformatted print statements\nâŒ Tables without context: Every table needs a caption and in-text reference\nâŒ Figures without interpretation: Explain what readers should notice\nâŒ P-values alone: Always include descriptive statistics and effect sizes",
    "crumbs": [
      "Reports",
      "Statistical Reporting Guide"
    ]
  },
  {
    "objectID": "reports/statistical-reporting-guide.html#formatting-numbers",
    "href": "reports/statistical-reporting-guide.html#formatting-numbers",
    "title": "Statistical Reporting Guide",
    "section": "Formatting Numbers",
    "text": "Formatting Numbers\n\nDecimal Places\n\nMeans and SDs: 1-2 decimal places (M = 12.3, SD = 3.14)\nProportions and percentages: 1 decimal place (23.4%)\nTest statistics: 2 decimal places (t = 3.45, F = 12.67)\nP-values: 3 decimal places (p = 0.003) or p &lt; 0.001, p &gt; 0.05\nEffect sizes: 2 decimal places (d = 0.68, Î·Â² = 0.14)\n\n\n\nItalics\nItalicize statistical symbols in text:\n\nM, SD, n (descriptive statistics)\nt, F, Ï‡Â², r, z (test statistics)\np, d, Î·Â², RÂ² (significance and effect sizes)\ndf (degrees of freedom)\n\nExample: â€œM = 45.2, SD = 12.3, n = 80â€\n\n\nReporting Exact vs.Â Threshold P-values\n\np â‰¥ 0.001: Report exact value: p = 0.023, p = 0.187\np &lt; 0.001: Use threshold: p &lt; 0.001\np &gt; 0.05: Report as p = 0.12 or p &gt; 0.05 (not â€œnsâ€ or â€œn.s.â€)",
    "crumbs": [
      "Reports",
      "Statistical Reporting Guide"
    ]
  },
  {
    "objectID": "reports/statistical-reporting-guide.html#method-specific-reporting",
    "href": "reports/statistical-reporting-guide.html#method-specific-reporting",
    "title": "Statistical Reporting Guide",
    "section": "Method-Specific Reporting",
    "text": "Method-Specific Reporting\n\nKeyness (Log-Likelihood)\nWhat to report: - Log-likelihood value (LL) - Effect size (LL effect size or normalized frequency difference) - Direction of difference - Word frequencies in both corpora\nTemplate: &gt; The word â€œcellsâ€ was significantly more frequent in Biology papers (45.2 per 1,000 words) than English papers (2.1 per 1,000 words), LL = 156.3, p &lt; 0.001, effect size = 1.2 (large).\nIn a table (from Mini Lab 4):\n| Word      | Biology freq | English freq | LL    | p      | Effect |\n|-----------|--------------|--------------|-------|--------|--------|\n| cells     | 45.2         | 2.1          | 156.3 | &lt;.001  | 1.2    |\n| suggests  | 3.4          | 12.8         | 89.4  | &lt;.001  | 0.8    |\nEffect size interpretation: - &lt; 0.2: Negligible - 0.2-0.4: Small - 0.4-0.6: Medium - &gt; 0.6: Large\n\n\nT-tests\nIndependent samples t-test (comparing two groups):\n\nBiology papers (M = 12.3, SD = 3.1) used significantly more passive voice than English papers (M = 7.8, SD = 2.4), t(158) = 8.2, p &lt; 0.001, d = 1.62.\n\nComponents: - t(df) = value - Degrees of freedom in parentheses - Cohenâ€™s d for effect size\nCohenâ€™s d interpretation: - 0.2: Small - 0.5: Medium - 0.8: Large\n\n\nANOVA (One-Way)\nWhat to report: - F-statistic with degrees of freedom - Eta-squared (Î·Â²) or partial eta-squared - Post-hoc test results (if significant)\nTemplate: &gt; A one-way ANOVA revealed significant differences in nominal density across disciplines, F(2, 237) = 45.3, p &lt; 0.001, Î·Â² = 0.28. Post-hoc Tukey HSD tests showed Biology (M = 34.2, SD = 8.1) differed significantly from both English (M = 22.3, SD = 6.4, p &lt; 0.001) and Psychology (M = 28.1, SD = 7.2, p = 0.002), while English and Psychology did not differ significantly (p = 0.15).\nEta-squared interpretation (from ANOVA tutorial): - 0.01: Small - 0.06: Medium - 0.14: Large\nANOVA table format:\n| Source    | SS      | df  | MS     | F     | p      | Î·Â²   |\n|-----------|---------|-----|--------|-------|--------|------|\n| Between   | 2450.3  | 2   | 1225.2 | 45.3  | &lt;.001  | 0.28 |\n| Within    | 6402.1  | 237 | 27.0   |       |        |      |\n| Total     | 8852.4  | 239 |        |       |        |      |\n\n\nCorrelation\nWhat to report: - Correlation coefficient (r, Ï, Ï„) - Sample size - P-value - Direction and strength\nTemplate: &gt; First-person pronoun frequency correlated negatively with nominal density, r(158) = -0.52, p &lt; 0.001, indicating that more personal writing tends to use fewer nominalizations.\nPearsonâ€™s r interpretation: - 0.10-0.29: Small/weak - 0.30-0.49: Medium/moderate - 0.50+: Large/strong\nCorrelation matrix (donâ€™t report full matrix in textâ€”use figure/table):\n# Create correlation heatmap (from correlation tutorial)\n# Export as figure, not raw output\n\n\nRegression (Linear)\nWhat to report: - RÂ² (variance explained) - F-statistic for overall model - Î² coefficients for predictors - Standard errors and p-values\nTemplate: &gt; Linear regression showed that first-person pronouns and hedging significantly predicted disciplinary category, RÂ² = 0.34, F(2, 157) = 40.3, p &lt; 0.001. First-person pronouns were a significant predictor (Î² = 0.45, SE = 0.08, p &lt; 0.001), as was hedging (Î² = 0.28, SE = 0.10, p = 0.006).\nRegression table:\n| Predictor           | Î²     | SE   | t     | p     | 95% CI        |\n|---------------------|-------|------|-------|-------|---------------|\n| First-person (I/we) | 0.45  | 0.08 | 5.63  | &lt;.001 | [0.29, 0.61]  |\n| Hedging             | 0.28  | 0.10 | 2.80  | .006  | [0.08, 0.48]  |\n\n\nClassification (Machine Learning)\nWhat to report: - Accuracy (overall and per-class) - Precision, recall, F1-score - Confusion matrix - Cross-validation details - Feature importance (top 5-10 features)\nTemplate: &gt; Random Forest classification distinguished Biology from English papers with 87% accuracy (10-fold cross-validation). Precision was 0.89 for Biology and 0.85 for English; recall was 0.85 for Biology and 0.89 for English (Table 2). The most important features were passive voice frequency (importance = 0.23), nominal density (0.18), and first-person pronouns (0.15).\nClassification report (from Mini Lab 12):\n| Class    | Precision | Recall | F1-score | Support |\n|----------|-----------|--------|----------|---------|\n| Biology  | 0.89      | 0.85   | 0.87     | 80      |\n| English  | 0.85      | 0.89   | 0.87     | 80      |\n| Accuracy |           |        | 0.87     | 160     |\nConfusion matrix (visualize, donâ€™t paste numbers): - Create heatmap with seaborn - Label axes clearly - Include caption\n\n\nTopic Modeling\nWhat to report: - Number of topics - Coherence score - Perplexity (optional) - Top words per topic (5-10 words) - Interpretation of each topic\nTemplate: &gt; LDA with 8 topics achieved a coherence score of 0.52 (acceptable) and perplexity of 245. Topic 3 (â€œcellular processesâ€) was characterized by cells, protein, DNA, membrane, enzyme, receptor and appeared in 34% of Biology papers but only 2% of English papers.\nTopic table:\n| Topic | Label              | Top Words                                      | % Bio | % Eng |\n|-------|--------------------|------------------------------------------------|-------|-------|\n| 1     | Cellular processes | cells, protein, DNA, membrane, enzyme          | 34%   | 2%    |\n| 2     | Literary analysis  | text, reader, theme, narrative, character      | 3%    | 41%   |\n\n\nClustering\nWhat to report: - Clustering method (hierarchical, k-means) - Number of clusters - Validation metric (silhouette score, dendrogram scree) - Cluster composition - Interpretation\nTemplate: &gt; Hierarchical clustering with Wardâ€™s linkage identified 5 clusters (silhouette score = 0.42). Cluster 1 (n = 42) consisted primarily of Biology lab reports (88% of cluster), characterized by high nominal density and low first-person usage. Cluster 2 (n = 38) contained predominantly English argumentative essays (76%), with high first-person and hedging frequencies.\nDendrogram: Include figure, not code output\nCluster composition table:\n| Cluster | n  | Primary Discipline | Key Features                          |\n|---------|----|--------------------|---------------------------------------|\n| 1       | 42 | Biology (88%)      | High nominal density, low 1st person  |\n| 2       | 38 | English (76%)      | High 1st person, high hedging         |\n\n\nMulti-Dimensional Analysis (MDA)\nWhat to report: - Number of dimensions extracted - Variance explained per dimension - Factor loadings (top features per dimension) - Dimension interpretation - How texts score on dimensions\nTemplate: &gt; Factor analysis of 67 Biber features extracted 3 dimensions explaining 58% of variance. Dimension 1 (â€œInformational density,â€ 31% variance) loaded positively on nominalizations (0.82), attributive adjectives (0.76), and prepositions (0.71). Biology papers scored significantly higher on this dimension (M = 1.8, SD = 0.6) than English papers (M = -1.2, SD = 0.8), F(1, 158) = 89.3, p &lt; 0.001, Î·Â² = 0.36.\nFactor loadings table (top 5-7 features per dimension):\n| Feature                  | Dim 1  | Dim 2  | Dim 3  |\n|--------------------------|--------|--------|--------|\n| Nominalizations          | 0.82   | -0.12  | 0.23   |\n| Attributive adjectives   | 0.76   | 0.08   | -0.15  |\n| First-person pronouns    | -0.14  | 0.88   | 0.22   |\n| Hedging                  | -0.23  | 0.79   | -0.18  |\nDimension scores visualization: Biplot or boxplots by discipline\n\n\nSentiment Analysis\nWhat to report: - Sentiment method/lexicon (NRC, VADER, syuzhet) - Overall sentiment scores - Differences between groups (if comparing) - Temporal patterns (if time series)\nTemplate: &gt; Using the NRC emotion lexicon, tragic novels showed significantly higher negative emotion word density (M = 8.2 per 100 words, SD = 2.1) than comedic novels (M = 4.3, SD = 1.6), t(78) = 9.8, p &lt; 0.001, d = 2.12. Positive emotion words did not differ significantly between genres, t(78) = 1.2, p = 0.23.\nSyuzhet arc (narrative arcs): &gt; DCT transformation revealed distinct narrative shapes: comedies showed the classic â€œrags to richesâ€ arc (negative start, positive end), while tragedies showed inverted arcs (positive start, negative end). Figure 3 shows normalized emotional arcs for both genres.\nSentiment over time: Include line plot, not raw numbers",
    "crumbs": [
      "Reports",
      "Statistical Reporting Guide"
    ]
  },
  {
    "objectID": "reports/statistical-reporting-guide.html#tables-and-figures",
    "href": "reports/statistical-reporting-guide.html#tables-and-figures",
    "title": "Statistical Reporting Guide",
    "section": "Tables and Figures",
    "text": "Tables and Figures\n\nTable Formatting\nEvery table must have:\n\nNumber and caption above the table\nClear column headers with units\nIn-text reference (â€œTable 1 showsâ€¦â€)\nAppropriate precision (donâ€™t over-report decimals)\nNotes (below table) for abbreviations or statistical notation\n\nExample:\nTable 1\nDescriptive Statistics for Linguistic Features by Discipline\n\n| Feature              | Biology        | English        |\n|                      | M (SD)         | M (SD)         |\n|----------------------|----------------|----------------|\n| Passive voice        | 12.3 (3.1)     | 7.8 (2.4)      |\n| Nominal density      | 34.2 (8.1)     | 22.3 (6.4)     |\n| First-person pronouns| 5.6 (2.3)      | 8.2 (3.0)      |\n\nNote. M = mean; SD = standard deviation. All values are \nfrequencies per 1,000 words. N = 80 per discipline.\nCreating tables in Python:\nfrom great_tables import GT\n\n# Create professional table (from Mini Lab 5)\n# Export as LaTeX for Overleaf\ntable.as_latex()  # Not screenshot!\nLaTeX table formatting (for Overleaf):\nThe same table in LaTeX syntax:\n\\begin{table}[h]\n\\centering\n\\caption{Descriptive Statistics for Linguistic Features by Discipline}\n\\label{tab:descriptives}\n\\begin{tabular}{lcc}\n\\toprule\nFeature & Biology & English \\\\\n        & \\textit{M} (\\textit{SD}) & \\textit{M} (\\textit{SD}) \\\\\n\\midrule\nPassive voice        & 12.3 (3.1) & 7.8 (2.4) \\\\\nNominal density      & 34.2 (8.1) & 22.3 (6.4) \\\\\nFirst-person pronouns & 5.6 (2.3) & 8.2 (3.0) \\\\\n\\bottomrule\n\\end{tabular}\n\\begin{tablenotes}\n\\small\n\\item \\textit{Note}. \\textit{M} = mean; \\textit{SD} = standard deviation. \nAll values are frequencies per 1,000 words. \\textit{N} = 80 per discipline.\n\\end{tablenotes}\n\\end{table}\nKey LaTeX table commands: - \\toprule, \\midrule, \\bottomrule: Professional horizontal lines (requires booktabs package) - \\centering: Centers the table - \\caption{}: Table title (appears above table) - \\label{}: For cross-referencing in text (e.g., \\ref{tab:descriptives}) - {lcc}: Column alignment (left, center, center) - \\textit{}: Italics for statistical symbols - &: Column separator - \\\\: Row separator\nBest practice: Use great_tables in Python to generate LaTeX code, then paste into Overleaf. This ensures consistency and saves manual formatting time.\n\n\nFigure Formatting\nEvery figure must have:\n\nNumber and caption below the figure\nClear axis labels with units (e.g., â€œFrequency per 1,000 wordsâ€)\nLegend (if multiple groups/categories)\nLegible font (minimum 12pt)\nProfessional styling (seaborn theme, not default matplotlib)\nIn-text reference (â€œFigure 1 showsâ€¦â€)\n\nBad caption: â€œKeyness plotâ€\nGood caption: â€œFigure 1. Keyness of Top 20 Words Distinguishing Biology and English Papers. Positive log-likelihood values (blue) indicate Biology-frequent words; negative values (orange) indicate English-frequent words. All LL &gt; 0.4 (medium-to-large effect), p &lt; 0.001.â€\nFigure types by purpose:\n\n\n\nPurpose\nRecommended Plot\n\n\n\n\nCompare group means\nBar plot, boxplot, violin plot\n\n\nShow distribution\nHistogram, density plot\n\n\nCorrelation\nScatterplot with regression line\n\n\nTime trend\nLine plot, area plot\n\n\nKeyness\nHorizontal bar plot (pos/neg)\n\n\nClustering\nDendrogram, PCA scatter\n\n\nClassification\nConfusion matrix heatmap\n\n\nTopic composition\nStacked bar, heatmap\n\n\nFactor loadings\nHorizontal bar plot\n\n\nSentiment arc\nLine plot with smoothing\n\n\n\nCreating publication-quality figures:\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Set style\nsns.set_style(\"whitegrid\")\nsns.set_context(\"paper\", font_scale=1.2)\n\n# Create plot\nfig, ax = plt.subplots(figsize=(8, 6))\n# ... your plot code ...\n\n# Clear labels with units\nax.set_xlabel(\"Discipline\", fontsize=12)\nax.set_ylabel(\"Frequency per 1,000 words\", fontsize=12)\nax.set_title(\"Passive Voice by Discipline\", fontsize=14)\n\n# Save high-resolution\nplt.savefig(\"passive_voice.png\", dpi=300, bbox_inches='tight')\nNever: - Screenshot your Jupyter/Colab output - Include code cells in your report - Paste matplotlibâ€™s default ugly plots - Forget axis labels or legends - Use pie charts (almost always a bad choice)",
    "crumbs": [
      "Reports",
      "Statistical Reporting Guide"
    ]
  },
  {
    "objectID": "reports/statistical-reporting-guide.html#inline-vs.-parenthetical-reporting",
    "href": "reports/statistical-reporting-guide.html#inline-vs.-parenthetical-reporting",
    "title": "Statistical Reporting Guide",
    "section": "Inline vs.Â Parenthetical Reporting",
    "text": "Inline vs.Â Parenthetical Reporting\n\nInline (sentence structure includes statistics)\n\nBiology papers used significantly more passive constructions (M = 12.3) than English papers (M = 7.8), t(158) = 8.2, p &lt; 0.001.\n\n\n\nParenthetical (statistics supplementary)\n\nBiology papers used significantly more passive constructions than English papers (Ms = 12.3 vs.Â 7.8, t(158) = 8.2, p &lt; 0.001, d = 1.62).\n\nBoth are acceptableâ€”choose based on what flows better in your sentence.",
    "crumbs": [
      "Reports",
      "Statistical Reporting Guide"
    ]
  },
  {
    "objectID": "reports/statistical-reporting-guide.html#common-mistakes-to-avoid",
    "href": "reports/statistical-reporting-guide.html#common-mistakes-to-avoid",
    "title": "Statistical Reporting Guide",
    "section": "Common Mistakes to Avoid",
    "text": "Common Mistakes to Avoid\n\nâŒ Reporting Only P-values\nBad: â€œThe difference was significant (p &lt; 0.05).â€\nGood: â€œBiology papers used significantly more passive voice (M = 12.3, SD = 3.1) than English papers (M = 7.8, SD = 2.4), t(158) = 8.2, p &lt; 0.001, d = 1.62.â€\nWhy: P-values alone donâ€™t tell readers the magnitude or practical importance of differences.\n\n\nâŒ Over-Precise Reporting\nBad: â€œMean = 12.3456789, SD = 3.14159265â€\nGood: â€œM = 12.3, SD = 3.1â€\nWhy: False precision implies unrealistic measurement accuracy.\n\n\nâŒ Inconsistent Decimal Places\nBad: â€œM = 12, SD = 3.142, t = 8.234567â€\nGood: â€œM = 12.0, SD = 3.1, t = 8.23â€\n\n\nâŒ Forgetting Units\nBad: â€œBiology papers had more passive voice (12.3)â€\nGood: â€œBiology papers had more passive voice (12.3 per 1,000 words)â€\n\n\nâŒ Screenshots of Code or Output\nNever do this: - Screenshot of Jupyter cell output - Screenshot of Python print statements - Screenshot of DataFrame.head() - Raw copy-paste from console\nInstead: - Create professional tables with great_tables - Create styled visualizations with seaborn - Export figures as high-res PNG/PDF - Format numbers in prose\n\n\nâŒ Tables/Figures Without Context\nBad: Just inserting Figure 1 with no mention in text\nGood: â€œFigure 1 shows that passive voice distinguishes disciplines, with Biology using 58% more passive constructions than English.â€",
    "crumbs": [
      "Reports",
      "Statistical Reporting Guide"
    ]
  },
  {
    "objectID": "reports/statistical-reporting-guide.html#quick-reference-common-statistical-symbols",
    "href": "reports/statistical-reporting-guide.html#quick-reference-common-statistical-symbols",
    "title": "Statistical Reporting Guide",
    "section": "Quick Reference: Common Statistical Symbols",
    "text": "Quick Reference: Common Statistical Symbols\n\n\n\nSymbol\nMeaning\nExample\n\n\n\n\nM\nMean\nM = 12.3\n\n\nSD\nStandard deviation\nSD = 3.1\n\n\nSE\nStandard error\nSE = 0.4\n\n\nn\nSample size (subset)\nn = 80\n\n\nN\nTotal sample size\nN = 160\n\n\nt\nt-statistic\nt(158) = 8.2\n\n\nF\nF-statistic (ANOVA)\nF(2, 237) = 45.3\n\n\nÏ‡Â²\nChi-square\nÏ‡Â²(3) = 12.4\n\n\nr\nPearson correlation\nr = 0.52\n\n\nÏ\nSpearman correlation\nÏ = 0.48\n\n\np\nProbability value\np &lt; 0.001\n\n\nd\nCohenâ€™s d (effect size)\nd = 1.62\n\n\nÎ·Â²\nEta-squared (effect size)\nÎ·Â² = 0.28\n\n\nRÂ²\nR-squared (variance explained)\nRÂ² = 0.34\n\n\nÎ²\nBeta coefficient (regression)\nÎ² = 0.45\n\n\nLL\nLog-likelihood (keyness)\nLL = 156.3\n\n\ndf\nDegrees of freedom\ndf = 158\n\n\nCI\nConfidence interval\n95% CI [0.29, 0.61]",
    "crumbs": [
      "Reports",
      "Statistical Reporting Guide"
    ]
  },
  {
    "objectID": "reports/statistical-reporting-guide.html#software-and-data-citations",
    "href": "reports/statistical-reporting-guide.html#software-and-data-citations",
    "title": "Statistical Reporting Guide",
    "section": "Software and Data Citations",
    "text": "Software and Data Citations\nAlways cite the tools and data you used:\nSoftware: &gt; Statistical analyses were performed in Python 3.11 using polars 0.19 (Vink, 2023), spaCy 3.7 (Honnibal et al., 2020), pybiber 0.1.5 (Brown, 2024), scikit-learn 1.3 (Pedregosa et al., 2011), and great_tables 0.8 (Mock, 2024).\nData: &gt; We analyzed the Michigan Corpus of Upper-level Student Papers (MICUSP; RÃ¶mer & Oâ€™Donnell, 2011), available at https://elicorpora.info/.\nGet citation info:\n# Most packages provide citation guidance\nimport spacy\n# Check package documentation for citation",
    "crumbs": [
      "Reports",
      "Statistical Reporting Guide"
    ]
  },
  {
    "objectID": "reports/statistical-reporting-guide.html#final-checklist",
    "href": "reports/statistical-reporting-guide.html#final-checklist",
    "title": "Statistical Reporting Guide",
    "section": "Final Checklist",
    "text": "Final Checklist\nBefore submitting your report, verify:\nâœ… All statistics include: - Descriptive statistics (M, SD, n) - Test statistics with df where applicable - Effect sizes (d, Î·Â², etc.) - P-values - Interpretation in plain language\nâœ… All tables include: - Number and descriptive caption - Clear headers with units - In-text reference - Appropriate precision (1-2 decimals)\nâœ… All figures include: - Number and descriptive caption - Axis labels with units - Legend (if multiple groups) - Professional styling (no screenshots!) - In-text reference\nâœ… No screenshots of: - Jupyter/Colab cells - Python console output - Raw DataFrames - Matplotlib default plots\nâœ… Citations for: - Software packages - Data sources - Statistical tests - Prior research\n\nRemember: The goal is clear communication of your findings. Statistics should support your argument, not overwhelm it. When in doubt, ask: â€œCan a reader understand this number without referring to my code?â€",
    "crumbs": [
      "Reports",
      "Statistical Reporting Guide"
    ]
  },
  {
    "objectID": "tutorials/using-colab.html",
    "href": "tutorials/using-colab.html",
    "title": "15Â  Using Google Colab",
    "section": "",
    "text": "15.1 Why Colab?",
    "crumbs": [
      "Course Resources",
      "<span class='chapter-number'>15</span>Â  <span class='chapter-title'>Using Google Colab</span>"
    ]
  },
  {
    "objectID": "tutorials/using-colab.html#why-colab",
    "href": "tutorials/using-colab.html#why-colab",
    "title": "15Â  Using Google Colab",
    "section": "",
    "text": "Advantages of Colab\n\n\n\n\nNo installation: Works entirely in your browser\nFree GPU access: For computationally intensive tasks (though we wonâ€™t need this much)\nEasy sharing: Notebooks can be shared like Google Docs\nPersistent storage: Save notebooks to your Google Drive\nPre-installed libraries: Most data science packages already available",
    "crumbs": [
      "Course Resources",
      "<span class='chapter-number'>15</span>Â  <span class='chapter-title'>Using Google Colab</span>"
    ]
  },
  {
    "objectID": "tutorials/using-colab.html#getting-started",
    "href": "tutorials/using-colab.html#getting-started",
    "title": "15Â  Using Google Colab",
    "section": "15.2 Getting Started",
    "text": "15.2 Getting Started\n\n15.2.1 1. Connect Colab to Google Drive (Optional but Recommended)\nWhile not strictly necessary, connecting Colab to Google Drive makes it easier to access and save notebooks.\nInstall the Colab app:\n\nGo to Google Drive\nClick the + New button (top left)\nSelect More â†’ Connect more apps\nSearch for â€œColaboratoryâ€\nClick Install\nGrant permissions when prompted\n\nNow you can open .ipynb (notebook) files directly from Drive using Colab.\n\n\n15.2.2 2. Opening Mini Labs\nFor this course, youâ€™ll typically open Mini Labs in one of two ways:\nOption A: Via Colab Badge (Recommended)\nEach tutorial has a Colab badge at the top that looks like this:\n  \nClicking this badge opens the notebook directly in Colab from the course GitHub repository.\nOption B: Upload to Drive\n\nDownload the .ipynb file from the course repository\nUpload to your Google Drive\nDouble-click to open with Colab\n\n\n\n\n\n\n\nAlways Make a Copy!\n\n\n\nWhen you open a notebook from the course GitHub, youâ€™ll see a warning that you canâ€™t save changes. Click File â†’ Save a copy in Drive to create your own editable version.\nThis saves your work and lets you experiment without affecting the original.",
    "crumbs": [
      "Course Resources",
      "<span class='chapter-number'>15</span>Â  <span class='chapter-title'>Using Google Colab</span>"
    ]
  },
  {
    "objectID": "tutorials/using-colab.html#understanding-the-colab-interface",
    "href": "tutorials/using-colab.html#understanding-the-colab-interface",
    "title": "15Â  Using Google Colab",
    "section": "15.3 Understanding the Colab Interface",
    "text": "15.3 Understanding the Colab Interface\nWhen you first open a Colab notebook, youâ€™ll see an interface with several key components. Hereâ€™s what everything does:\n\n\n\nColab Interface Overview\n\n\nKey Interface Elements:\n\nFiles (ğŸ“): Upload datasets and files from your computer or Google Drive. Files uploaded here are temporary.\nCode Snippets: Find pre-written code for common tasks like installing libraries or importing data.\nRun Cell (â–¶ï¸): Click to execute the code in the adjacent cell. Shortcut: Shift + Enter runs the cell and moves to the next one.\nTable of Contents: Create and navigate between sections in your notebook for better organization.\nMenu Bar: Standard application menu for file operations. Key menus:\n\nFile: Open, upload, download, save notebooks\nEdit: Cut, copy, paste cells\nRuntime: Manage your Python environment\n\nFile Name: Click to rename your notebook. Donâ€™t change the .ipynb extension!\nInsert Code Cell (+Code): Add a new code cell below the currently selected cell.\nInsert Text Cell (+Text): Add a new text (Markdown) cell below the currently selected cell.\nCell: The main work area where you write code or text. Click inside to edit.\nOutput: Results from running code appear here, including text, tables, graphs, and error messages.\nClear Output: Remove the output display (doesnâ€™t delete the cell or code).\n\n\n\n\nAdditional Colab Interface Elements\n\n\nAdditional Interface Elements:\n\nRAM and Disk: Shows your resource usage. All code runs on Googleâ€™s servers, not your computer. This means even slow computers can handle large computations. However, Google limits RAM and disk space per user, so be mindful with very large datasets.\nLink to Cell: Creates a URL that links directly to the selected cell. Useful for sharing specific parts of notebooks.\nComment: Add a comment about the cell (for collaboration or notes). This is different from code comments inside the cell.\nSettings: Customize notebook appearanceâ€”theme (dark/light), font type, font size, indentation width, and more.\nDelete Cell: Permanently removes the selected cell. Use with caution!\nMore Options (â‹®): Additional cell operations:\n\nCut/copy cell\nAdd form fields\nHide code (show only output)\n\n\n\n\n\n\n\n\nPro Tip: Resource Management\n\n\n\nKeep an eye on the RAM/Disk indicator (12). If youâ€™re running out of memory: - Restart the runtime (Runtime â†’ Restart runtime) - Delete large variables you no longer need - Process data in smaller chunks\n\n\n\n15.3.1 The Notebook Structure\nA Colab notebook consists of cells, which come in two types:\nCode Cells (have a play button):\n# This is a code cell\nprint(\"Hello, Colab!\")\nText Cells (formatted text): - Used for explanations, instructions, and notes - Written in Markdown (a simple formatting language) - Youâ€™re reading text cells in this tutorial right now\n\n\n15.3.2 Running Code Cells\nThere are several ways to run a code cell:\n\nClick the play button (â–¶ï¸) on the left side of the cell\nKeyboard shortcut: Shift + Enter (runs cell and moves to next)\nAlternative: Ctrl + Enter (runs cell, stays in same cell)\n\nWhat happens when you run a cell: - Code executes on Googleâ€™s servers - Output appears below the cell - Any variables or data created remain in memory for the session\n\n\n\n\n\n\nExecution Order Matters\n\n\n\nCells can be run in any order, but the logical order matters. If Cell 2 uses a variable defined in Cell 1, you must run Cell 1 first.\nColab shows execution numbers like [1], [2], etc. to track run order.\n\n\n\n\n15.3.3 Working with Code\nEditing code cells: - Click inside the cell to edit - Colab provides syntax highlighting and auto-completion - Press Tab for code suggestions - Ctrl + / to comment/uncomment lines\nCommon operations: - Add a cell: Click + Code or + Text buttons - Delete a cell: Click the trash icon (hover over cell for menu) - Move cells: Click and drag the cell - Copy/paste: Use standard keyboard shortcuts",
    "crumbs": [
      "Course Resources",
      "<span class='chapter-number'>15</span>Â  <span class='chapter-title'>Using Google Colab</span>"
    ]
  },
  {
    "objectID": "tutorials/using-colab.html#running-mini-labs-step-by-step",
    "href": "tutorials/using-colab.html#running-mini-labs-step-by-step",
    "title": "15Â  Using Google Colab",
    "section": "15.4 Running Mini Labs: Step-by-Step",
    "text": "15.4 Running Mini Labs: Step-by-Step\nHereâ€™s a typical workflow for completing a Mini Lab:\n\n15.4.1 Step 1: Open the Lab\nClick the Colab badge from the tutorial or mini labs page.\n\n\n15.4.2 Step 2: Save Your Copy\nFile â†’ Save a copy in Drive\nThis creates a version you can edit. The copy will open automatically.\n\n\n15.4.3 Step 3: Read the Instructions\nEach Mini Lab starts with text cells explaining: - What youâ€™ll learn - Prerequisites - Expected outcomes\n\n\n15.4.4 Step 4: Run Setup Cells\nMany labs start with installation cells that look like this:\n%%capture\n!pip install polars matplotlib\n\n%%capture suppresses verbose output\n!pip install installs Python packages\nRun these cells first (they may take 30-60 seconds)\n\n\n\n\n\n\n\nRestart If Needed\n\n\n\nIf a package installation requires a runtime restart, youâ€™ll see a message. Click the RESTART RUNTIME button when prompted. Youâ€™ll then need to re-run the installation cell.\n\n\n\n\n15.4.5 Step 5: Work Through the Lab\n\nRead each text cell carefully\nRun each code cell in order\nExamine the outputs\nTry to understand what each code block does\nComplete any questions or exercises\n\n\n\n15.4.6 Step 6: Experiment (Optional but Encouraged!)\nAfter completing the lab: - Modify code to see what happens - Try different parameter values - Add your own cells with notes or experiments - Test edge cases\nYour copy is yours to break! Experimentation is how you learn.\n\n\n15.4.7 Step 7: Save Your Work\nColab auto-saves to your Drive periodically, but you can manually save: - File â†’ Save (or Ctrl + S)",
    "crumbs": [
      "Course Resources",
      "<span class='chapter-number'>15</span>Â  <span class='chapter-title'>Using Google Colab</span>"
    ]
  },
  {
    "objectID": "tutorials/using-colab.html#common-issues-and-solutions",
    "href": "tutorials/using-colab.html#common-issues-and-solutions",
    "title": "15Â  Using Google Colab",
    "section": "15.5 Common Issues and Solutions",
    "text": "15.5 Common Issues and Solutions\n\n15.5.1 Issue: â€œNo module named Xâ€\nProblem: You tried to import a package that isnâ€™t installed.\nSolution: Add a cell with:\n!pip install package-name\nThen run it and re-run the import cell.\n\n\n15.5.2 Issue: â€œName â€˜variableâ€™ is not definedâ€\nProblem: Youâ€™re using a variable that hasnâ€™t been created yet.\nSolution: - Make sure youâ€™ve run all previous cells in order - Check that the variable is defined in an earlier cell - Look at execution numbers to see run order\n\n\n15.5.3 Issue: Code runs forever (spinning circle)\nProblem: Cell is taking too long or stuck in an infinite loop.\nSolution: - Click the Stop button (square icon) next to the cell - Check your code for infinite loops - For long operations (like parsing large corpora), be patientâ€”some cells take 2-5 minutes\n\n\n15.5.4 Issue: â€œRuntime disconnectedâ€\nProblem: Inactive for too long, or Colab session expired.\nSolution: - Click RECONNECT button - Youâ€™ll need to re-run cells from the top - (Another reason to run labs in one sitting when possible!)\n\n\n15.5.5 Issue: Lost my changes\nProblem: Forgot to save a copy, or closed without saving.\nSolution: - Always click â€œSave a copy in Driveâ€ at the start - Check Driveâ€™s â€œRecentâ€ or â€œColab Notebooksâ€ folder - Colab auto-saves, so recent changes should be there",
    "crumbs": [
      "Course Resources",
      "<span class='chapter-number'>15</span>Â  <span class='chapter-title'>Using Google Colab</span>"
    ]
  },
  {
    "objectID": "tutorials/using-colab.html#keyboard-shortcuts",
    "href": "tutorials/using-colab.html#keyboard-shortcuts",
    "title": "15Â  Using Google Colab",
    "section": "15.6 Keyboard Shortcuts",
    "text": "15.6 Keyboard Shortcuts\nSpeed up your workflow with these shortcuts:\nExecution: - Shift + Enter: Run cell and move to next - Ctrl + Enter: Run cell, stay in place - Ctrl + Shift + Enter: Run cell, insert new cell below\nCell operations: - Ctrl + M, B: Insert cell below - Ctrl + M, A: Insert cell above - Ctrl + M, D: Delete selected cell - Ctrl + M, M: Convert cell to text (Markdown) - Ctrl + M, Y: Convert cell to code\nEditing: - Ctrl + /: Comment/uncomment code - Tab: Auto-complete - Shift + Tab: Show function documentation\nView all shortcuts: Ctrl + M, H",
    "crumbs": [
      "Course Resources",
      "<span class='chapter-number'>15</span>Â  <span class='chapter-title'>Using Google Colab</span>"
    ]
  },
  {
    "objectID": "tutorials/using-colab.html#working-with-data",
    "href": "tutorials/using-colab.html#working-with-data",
    "title": "15Â  Using Google Colab",
    "section": "15.7 Working with Data",
    "text": "15.7 Working with Data\nMini Labs often load data from URLs (like our course GitHub repository):\nimport polars as pl\n\n# Load data directly from web\ndf = pl.read_parquet(\"https://github.com/browndw/humanities_analytics/raw/refs/heads/main/data/data_tables/sample_corpus.parquet\")\nThis approach is preferred for this course because: - No need to mount Google Drive - Data stays current with course repository - Simpler and more reliable\n\n\n\n\n\n\nUploading Your Own Data (Optional)\n\n\n\nIf you want to work with your own data files:\n\nClick the folder icon (ğŸ“) in the left sidebar\nClick the upload button (ğŸ“¤)\nSelect your file\n\nNote: Uploaded files are temporary and disappear when the runtime disconnects!",
    "crumbs": [
      "Course Resources",
      "<span class='chapter-number'>15</span>Â  <span class='chapter-title'>Using Google Colab</span>"
    ]
  },
  {
    "objectID": "tutorials/using-colab.html#understanding-runtime-and-sessions",
    "href": "tutorials/using-colab.html#understanding-runtime-and-sessions",
    "title": "15Â  Using Google Colab",
    "section": "15.8 Understanding Runtime and Sessions",
    "text": "15.8 Understanding Runtime and Sessions\nWhat is a runtime? - The Python environment running your code - Includes memory, variables, installed packages - Allocated when you open a notebook, released when you close it\nImportant facts: - Free Colab runtimes have time limits (usually 12 hours) - Inactive sessions disconnect after ~90 minutes - All variables/data in memory are lost when runtime disconnects - Installed packages must be reinstalled each session\nBest practice: Complete labs in one sitting when possible, or at least reach a good stopping point before closing.",
    "crumbs": [
      "Course Resources",
      "<span class='chapter-number'>15</span>Â  <span class='chapter-title'>Using Google Colab</span>"
    ]
  },
  {
    "objectID": "tutorials/using-colab.html#tips-for-success",
    "href": "tutorials/using-colab.html#tips-for-success",
    "title": "15Â  Using Google Colab",
    "section": "15.9 Tips for Success",
    "text": "15.9 Tips for Success\n\n15.9.1 For Complete Beginners\n\nRun cells in order - Donâ€™t skip around at first\nRead error messages - They often tell you exactly whatâ€™s wrong\nStart simple - Get the basic workflow down before experimenting\nAsk for help - If stuck for more than 10 minutes, reach out\n\n\n\n15.9.2 For Experienced Coders\n\nExperiment freely - Your copy wonâ€™t break the original\nRead instructions anyway - Some cells require specific setup\nCheck dependencies - Package versions may differ from your local setup\nShare solutions - Help others debug issues\n\n\n\n15.9.3 For Everyone\n\nSave early, save often - Make that copy in Drive immediately\nName your notebooks - â€œCopy of Mini_Lab_01â€ isnâ€™t helpful later\nAdd notes - Use text cells to document your thinking\nCompare outputs - Check if your results match expected outputs\nRestart when confused - Sometimes a fresh runtime clears mysterious errors",
    "crumbs": [
      "Course Resources",
      "<span class='chapter-number'>15</span>Â  <span class='chapter-title'>Using Google Colab</span>"
    ]
  },
  {
    "objectID": "tutorials/using-colab.html#privacy-and-sharing",
    "href": "tutorials/using-colab.html#privacy-and-sharing",
    "title": "15Â  Using Google Colab",
    "section": "15.10 Privacy and Sharing",
    "text": "15.10 Privacy and Sharing\nYour notebooks: - Stored in your Google Drive - Private by default - You control sharing permissions\nTo share your work: 1. Click the Share button (top right) 2. Add instructor email or get shareable link 3. Set permissions (view, comment, or edit)\nWhat we can see: - Only notebooks you explicitly share with instructors - Not your entire Drive - Not your browsing or other Google activity",
    "crumbs": [
      "Course Resources",
      "<span class='chapter-number'>15</span>Â  <span class='chapter-title'>Using Google Colab</span>"
    ]
  },
  {
    "objectID": "tutorials/using-colab.html#differences-from-local-python",
    "href": "tutorials/using-colab.html#differences-from-local-python",
    "title": "15Â  Using Google Colab",
    "section": "15.11 Differences from Local Python",
    "text": "15.11 Differences from Local Python\nIf youâ€™ve used Python on your computer before, note these Colab-specific features:\nSpecial commands: - !command - Runs shell commands (like !pip install) - %%capture - Suppresses cell output - %load_ext - Loads IPython extensions\nPre-installed libraries: - NumPy, Pandas, Matplotlib already available - But course-specific packages (docuscospacy, moodswing) need installation\nFile system: - Youâ€™re on a Linux VM, not your local machine - Files uploaded are temporary - Use URLs for data loading in this course",
    "crumbs": [
      "Course Resources",
      "<span class='chapter-number'>15</span>Â  <span class='chapter-title'>Using Google Colab</span>"
    ]
  },
  {
    "objectID": "tutorials/using-colab.html#getting-help",
    "href": "tutorials/using-colab.html#getting-help",
    "title": "15Â  Using Google Colab",
    "section": "15.12 Getting Help",
    "text": "15.12 Getting Help\nWithin Colab: - Hover over function names for documentation - Shift + Tab shows function parameters - ?function_name displays full documentation\nCourse resources: - Tutorial pages explain concepts - Mini Lab instructions guide you step-by-step - Discussion forums for peer help - Office hours for instructor support\nGeneral Python help: - Python documentation - Stack Overflow (search before asking) - Polars documentation",
    "crumbs": [
      "Course Resources",
      "<span class='chapter-number'>15</span>Â  <span class='chapter-title'>Using Google Colab</span>"
    ]
  },
  {
    "objectID": "tutorials/using-colab.html#next-steps",
    "href": "tutorials/using-colab.html#next-steps",
    "title": "15Â  Using Google Colab",
    "section": "15.13 Next Steps",
    "text": "15.13 Next Steps\nReady to get started? Try:\n\nOpen Mini Lab 01 via the Colab badge\nSave a copy to your Drive\nWork through the first few cells\nGet comfortable with the interface\n\n\n\n\n\n\n\nYouâ€™ve Got This!\n\n\n\nColab might feel unfamiliar at first, but youâ€™ll quickly get comfortable. Most students find it more convenient than installing Python locally, and it ensures everyone has the same environment.\nThe mini labs are designed for learners at all levelsâ€”just work through them at your own pace.",
    "crumbs": [
      "Course Resources",
      "<span class='chapter-number'>15</span>Â  <span class='chapter-title'>Using Google Colab</span>"
    ]
  },
  {
    "objectID": "tutorials/using-colab.html#quick-reference-card",
    "href": "tutorials/using-colab.html#quick-reference-card",
    "title": "15Â  Using Google Colab",
    "section": "15.14 Quick Reference Card",
    "text": "15.14 Quick Reference Card\nESSENTIAL OPERATIONS\n- Save a copy: File â†’ Save a copy in Drive\n- Run cell: Shift + Enter\n- Stop execution: Click stop button (â– )\n- Add code cell: Click + Code\n- Add text cell: Click + Text\n\nCOMMON ISSUES\n- Import error â†’ !pip install package-name\n- Name error â†’ Run earlier cells first\n- Stuck â†’ Click stop, check for infinite loops\n- Disconnected â†’ Reconnect, re-run from top\n\nREMEMBER\nâœ“ Save a copy first!\nâœ“ Run cells in order\nâœ“ Be patient with long operations\nâœ“ Experiment after completing the lab\nâœ“ Errors are learning opportunities!\nNow youâ€™re ready to dive into the mini labs. Happy coding!",
    "crumbs": [
      "Course Resources",
      "<span class='chapter-number'>15</span>Â  <span class='chapter-title'>Using Google Colab</span>"
    ]
  },
  {
    "objectID": "resources/packages.html",
    "href": "resources/packages.html",
    "title": "Python Packages for This Course",
    "section": "",
    "text": "Core Packages",
    "crumbs": [
      "Course Resources",
      "Python Packages for This Course"
    ]
  },
  {
    "objectID": "resources/packages.html#core-packages",
    "href": "resources/packages.html#core-packages",
    "title": "Python Packages for This Course",
    "section": "",
    "text": "docuscospacy\nPurpose: Rhetorical and functional text analysis using DocuScope categories\nWhat it does:\ndocuscospacy provides automated tagging of texts using the DocuScope dictionary, a comprehensive taxonomy of rhetorical and functional categories developed by David Kaufer and Suguru Ishizaki. The package includes a specially-trained spaCy model that tags text for both part-of-speech and rhetorical categories.\nKey features:\n\n67 million+ pattern dictionary covering rhetorical moves, speech acts, and functional language\nPre-trained spaCy model (en_docusco_spacy) with CLAWS7 tagset\nHigh-performance processing with Polars DataFrames\nBuilt-in corpus analysis functions (frequencies, keyness, collocations)\nIntegration with topic modeling and visualization tools\n\nExample use cases:\n\nAnalyzing persuasive strategies in political speeches\nComparing formality levels across academic disciplines\nIdentifying stance markers in student writing\nTracking rhetorical patterns in literary genres\n\nDocumentation: docuscospacy.readthedocs.org\nGitHub: github.com/browndw/docuscospacy\nInteractive Demo: docuscope-ca.eberly.cmu.edu\nInstallation:\n# Install the package\npip install docuscospacy\n\n# Install the pre-trained model\npip install \"en_docusco_spacy @ https://huggingface.co/browndw/en_docusco_spacy/resolve/main/en_docusco_spacy-1.5-py3-none-any.whl\"\n\n\n\npybiber\nPurpose: Multi-Dimensional Analysis (MDA) of text registers and genres\nWhat it does:\npybiber implements Douglas Biberâ€™s influential methodology for analyzing linguistic variation across text types. It automatically extracts 67 lexicogrammatical features (tense markers, pronouns, subordination, modals, etc.) and performs factor analysis to identify underlying dimensions of variation.\nKey features:\n\nAutomated extraction of 67 linguistic features from Biber (1988)\nFull implementation of Multi-Dimensional Analysis methodology\nPrincipal Component Analysis (PCA) as alternative to MDA\nBuilt-in visualization tools (scree plots, dimension plots, biplots)\nHigh-performance processing with spaCy and Polars\nSupport for custom corpora and comparative analysis\n\nExample use cases:\n\nComparing academic writing across disciplines\nAnalyzing register variation in social media\nTracking language change over time\nClassifying text genres based on linguistic features\nStudying differences between spoken and written language\n\nDocumentation: browndw.github.io/pybiber\nGitHub: github.com/browndw/pybiber\nRelated: See pseudobibeR for R implementation\nInstallation:\npip install pybiber\npython -m spacy download en_core_web_sm\n\n\n\nmoodswing\nPurpose: Sentiment trajectory analysis for narrative texts\nWhat it does:\nInspired by Matthew Jockersâ€™ syuzhet R package, moodswing tracks emotional arcs over the course of narratives. It provides both dictionary-based and neural approaches to sentiment analysis, with specialized tools for smoothing and visualizing sentiment trajectories.\nKey features:\n\nFour proven sentiment lexicons (Syuzhet, AFINN, Bing, NRC)\nDictionary-based and spaCy-based sentiment scoring\nDCT (Discrete Cosine Transform) smoothing for narrative structure\nRolling window smoothing options\nPublication-ready visualization tools\nSupport for multi-language analysis (via NRC lexicon)\n\nExample use cases:\n\nVisualizing emotional arcs in novels and memoirs\nComparing narrative structures across genres\nAnalyzing sentiment patterns in social media narratives\nStudying emotional progression in political speeches\nIdentifying plot structure through sentiment\n\nDocumentation: browndw.github.io/moodswing\nGitHub: github.com/browndw/moodswing\nInstallation:\npip install moodswing\n# Optional: for spaCy support\npython -m spacy download en_core_web_sm\n\n\n\ngoogle_ngrams\nPurpose: Diachronic analysis using Google Books Ngram data\nWhat it does:\ngoogle_ngrams provides tools for downloading, processing, and analyzing the Google Books Ngram dataset, enabling large-scale studies of language change over centuries. It includes functions for smoothing, clustering, and visualizing temporal trends.\nKey features:\n\nDirect access to Google Books Ngram data (1500-2019)\nMultiple corpus options (English, Spanish, German, French, etc.)\nTime-series analysis and visualization\nClustering algorithms for grouping similar trajectories\nStatistical smoothing functions\nExport capabilities for further analysis\n\nExample use cases:\n\nTracking word frequency changes over centuries\nComparing usage patterns across historical periods\nIdentifying semantic shifts in vocabulary\nAnalyzing the rise and fall of cultural concepts\nStudying historical discourse patterns\n\nDocumentation: browndw.github.io/google_ngrams (coming soon)\nGitHub: github.com/browndw/google_ngrams\nInstallation:\npip install google-ngrams",
    "crumbs": [
      "Course Resources",
      "Python Packages for This Course"
    ]
  },
  {
    "objectID": "resources/packages.html#essential-dependencies",
    "href": "resources/packages.html#essential-dependencies",
    "title": "Python Packages for This Course",
    "section": "Essential Dependencies",
    "text": "Essential Dependencies\nThese packages rely on several widely-used Python libraries that you should be familiar with:\n\nspaCy\nPurpose: Industrial-strength Natural Language Processing\nWebsite: spacy.io\nUse in course: Part-of-speech tagging, dependency parsing, named entity recognition\nInstallation: pip install spacy + python -m spacy download en_core_web_sm\n\n\nPolars\nPurpose: Fast DataFrame library (similar to pandas but faster)\nWebsite: pola.rs\nUse in course: Data manipulation, corpus processing, feature extraction\nInstallation: pip install polars",
    "crumbs": [
      "Course Resources",
      "Python Packages for This Course"
    ]
  },
  {
    "objectID": "resources/packages.html#machine-learning-statistics",
    "href": "resources/packages.html#machine-learning-statistics",
    "title": "Python Packages for This Course",
    "section": "Machine Learning & Statistics",
    "text": "Machine Learning & Statistics\n\nscikit-learn\nPurpose: Machine learning and statistical modeling\nWebsite: scikit-learn.org\nWhat it does:\nPythonâ€™s most popular machine learning library, providing tools for classification, clustering, dimensionality reduction, and model evaluation.\nUse in course: - Classification (Mini Lab 12, Classification tutorial): Random Forest classifiers, train/test splits, accuracy metrics - Clustering (Cluster Analysis tutorial): K-means clustering, hierarchical clustering, silhouette scores - Dimensionality reduction (Contextual Embeddings, Cluster Analysis): PCA for visualization, TruncatedSVD for topic modeling - Text vectorization: TF-IDF vectorization for document similarity - Model evaluation: Confusion matrices, classification reports, cross-validation\nKey modules used: - sklearn.ensemble.RandomForestClassifier - Classification - sklearn.cluster.KMeans - K-means clustering - sklearn.decomposition.PCA - Principal Component Analysis - sklearn.model_selection.train_test_split - Data splitting - sklearn.metrics - Accuracy, precision, recall, F1-score\nInstallation: pip install scikit-learn\n\n\nsentence-transformers\nPurpose: State-of-the-art sentence and document embeddings\nWebsite: sbert.net\nWhat it does:\nProvides pre-trained models for generating semantic embeddings of sentences and documents. Built on transformer models like BERT, these embeddings capture meaning and enable semantic similarity comparisons.\nUse in course: - Contextual embeddings (Contextual Embeddings tutorial): Generating sentence embeddings with all-MiniLM-L6-v2 - Semantic similarity: Finding similar documents based on meaning - Clustering: Grouping texts by semantic content - Classification: Using embeddings as features for genre prediction\nExample models: - all-MiniLM-L6-v2 - Fast, lightweight, good for most tasks - paraphrase-MiniLM-L6-v2 - Optimized for paraphrase detection\nInstallation: pip install sentence-transformers\n\n\nstatsmodels\nPurpose: Statistical modeling and hypothesis testing\nWebsite: statsmodels.org\nWhat it does:\nProvides statistical tests, regression models, and diagnostic tools for rigorous statistical analysis.\nUse in course: - VIF (Variance Inflation Factor): Testing for multicollinearity in regression (Correlation tutorial) - Time series analysis: ARIMA models, trend detection (Time Series tutorial) - ANOVA: Testing group differences (ANOVA tutorial) - Regression diagnostics: Checking model assumptions\nInstallation: pip install statsmodels\n\n\nscipy\nPurpose: Scientific computing and statistical functions\nWebsite: scipy.org\nWhat it does:\nCore library for scientific computing, providing statistical tests, optimization, and hierarchical clustering.\nUse in course: - Hierarchical clustering (Cluster Analysis tutorial): scipy.cluster.hierarchy for dendrograms, linkage methods - Statistical tests: T-tests, chi-square, correlation tests - Distance metrics: Cosine distance, Euclidean distance for clustering\nInstallation: pip install scipy",
    "crumbs": [
      "Course Resources",
      "Python Packages for This Course"
    ]
  },
  {
    "objectID": "resources/packages.html#network-visualization",
    "href": "resources/packages.html#network-visualization",
    "title": "Python Packages for This Course",
    "section": "Network & Visualization",
    "text": "Network & Visualization\n\nnetworkx\nPurpose: Network analysis and graph visualization\nWebsite: networkx.org\nWhat it does:\nCreates and analyzes network structures, useful for visualizing relationships between linguistic features or texts.\nUse in course: - Correlation networks (Correlation tutorial): Visualizing feature correlations as network graphs - Word co-occurrence networks: Showing which words appear together - Document similarity networks: Connecting similar texts\nInstallation: pip install networkx\n\n\nmatplotlib\nPurpose: Fundamental plotting and visualization\nWebsite: matplotlib.org\nUse in course: Creating charts, graphs, and visualizations\nInstallation: pip install matplotlib\n\n\nseaborn\nPurpose: Statistical data visualization (built on matplotlib)\nWebsite: seaborn.pydata.org\nUse in course: Heatmaps, boxplots, distribution plots, styled figures\nInstallation: pip install seaborn\n\n\ngreat_tables\nPurpose: Publication-quality table formatting\nWebsite: posit-dev.github.io/great_tables\nUse in course: Formatting results for reports and papers, LaTeX export\nInstallation: pip install great_tables",
    "crumbs": [
      "Course Resources",
      "Python Packages for This Course"
    ]
  },
  {
    "objectID": "resources/packages.html#word-embeddings-optional",
    "href": "resources/packages.html#word-embeddings-optional",
    "title": "Python Packages for This Course",
    "section": "Word Embeddings (Optional)",
    "text": "Word Embeddings (Optional)\n\ngensim\nPurpose: Topic modeling and word embeddings\nWebsite: radimrehurek.com/gensim\nWhat it does:\nProvides implementations of word2vec, doc2vec, and other embedding algorithms for learning word representations from text.\nUse in course: - Word2Vec (Mini Lab 9): Training word embeddings on custom corpora - Semantic similarity: Finding words with similar meanings - Analogies: Exploring semantic relationships (king - man + woman â‰ˆ queen)\nInstallation: pip install gensim\nNote: While covered in Mini Lab 9, most semantic tasks in tutorials use sentence-transformers instead, which provides pre-trained models without requiring corpus-specific training.",
    "crumbs": [
      "Course Resources",
      "Python Packages for This Course"
    ]
  },
  {
    "objectID": "resources/packages.html#learning-path",
    "href": "resources/packages.html#learning-path",
    "title": "Python Packages for This Course",
    "section": "Learning Path",
    "text": "Learning Path\nWeeks 1-3: Core Text Processing\n\nspaCy + docuscospacy: Basic NLP and rhetorical analysis\npolars: Data manipulation\nmatplotlib + seaborn: Visualization basics\nLearn frequency analysis, keyness, collocations\n\nWeeks 4-6: Sentiment & Register Analysis\n\nmoodswing: Sentiment trajectories and narrative arcs\npybiber: Multi-dimensional analysis of registers\nscipy: Statistical tests (t-tests, ANOVA)\nstatsmodels: Correlation, VIF, regression diagnostics\n\nWeeks 7-9: Advanced Methods\n\nscikit-learn: Classification, clustering, PCA\nsentence-transformers: Contextual embeddings\nnetworkx: Network visualization\ngreat_tables: Professional reporting\n\nWeeks 10-12: Integration & Projects\n\nCombine multiple methods for robust analysis\nOptional: gensim for custom word2vec models\nOptional: google_ngrams for diachronic studies\nApply full toolkit to final projects",
    "crumbs": [
      "Course Resources",
      "Python Packages for This Course"
    ]
  },
  {
    "objectID": "resources/packages.html#getting-help",
    "href": "resources/packages.html#getting-help",
    "title": "Python Packages for This Course",
    "section": "Getting Help",
    "text": "Getting Help\nEach package has comprehensive documentation with tutorials, API references, and examples. If you encounter issues:\n\nCheck the documentation - Most questions are answered there\nReview mini labs - See working examples of each package\nConsult the tutorials - Conceptual background on methods\nAsk questions - Use course discussion boards or office hours\nGitHub Issues - Report bugs or request features on GitHub\n\n\n\n\n\n\n\nPro Tip: Version Management\n\n\n\nWhen reporting issues or asking for help, always include: - Package version (e.g., docuscospacy==0.3.0) - Python version - Error messages (if any) - Minimal code example that reproduces the problem",
    "crumbs": [
      "Course Resources",
      "Python Packages for This Course"
    ]
  },
  {
    "objectID": "resources/data.html",
    "href": "resources/data.html",
    "title": "Course Data Resources",
    "section": "",
    "text": "Accessing Data",
    "crumbs": [
      "Course Resources",
      "Course Data Resources"
    ]
  },
  {
    "objectID": "resources/data.html#accessing-data",
    "href": "resources/data.html#accessing-data",
    "title": "Course Data Resources",
    "section": "",
    "text": "From Google Colab (Recommended)\nAll mini labs use direct URLs to load data from GitHub, which works seamlessly in Colab:\nimport polars as pl\n\n# Load a dataset directly from GitHub\ndf = pl.read_parquet('https://github.com/browndw/humanities_analytics/raw/refs/heads/main/data/data_tables/sample_corpus.parquet')\nThis approach:\n\nâœ… Requires no Google Drive mounting\nâœ… Works immediately without setup\nâœ… Always loads the current version\nâœ… Can be used from anywhere with internet\n\n\n\nFrom Your Local Machine\nIf you clone the repository, data is available at:\nhumanities_analytics/data/",
    "crumbs": [
      "Course Resources",
      "Course Data Resources"
    ]
  },
  {
    "objectID": "resources/data.html#available-datasets",
    "href": "resources/data.html#available-datasets",
    "title": "Course Data Resources",
    "section": "Available Datasets",
    "text": "Available Datasets\n\nSample Corpora\n\nsample_corpus.parquet\nDescription: A small corpus designed to replicate the Corpus of Contemporary American English (COCA) on a toy scale. Includes multiple text types with metadata.\nSize: ~1 million tokens\nText types: Academic, blog, fiction, magazine, news, spoken, TV/movies, web\nVariables: doc_id, text, text_type, tokens\nUse cases: Learning basic corpus analysis, frequency studies, keyness analysis, classification\nRelated tutorials: Corpus Basics, Keyness, Frequency & Distributions\n# Load and explore\nimport polars as pl\ndf = pl.read_parquet('https://github.com/browndw/humanities_analytics/raw/refs/heads/main/data/data_tables/sample_corpus.parquet')\nprint(df.head())\n\n\n\n\nWord Lists and Frequency Data\n\nmicusp_wordlist.csv\nDescription: Complete word frequency list from the Michigan Corpus of Upper-Level Student Papers (MICUSP)\nColumns: token, frequency, documents\nSize: ~30,000 unique word types\nUse cases: Reference frequencies for academic writing, Zipfâ€™s law demonstrations, vocabulary analysis\nRelated tutorials: Frequency & Distributions\n\n\neng_words.csv & bio_words.csv\nDescription: Specialized vocabulary lists for different disciplines\nUse cases: Domain-specific analysis, keyword extraction, vocabulary studies\n\n\npronoun_frequencies.csv\nDescription: Pronoun usage frequencies across text types\nUse cases: Register analysis, stance analysis, personal vs.Â impersonal writing\n\n\n\n\nHistorical & Literary Texts\n\nInaugural Addresses\nMultiple formats available:\n\ninaugural.tsv - Complete corpus with metadata\ninaugural_subset.csv - Selected speeches for quick analysis\ninaugural_subset.tsv - Same subset in TSV format\n\nDescription: U.S. Presidential inaugural addresses from 1789-2021\nMetadata: President, year, party, speech length\nUse cases: Diachronic analysis, political discourse, time series studies, clustering, classification\nRelated tutorials: Time Series, Cluster Analysis, Keyness\n# Load inaugural addresses\ndf = pl.read_csv('https://github.com/browndw/humanities_analytics/raw/refs/heads/main/data/data_tables/inaugural.tsv', separator='\\t')\n\n\nClassic Novels\nFull text files available in data/data_text/:\n\ngrail.txt - Monty Python and the Holy Grail screenplay\npride.txt - Jane Austenâ€™s Pride and Prejudice\n\nUse cases: Sentiment analysis, character analysis, narrative arc studies, contextual embeddings\nRelated tutorials: Sentiment & Syuzhet, Contextual Embeddings\n# Load full text\nimport requests\n\nurl = 'https://raw.githubusercontent.com/browndw/humanities_analytics/main/data/data_text/pride.txt'\nresponse = requests.get(url)\ntext = response.text\n\n\n\n\nSentiment & Narrative Data\n\nsentiment_data.tsv\nDescription: Four classic novels used in sentiment trajectory research\nNovels included:\n\nMadame Bovary (Gustave Flaubert)\nPortrait of the Artist as a Young Man (James Joyce)\nThe Rise of Silas Lapham (William Dean Howells)\n\nRagged Dick; or, Street Life in New York (Horatio Alger Jr.)\n\nUse cases: Sentiment analysis, narrative arc visualization, DCT transformation studies\nRelated tutorials: Sentiment & Syuzhet, Mini Labs 1, 11, 12\n\n\n\n\nMetadata & Reference\n\nmicusp_meta.csv\nDescription: Complete metadata for MICUSP texts\nVariables: Document ID, discipline, student level, paper type, nativeness, gender, raw text\nSize: 829 student papers\nDisciplines: Biology, Civil Engineering, Economics, Education, English, History, Industrial Engineering, Linguistics, Mechanical Engineering, Natural Resources, Nursing, Philosophy, Physics, Political Science, Psychology, Statistics, Sociology\nUse cases: Register variation studies, academic writing analysis, multidimensional analysis, disciplinary comparison, classification\nRelated tutorials: Multi-Dimensional Analysis, Correlation Analysis, ANOVA & RÂ²\n\n\n\n\nProcessed Corpora\n\nmicusp.dfm.csv\nDescription: Document-Feature Matrix from MICUSP\nFormat: Pre-processed document-term matrix with normalized frequencies\nUse cases: Skip preprocessing, jump directly to statistical analysis, topic modeling, classification\nRelated tutorials: Mini Labs 9, 12\n\n\nbrown_corpus.tsv\nDescription: Token-level data from the Brown Corpus\nVariables: doc_id, token, pos, lemma\nUse cases: Part-of-speech analysis, lemmatization studies, linguistic feature extraction, Biber feature calculation\nRelated tutorials: Multi-Dimensional Analysis, Correlation Analysis, Mini Lab 10",
    "crumbs": [
      "Course Resources",
      "Course Data Resources"
    ]
  },
  {
    "objectID": "resources/data.html#data-format-guide",
    "href": "resources/data.html#data-format-guide",
    "title": "Course Data Resources",
    "section": "Data Format Guide",
    "text": "Data Format Guide\n\nParquet Files\n\nFast loading, compressed format\nBest for large datasets\nUse pl.read_parquet() or pd.read_parquet()\n\n\n\nCSV Files\n\nHuman-readable, widely compatible\nUse pl.read_csv() or pd.read_csv()\nSpecify separator=','\n\n\n\nTSV Files\n\nTab-separated values\nUse separator='\\t' when loading\nGood for text with commas\n\n\n\nTXT Files\n\nPlain text, one document per file\nLoad with open() or requests.get()",
    "crumbs": [
      "Course Resources",
      "Course Data Resources"
    ]
  },
  {
    "objectID": "resources/data.html#example-workflows",
    "href": "resources/data.html#example-workflows",
    "title": "Course Data Resources",
    "section": "Example Workflows",
    "text": "Example Workflows\n\nQuick Frequency Analysis\nimport polars as pl\n\n# Load pre-processed wordlist\nwordlist = pl.read_csv('https://github.com/browndw/humanities_analytics/raw/refs/heads/main/data/data_tables/micusp_wordlist.csv')\n\n# View most frequent words\nprint(wordlist.head(20))\n\n\nProcessing Your Own Data\nimport docuscospacy as ds\nimport spacy\n\n# Load model\nnlp = spacy.load(\"en_docusco_spacy\")\n\n# Process your text\ndoc = nlp(\"Your text here...\")\n\n# Extract features\nfeatures = ds.corpus_features(doc)\n\n\nComparative Analysis\n# Load two corpora\nacademic = pl.read_parquet('path/to/academic.parquet')\nfiction = pl.read_parquet('path/to/fiction.parquet')\n\n# Calculate keyness\nkeyness = ds.keyness_table(academic, fiction)",
    "crumbs": [
      "Course Resources",
      "Course Data Resources"
    ]
  },
  {
    "objectID": "resources/data.html#data-ethics-attribution",
    "href": "resources/data.html#data-ethics-attribution",
    "title": "Course Data Resources",
    "section": "Data Ethics & Attribution",
    "text": "Data Ethics & Attribution\n\n\n\n\n\n\nResponsible Use\n\n\n\nWhen using these datasets in your research:\n\nCite appropriately - Many datasets have original sources that should be cited\nRespect copyright - Literary texts may have restrictions on redistribution\nConsider context - Student writing (MICUSP) was collected with consent for research\nVerify quality - External sources vary in quality and completeness\nBe transparent - Document your data sources and processing steps\n\n\n\n\nKey Citations\nMICUSP: &gt; University of Michigan. (2009). Michigan Corpus of Upper-Level Student Papers. Ann Arbor, MI: Regents of the University of Michigan.\nBrown Corpus: &gt; Francis, W.N. and KuÄera, H. (1979). Brown Corpus Manual, Brown University.\nInaugural Addresses: &gt; American Presidency Project, UC Santa Barbara",
    "crumbs": [
      "Course Resources",
      "Course Data Resources"
    ]
  },
  {
    "objectID": "resources/data.html#external-data-sources",
    "href": "resources/data.html#external-data-sources",
    "title": "Course Data Resources",
    "section": "External Data Sources",
    "text": "External Data Sources\nBeyond the course datasets, several repositories provide texts suitable for humanities computational analysis. Each has strengths and limitations to consider.\n\nProject Gutenberg\nWebsite: gutenberg.org\nWhat it is:\nOver 70,000 public domain books (primarily English literature published before 1928), all freely available. Focuses on literary classics, historical texts, and cultural heritage materials.\nStrengths: - âœ… Completely free and legal (public domain) - âœ… High-quality OCR and proofreading - âœ… Plain text format (easy to process) - âœ… Standardized structure (easy to parse) - âœ… Extensive metadata (author, title, date, genre)\nLimitations: - âŒ Copyright restrictions (pre-1928 for most works) - âŒ Limited non-English texts - âŒ Inconsistent formatting across texts - âŒ Header/footer boilerplate must be removed\nAccessing texts (simple HTTP approach):\nMost Python packages for Gutenberg build local databases, which is overkill for student projects. Instead, use direct HTTP access:\nimport requests\nimport re\n\n# Download a specific text by ID\n# Find IDs by browsing gutenberg.org\ngutenberg_id = 1342  # Pride and Prejudice\n\nurl = f'https://www.gutenberg.org/files/{gutenberg_id}/{gutenberg_id}-0.txt'\nresponse = requests.get(url)\ntext = response.text\n\n# Remove Gutenberg header/footer\n# Start of book usually marked by \"*** START OF\"\n# End usually marked by \"*** END OF\"\nstart_pattern = r'\\*\\*\\* START OF .+ \\*\\*\\*'\nend_pattern = r'\\*\\*\\* END OF .+ \\*\\*\\*'\n\nstart_match = re.search(start_pattern, text)\nend_match = re.search(end_pattern, text)\n\nif start_match and end_match:\n    clean_text = text[start_match.end():end_match.start()].strip()\nelse:\n    # Fallback: just use the whole text\n    clean_text = text\n\nprint(f\"Retrieved {len(clean_text)} characters\")\nFinding book IDs: 1. Search gutenberg.org for your title 2. Look at the URL: gutenberg.org/ebooks/1342 â† this is the ID 3. Try https://www.gutenberg.org/files/1342/1342-0.txt (UTF-8 version) 4. If that fails, try 1342.txt or check the bookâ€™s page for download links\nBatch downloading:\n# Download multiple books\nbook_ids = {\n    'pride_prejudice': 1342,\n    'frankenstein': 84,\n    'dracula': 345,\n    'moby_dick': 2701\n}\n\ncorpus = {}\nfor name, book_id in book_ids.items():\n    url = f'https://www.gutenberg.org/files/{book_id}/{book_id}-0.txt'\n    try:\n        response = requests.get(url)\n        corpus[name] = response.text\n        print(f\"âœ“ Downloaded {name}\")\n    except:\n        print(f\"âœ— Failed to download {name}\")\nUse cases for this course: - Sentiment analysis on 19th-century novels - Stylometric analysis (authorship attribution) - Diachronic change in literary language - Genre comparison (Gothic vs.Â Realist vs.Â Romantic) - Character network analysis\n\n\n\nOxford Text Archive (OTA)\nWebsite: ota.bodleian.ox.ac.uk\nWhat it is:\nOne of the oldest and most reputable digital humanities text repositories, maintained by Oxford University. Contains over 2,500 literary and linguistic resources, including corpora, editions, and linguistic datasets.\nStrengths: - âœ… High scholarly quality (peer-reviewed, curated) - âœ… Diverse text types (literature, drama, poetry, correspondence, historical documents) - âœ… Rich metadata and documentation - âœ… Multiple languages and time periods - âœ… Linguistically annotated corpora (POS-tagged, parsed) - âœ… Freely available for research (with registration)\nLimitations: - âŒ Requires free registration to download - âŒ Some texts have licensing restrictions - âŒ Variable formats (XML, plain text, TEI) - âŒ May require XML parsing skills - âŒ Smaller than Gutenberg (more selective)\nAccess process: 1. Register for free account at ota.bodleian.ox.ac.uk 2. Search catalog by author, period, genre, or language 3. Download texts (usually as ZIP files) 4. Check README for format details and licensing\nNotable collections: - Shakespeareâ€™s plays (TEI-encoded) - British National Corpus (100 million word sample of British English) - Corpus of Early English Correspondence (historical letters) - Lampeter Corpus (18th-19th century essays and treatises)\nProcessing OTA texts:\nMany OTA texts use TEI (Text Encoding Initiative) XML. Hereâ€™s a simple parser:\nimport requests\nfrom bs4 import BeautifulSoup\n\n# Example: parse TEI-encoded text\n# (You'd download this from OTA first)\n\nwith open('shakespeare_hamlet.xml', 'r', encoding='utf-8') as f:\n    xml_content = f.read()\n\n# Parse with BeautifulSoup\nsoup = BeautifulSoup(xml_content, 'xml')\n\n# Extract all speech text (for drama)\nspeeches = soup.find_all('sp')  # Speech elements\nfor speech in speeches[:5]:  # First 5 speeches\n    speaker = speech.find('speaker')\n    lines = speech.find_all('l')  # Line elements\n    if speaker and lines:\n        print(f\"{speaker.text}:\")\n        for line in lines:\n            print(f\"  {line.text}\")\nFor simpler workflows:\nMany OTA texts are also available in plain text. Check the download options or convert XML to text:\n# Extract just the text content from TEI\nsoup = BeautifulSoup(xml_content, 'xml')\ntext_body = soup.find('text')\nplain_text = text_body.get_text(separator=' ', strip=True)\nUse cases for this course: - Register variation (academic vs.Â literary vs.Â spoken) - Historical language change (Early Modern â†’ Contemporary) - Drama analysis (character speech patterns) - Corpus linguistics (using pre-annotated corpora)\n\n\n\nKaggle\nWebsite: kaggle.com/datasets\nWhat it is:\nA platform for data science competitions and dataset sharing. Contains thousands of user-contributed datasets on diverse topics, including text corpora.\nStrengths: - âœ… Huge variety (100,000+ datasets) - âœ… Modern, relevant corpora (social media, news, reviews) - âœ… Often pre-cleaned and formatted - âœ… Includes code notebooks showing example analyses - âœ… Easy download (API or direct download) - âœ… Community discussions and ratings\nLimitations & Pitfalls: - âš ï¸ Quality varies widely - User-contributed, not peer-reviewed - âš ï¸ Copyright unclear - Not all datasets have clear licensing - âš ï¸ Bias and representativeness - Often scraped from social media (skewed demographics) - âš ï¸ Sustainability - Datasets can be removed or become unavailable - âš ï¸ Documentation inconsistent - Some datasets poorly described - âš ï¸ Computational focus - Designed for ML, not humanities research\n\n\n\n\n\n\nUse Kaggle Datasets Critically\n\n\n\nBefore using a Kaggle dataset for academic work:\n\nCheck provenance: Where did the data come from? Is scraping/collection ethical?\nVerify licensing: Can you legally use it? Can you redistribute findings?\nAssess quality: Is it clean? Complete? Representative?\nConsider bias: Who is included/excluded? What are the demographic skews?\nDocument thoroughly: Kaggle datasets can disappearâ€”save metadata and URLs\nCross-reference: Can findings be validated with established corpora?\n\nRed flags: - No information about data collection methods - Scraped from platforms without API authorization - No demographic metadata (who is represented?) - Dataset owner has no credentials or history - Violates platform terms of service (e.g., Twitter scraping after 2023)\n\n\nAppropriate use cases:\nâœ… Good for exploratory work: - Testing methods on contemporary language - Comparing social media vs.Â formal writing - Analyzing product reviews for sentiment patterns - Exploring informal language and emoji use\nâŒ Problematic for: - Making generalizable claims about language use - Historical or canonical literary analysis - Academic writing analysis (use MICUSP instead) - Any work requiring demographic representativeness\nAccessing Kaggle datasets:\n# Option 1: Manual download from website\n# 1. Go to kaggle.com/datasets\n# 2. Search for your topic\n# 3. Download CSV/JSON\n# 4. Upload to Colab or load locally\n\n# Option 2: Kaggle API (requires setup)\n# Not recommended for novicesâ€”stick with manual download\n\nimport polars as pl\n\n# After downloading, load like any other CSV\ndf = pl.read_csv('path/to/kaggle_dataset.csv')\nExample datasets relevant to humanities: - Movie reviews (sentiment, rhetoric) - Reddit comments (informal language, communities) - News articles (genre, bias, framing) - Song lyrics (poetic language, cultural trends) - Wikipedia articles (encyclopedic register)\nBottom line:\nKaggle is useful for exploring modern, informal, or domain-specific language, but should be used cautiously and critically. For canonical humanities research, prefer established repositories like OTA, Gutenberg, or domain-specific corpora.\n\n\n\nOther Useful Repositories\nHathiTrust Digital Library\nWebsite: hathitrust.org\nContent: 17+ million digitized books, many post-1928\nAccess: Browse freely, bulk access requires research agreement\nUse case: Large-scale diachronic studies, 20th-century texts\nInternet Archive\nWebsite: archive.org\nContent: Books, periodicals, audio, video, web archives\nAccess: Free download (check copyright)\nUse case: Historical periodicals, early 20th-century materials\nCorpus of Contemporary American English (COCA)\nWebsite: english-corpora.org/coca\nContent: 1 billion words of American English (1990-2019)\nAccess: Free online interface, full download requires purchase\nUse case: Contemporary American English reference corpus\nBritish National Corpus (BNC)\nWebsite: english-corpora.org/bnc\nContent: 100 million words of British English\nAccess: Free online, downloadable via OTA\nUse case: British English reference, register variation",
    "crumbs": [
      "Course Resources",
      "Course Data Resources"
    ]
  },
  {
    "objectID": "resources/data.html#adding-your-own-data-adding-your-own-data",
    "href": "resources/data.html#adding-your-own-data-adding-your-own-data",
    "title": "Course Data Resources",
    "section": "Adding Your Own Data## Adding Your Own Data",
    "text": "Adding Your Own Data## Adding Your Own Data\nYou can adapt most mini labs to work with your own data:\n\nOption 1: Upload to Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Load from your Drive\ndf = pl.read_csv('/content/drive/MyDrive/my_data.csv')\n\n\nOption 2: Upload Directly to Colab Session\nfrom google.colab import files\nuploaded = files.upload()\n\nimport io\ndf = pl.read_csv(io.BytesIO(uploaded['filename.csv']))\n\n\nOption 3: Load from URL\n# If your data is hosted online\ndf = pl.read_csv('https://your-url.com/data.csv')",
    "crumbs": [
      "Course Resources",
      "Course Data Resources"
    ]
  },
  {
    "objectID": "resources/data.html#getting-help-with-data",
    "href": "resources/data.html#getting-help-with-data",
    "title": "Course Data Resources",
    "section": "Getting Help with Data",
    "text": "Getting Help with Data\nIf you encounter issues loading or processing data:\n\nCheck the file path - URLs must be exact\nVerify the format - Is it CSV, TSV, or Parquet?\nLook at the examples - Mini labs show working code\nCheck encoding - Some text files may need encoding='utf-8'\nAsk for help - Share error messages on the discussion board\n\n\n\n\n\n\n\nPro Tip: Exploring New Data\n\n\n\nWhen working with a new dataset:\n# Quick exploration\nprint(df.shape)       # How many rows/columns?\nprint(df.columns)     # What variables are there?\nprint(df.head())      # What do the first rows look like?\nprint(df.describe())  # What are the summary statistics?",
    "crumbs": [
      "Course Resources",
      "Course Data Resources"
    ]
  }
]