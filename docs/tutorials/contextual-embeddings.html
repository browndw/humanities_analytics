<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.557">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Methods in Humanities Analytics - 13&nbsp; Contextual Embeddings and Transformers</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../tutorials/classification-federalist-papers.html" rel="next">
<link href="../tutorials/multi-dimensional-analysis.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../images/text_analysis.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Methods in Humanities Analytics</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
    <a href="https://github.com/browndw/humanities_analytics" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
</div>
          <div id="quarto-search" class="" title="Search"></div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../tutorials/categorical-variables.html">Advanced Analysis</a></li><li class="breadcrumb-item"><a href="../tutorials/contextual-embeddings.html"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Contextual Embeddings and Transformers</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Foundations</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/sentiment-and-syuzhet.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Texts, Algorithms, and Black-Boxes</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/corpus-basics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Corpus Basics: Tokenization and Processing Pipelines</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/spacy-basics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">NLP Processing with spaCy</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/frequency-and-distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Distributions</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true">
 <span class="menu-text">Core Methods</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/keyness.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Keyness</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/collocations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Collocations and N-grams</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/time-series.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Time Series Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/topic-modeling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Topic Modeling</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true">
 <span class="menu-text">Advanced Analysis</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/categorical-variables.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">ANOVA and R²: Comparing Groups and Explaining Variance</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/cluster-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Clustering Methods</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/correlations.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Correlation and Multi-Collinearity</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/multi-dimensional-analysis.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Multi-Dimensional Analysis</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/contextual-embeddings.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Contextual Embeddings and Transformers</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Applications</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/classification-federalist-papers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Classification &amp; Authorship Attribution</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true">
 <span class="menu-text">Reports</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../reports/cbe_01.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Coffee Break Experiment #1: From Curiosity to Visualization</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../reports/cbe_02.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Coffee Break Experiment #2: From Results to Research</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../reports/cbe_03.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Coffee Break Experiment #3: Final Project Pilot Study</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../reports/final-project.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Final Project: Independent Corpus Analysis</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../reports/statistical-reporting-guide.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Statistical Reporting Guide</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true">
 <span class="menu-text">Course Resources</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../tutorials/using-colab.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Using Google Colab</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../resources/packages.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Python Packages for This Course</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../resources/data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Course Data Resources</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">13.1</span> Introduction</a>
  <ul class="collapse">
  <li><a href="#from-static-to-contextual-representations" id="toc-from-static-to-contextual-representations" class="nav-link" data-scroll-target="#from-static-to-contextual-representations"><span class="header-section-number">13.1.1</span> From Static to Contextual Representations</a></li>
  <li><a href="#how-this-powers-modern-ai" id="toc-how-this-powers-modern-ai" class="nav-link" data-scroll-target="#how-this-powers-modern-ai"><span class="header-section-number">13.1.2</span> How This Powers Modern AI</a></li>
  </ul></li>
  <li><a href="#understanding-contextual-embeddings" id="toc-understanding-contextual-embeddings" class="nav-link" data-scroll-target="#understanding-contextual-embeddings"><span class="header-section-number">13.2</span> Understanding Contextual Embeddings</a>
  <ul class="collapse">
  <li><a href="#the-attention-mechanism" id="toc-the-attention-mechanism" class="nav-link" data-scroll-target="#the-attention-mechanism"><span class="header-section-number">13.2.1</span> The Attention Mechanism</a></li>
  <li><a href="#from-words-to-documents" id="toc-from-words-to-documents" class="nav-link" data-scroll-target="#from-words-to-documents"><span class="header-section-number">13.2.2</span> From Words to Documents</a></li>
  </ul></li>
  <li><a href="#research-applications" id="toc-research-applications" class="nav-link" data-scroll-target="#research-applications"><span class="header-section-number">13.3</span> Research Applications</a>
  <ul class="collapse">
  <li><a href="#semantic-search" id="toc-semantic-search" class="nav-link" data-scroll-target="#semantic-search"><span class="header-section-number">13.3.1</span> 1. Semantic Search</a></li>
  <li><a href="#clustering-by-semantic-content" id="toc-clustering-by-semantic-content" class="nav-link" data-scroll-target="#clustering-by-semantic-content"><span class="header-section-number">13.3.2</span> 2. Clustering by Semantic Content</a></li>
  <li><a href="#document-similarity" id="toc-document-similarity" class="nav-link" data-scroll-target="#document-similarity"><span class="header-section-number">13.3.3</span> 3. Document Similarity</a></li>
  </ul></li>
  <li><a href="#methodological-considerations" id="toc-methodological-considerations" class="nav-link" data-scroll-target="#methodological-considerations"><span class="header-section-number">13.4</span> Methodological Considerations</a>
  <ul class="collapse">
  <li><a href="#when-to-use-contextual-vs.-static-embeddings" id="toc-when-to-use-contextual-vs.-static-embeddings" class="nav-link" data-scroll-target="#when-to-use-contextual-vs.-static-embeddings"><span class="header-section-number">13.4.1</span> When to Use Contextual vs.&nbsp;Static Embeddings</a></li>
  <li><a href="#limitations-and-biases" id="toc-limitations-and-biases" class="nav-link" data-scroll-target="#limitations-and-biases"><span class="header-section-number">13.4.2</span> Limitations and Biases</a></li>
  <li><a href="#interpreting-similarity-scores" id="toc-interpreting-similarity-scores" class="nav-link" data-scroll-target="#interpreting-similarity-scores"><span class="header-section-number">13.4.3</span> Interpreting Similarity Scores</a></li>
  <li><a href="#computational-costs-and-scalability" id="toc-computational-costs-and-scalability" class="nav-link" data-scroll-target="#computational-costs-and-scalability"><span class="header-section-number">13.4.4</span> Computational Costs and Scalability</a></li>
  </ul></li>
  <li><a href="#connecting-to-other-methods" id="toc-connecting-to-other-methods" class="nav-link" data-scroll-target="#connecting-to-other-methods"><span class="header-section-number">13.5</span> Connecting to Other Methods</a>
  <ul class="collapse">
  <li><a href="#embeddings-multi-dimensional-analysis-mda" id="toc-embeddings-multi-dimensional-analysis-mda" class="nav-link" data-scroll-target="#embeddings-multi-dimensional-analysis-mda"><span class="header-section-number">13.5.1</span> Embeddings + Multi-Dimensional Analysis (MDA)</a></li>
  <li><a href="#embeddings-topic-modeling" id="toc-embeddings-topic-modeling" class="nav-link" data-scroll-target="#embeddings-topic-modeling"><span class="header-section-number">13.5.2</span> Embeddings + Topic Modeling</a></li>
  <li><a href="#embeddings-keyness-analysis" id="toc-embeddings-keyness-analysis" class="nav-link" data-scroll-target="#embeddings-keyness-analysis"><span class="header-section-number">13.5.3</span> Embeddings + Keyness Analysis</a></li>
  </ul></li>
  <li><a href="#ethical-and-critical-considerations" id="toc-ethical-and-critical-considerations" class="nav-link" data-scroll-target="#ethical-and-critical-considerations"><span class="header-section-number">13.6</span> Ethical and Critical Considerations</a>
  <ul class="collapse">
  <li><a href="#understanding-model-biases" id="toc-understanding-model-biases" class="nav-link" data-scroll-target="#understanding-model-biases"><span class="header-section-number">13.6.1</span> Understanding Model Biases</a></li>
  <li><a href="#privacy-and-data-sovereignty" id="toc-privacy-and-data-sovereignty" class="nav-link" data-scroll-target="#privacy-and-data-sovereignty"><span class="header-section-number">13.6.2</span> Privacy and Data Sovereignty</a></li>
  <li><a href="#interpretive-authority" id="toc-interpretive-authority" class="nav-link" data-scroll-target="#interpretive-authority"><span class="header-section-number">13.6.3</span> Interpretive Authority</a></li>
  </ul></li>
  <li><a href="#practical-workflow" id="toc-practical-workflow" class="nav-link" data-scroll-target="#practical-workflow"><span class="header-section-number">13.7</span> Practical Workflow</a>
  <ul class="collapse">
  <li><a href="#step-1-choose-a-model" id="toc-step-1-choose-a-model" class="nav-link" data-scroll-target="#step-1-choose-a-model"><span class="header-section-number">13.7.1</span> Step 1: Choose a Model</a></li>
  <li><a href="#step-2-encode-your-corpus" id="toc-step-2-encode-your-corpus" class="nav-link" data-scroll-target="#step-2-encode-your-corpus"><span class="header-section-number">13.7.2</span> Step 2: Encode Your Corpus</a></li>
  <li><a href="#step-3-analyze" id="toc-step-3-analyze" class="nav-link" data-scroll-target="#step-3-analyze"><span class="header-section-number">13.7.3</span> Step 3: Analyze</a></li>
  <li><a href="#step-4-interpret-and-validate" id="toc-step-4-interpret-and-validate" class="nav-link" data-scroll-target="#step-4-interpret-and-validate"><span class="header-section-number">13.7.4</span> Step 4: Interpret and Validate</a></li>
  </ul></li>
  <li><a href="#see-also" id="toc-see-also" class="nav-link" data-scroll-target="#see-also"><span class="header-section-number">13.8</span> See Also</a>
  <ul class="collapse">
  <li><a href="#foundational-papers" id="toc-foundational-papers" class="nav-link" data-scroll-target="#foundational-papers"><span class="header-section-number">13.8.1</span> Foundational Papers</a></li>
  <li><a href="#online-resources" id="toc-online-resources" class="nav-link" data-scroll-target="#online-resources"><span class="header-section-number">13.8.2</span> Online Resources</a></li>
  <li><a href="#related-tutorials" id="toc-related-tutorials" class="nav-link" data-scroll-target="#related-tutorials"><span class="header-section-number">13.8.3</span> Related Tutorials</a></li>
  <li><a href="#methodological-readings" id="toc-methodological-readings" class="nav-link" data-scroll-target="#methodological-readings"><span class="header-section-number">13.8.4</span> Methodological Readings</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">13.9</span> Summary</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/browndw/humanities_analytics/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../tutorials/categorical-variables.html">Advanced Analysis</a></li><li class="breadcrumb-item"><a href="../tutorials/contextual-embeddings.html"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Contextual Embeddings and Transformers</span></a></li></ol></nav>
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Contextual Embeddings and Transformers</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p><a target="_blank" href="https://colab.research.google.com/github/browndw/humanities_analytics/blob/main/mini_labs/Mini_Lab_11_Contextual_Embeddings.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"> </a></p>
<section id="introduction" class="level2" data-number="13.1">
<h2 data-number="13.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">13.1</span> Introduction</h2>
<p>When you use ChatGPT, Claude, or other AI language models, you might wonder: <em>How does the AI “understand” what I’m asking?</em> The answer lies in <strong>contextual embeddings</strong>—the technology that allows modern language models to represent meaning as geometric relationships in high-dimensional space.</p>
<p>This tutorial introduces contextual embeddings, explains how they differ from earlier methods (like word2vec from Mini Lab 9), and demonstrates practical applications for humanities research: semantic search, clustering by meaning, and tracking how word meanings shift across contexts.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Computational Requirements
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Running locally</strong>: This tutorial uses sentence-transformers, which downloads pre-trained models (~100-400 MB). First-time model download requires internet; subsequent runs are faster. The inaugural corpus examples run well on a standard laptop (8GB RAM).</p>
<p><strong>Running on Colab</strong>: Click the “Open in Colab” badge above to run this tutorial in Google Colab with free GPU access—faster encoding and no local installation needed.</p>
<p><strong>Companion lab</strong>: <a href="../mini_labs/Mini_Lab_11_Contextual_Embeddings.ipynb">Mini Lab 11: Contextual Embeddings</a> provides hands-on exercises with the same concepts.</p>
</div>
</div>
<section id="from-static-to-contextual-representations" class="level3" data-number="13.1.1">
<h3 data-number="13.1.1" class="anchored" data-anchor-id="from-static-to-contextual-representations"><span class="header-section-number">13.1.1</span> From Static to Contextual Representations</h3>
<p><strong>Static embeddings</strong> (word2vec, GloVe) assign each word a single vector representation:</p>
<ul>
<li>“bank” → <code>[0.45, -0.23, 0.89, ...]</code> (always the same 300 numbers)</li>
<li>Works well for <strong>vocabulary analysis</strong>: finding synonyms, semantic fields, analogies</li>
<li>Limitation: The word “bank” gets the <strong>same vector</strong> whether it appears in “river bank” or “bank account”</li>
</ul>
<p><strong>Contextual embeddings</strong> (BERT, RoBERTa, sentence-transformers) compute <strong>different vectors</strong> for the same word depending on its surrounding context:</p>
<ul>
<li>“I deposited money at the <strong>bank</strong>” → <code>[0.52, 0.18, -0.34, ...]</code> (finance-related)</li>
<li>“We sat by the river <strong>bank</strong>” → <code>[-0.12, 0.67, 0.41, ...]</code> (geography-related)</li>
</ul>
<p>This is achieved through the <strong>attention mechanism</strong>, which allows the model to examine all words in a sentence simultaneously and adjust each word’s representation based on what’s around it.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why “Transformers”?
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Transformer</strong> is the neural architecture that enables contextual embeddings. Introduced by Vaswani et al.&nbsp;(2017) in “Attention Is All You Need,” transformers process sequences in parallel rather than one word at a time.</p>
<p><strong>Key innovation</strong>: The <strong>self-attention mechanism</strong> lets each word “attend to” every other word in the input, learning which words are most relevant for understanding it.</p>
<p>Example: In “The bank by the river was steep,” the model learns that “bank” should pay high attention to “river” and “steep” (not “money” or “account”), producing a geography-specific representation.</p>
</div>
</div>
</section>
<section id="how-this-powers-modern-ai" class="level3" data-number="13.1.2">
<h3 data-number="13.1.2" class="anchored" data-anchor-id="how-this-powers-modern-ai"><span class="header-section-number">13.1.2</span> How This Powers Modern AI</h3>
<p>Large Language Models (LLMs) like GPT-4, Claude, and Gemini are all transformer-based systems trained on massive text corpora:</p>
<p><strong>The process</strong>:</p>
<ol type="1">
<li><strong>Tokenization</strong>: Split input into tokens (roughly words or word pieces)</li>
<li><strong>Embedding</strong>: Convert each token to an initial vector</li>
<li><strong>Attention layers</strong>: Multiple transformer layers refine representations by attending to context</li>
<li><strong>Output</strong>: Contextual embeddings that capture meaning in this specific sentence</li>
</ol>
<p><strong>What we’re doing in this tutorial</strong>:</p>
<ul>
<li>Using <strong>sentence-transformers</strong>, which are BERT models fine-tuned for creating high-quality sentence/document embeddings</li>
<li>Applying these embeddings to research tasks: semantic similarity, search, clustering, word-sense analysis</li>
<li>Building intuition about how AI represents language geometrically</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Hugging Face: The AI Model Repository
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Hugging Face</strong> (<a href="https://huggingface.co/" class="uri">https://huggingface.co/</a>) is the primary platform for sharing, discovering, and using AI models. Think of it as “GitHub for machine learning models.”</p>
<p><strong>What it offers</strong>:</p>
<ul>
<li><strong>Model Hub</strong>: 500,000+ pre-trained models (BERT, GPT, LLaMA, specialized domain models)</li>
<li><strong>Datasets</strong>: Thousands of text corpora for training/testing</li>
<li><strong>Spaces</strong>: Interactive demos to try models in your browser</li>
<li><strong>Transformers library</strong>: Python tools to download and use any model with a few lines of code</li>
</ul>
<p><strong>For humanities researchers</strong>: You can find models fine-tuned for specific tasks (sentiment analysis, named entity recognition, translation) or domains (historical texts, literary analysis, legal documents). All freely accessible and ready to use in Python.</p>
<p><strong>Beyond embeddings</strong>: While this lab focuses on document embeddings, Python provides powerful tools to interact with full LLMs—even running smaller models (Llama 3, Mistral, Phi) <strong>locally on your computer</strong> using libraries like <code>transformers</code>, <code>llama-cpp-python</code>, or <code>ollama</code>. This means you can analyze sensitive texts privately, experiment without API costs, or fine-tune models on your specific corpus. We don’t cover this here, but understanding embeddings is the foundation for working with any transformer-based model.</p>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Why This Matters for Humanities Research
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Beyond keyword search</strong>: Traditional text analysis relies on exact word matches. If you search for “economic crisis,” you only find texts containing those words—missing passages about “financial collapse,” “market downturn,” or “commercial distress.”</p>
<p><strong>Semantic understanding</strong>: Contextual embeddings represent <strong>meaning</strong> not vocabulary. Documents about economic crises cluster together in high-dimensional space even if they use completely different words.</p>
<p><strong>Applications</strong>:</p>
<ul>
<li><strong>Semantic search</strong>: Find relevant passages across large archives without knowing exact terminology
<ul>
<li><em>Example</em>: Search thousands of 19th-century novels for “urban alienation” even when authors write about “metropolitan loneliness” or “city estrangement”</li>
</ul></li>
<li><strong>Clustering by theme</strong>: Group texts by what they’re <em>about</em>, not which words they share
<ul>
<li><em>Example</em>: Discover that Civil War letters and Vietnam War letters cluster together despite different vocabulary, revealing transhistorical patterns in wartime communication</li>
</ul></li>
<li><strong>Historical semantics</strong>: Track how concepts like “freedom” or “democracy” shift meaning across time
<ul>
<li><em>Example</em>: Visualize how “democracy” meant different things to Founding Fathers (property-holding citizens) vs.&nbsp;Civil Rights movement (universal suffrage)</li>
</ul></li>
<li><strong>Cross-corpus comparison</strong>: Identify what different communities mean by the same term
<ul>
<li><em>Example</em>: Compare how “justice” is discussed in legal documents, activist manifestos, and philosophical texts—same word, different semantic neighborhoods</li>
</ul></li>
<li><strong>Authorship and influence</strong>: Detect which authors are semantically similar even when using different styles
<ul>
<li><em>Example</em>: Find that George Eliot and Henry James cluster together thematically (psychological realism) despite different sentence structures</li>
</ul></li>
<li><strong>LLM literacy</strong>: Understanding embeddings helps you use AI tools critically and effectively
<ul>
<li><em>Example</em>: Recognize that ChatGPT’s “knowledge” is really proximity in embedding space—why it can answer some questions confidently and others with plausible-sounding nonsense</li>
</ul></li>
</ul>
<p>This is computational reasoning for the humanities: using geometry (distances in vector space) to discover semantic patterns that close reading might miss in large corpora. <strong>The goal isn’t automation—it’s augmentation</strong>: finding patterns that generate new interpretive questions.</p>
</div>
</div>
</section>
</section>
<section id="understanding-contextual-embeddings" class="level2" data-number="13.2">
<h2 data-number="13.2" class="anchored" data-anchor-id="understanding-contextual-embeddings"><span class="header-section-number">13.2</span> Understanding Contextual Embeddings</h2>
<section id="the-attention-mechanism" class="level3" data-number="13.2.1">
<h3 data-number="13.2.1" class="anchored" data-anchor-id="the-attention-mechanism"><span class="header-section-number">13.2.1</span> The Attention Mechanism</h3>
<p>The key to contextual embeddings is <strong>self-attention</strong>—each word’s representation is influenced by every other word in the sentence.</p>
<p>Let’s demonstrate this with the word “power” in different contexts:</p>
<div id="071b36de" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load model</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SentenceTransformer(<span class="st">'all-MiniLM-L6-v2'</span>)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create sentences where "power" has different meanings</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>sentences <span class="op">=</span> [</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The government has the power to pass new laws."</span>,  <span class="co"># political authority</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The power plant generates electricity for the city."</span>,  <span class="co"># energy/electricity</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    <span class="st">"Her speech had the power to move the audience."</span>,  <span class="co"># influence/effect</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    <span class="st">"The king wielded absolute power over his subjects."</span>,  <span class="co"># political authority</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Get contextual embeddings</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>embeddings <span class="op">=</span> model.encode(sentences)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute similarities</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>similarities <span class="op">=</span> cosine_similarity(embeddings)</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Display as DataFrame</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> polars <span class="im">as</span> pl</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>similarity_df <span class="op">=</span> pl.DataFrame(similarities, schema<span class="op">=</span>[<span class="ss">f"S</span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">"</span> <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(sentences))])</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Cosine similarities between sentences:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(similarity_df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Cosine similarities between sentences:

shape: (4, 4)
┌──────────┬──────────┬──────────┬──────────┐
│ S1       ┆ S2       ┆ S3       ┆ S4       │
│ ---      ┆ ---      ┆ ---      ┆ ---      │
│ f32      ┆ f32      ┆ f32      ┆ f32      │
╞══════════╪══════════╪══════════╪══════════╡
│ 1.0      ┆ 0.224048 ┆ 0.161138 ┆ 0.292017 │
│ 0.224048 ┆ 1.0      ┆ 0.092147 ┆ 0.066269 │
│ 0.161138 ┆ 0.092147 ┆ 1.0      ┆ 0.336578 │
│ 0.292017 ┆ 0.066269 ┆ 0.336578 ┆ 1.0      │
└──────────┴──────────┴──────────┴──────────┘</code></pre>
</div>
</div>
<p><strong>What this shows</strong>:</p>
<ul>
<li>S1-S4 (both political authority) have moderate similarity (<strong>0.29</strong>)—same sense but different contexts</li>
<li>S3-S4 (rhetorical power ↔︎ political power) actually highest (<strong>0.34</strong>)—abstract power concepts overlap</li>
<li>S2 (electricity) is quite distinct from political uses: S1-S2 = <strong>0.22</strong>, S2-S4 = <strong>0.07</strong></li>
<li>The model distinguishes “power plant” from “government power” based purely on surrounding words</li>
</ul>
<p><strong>Why this works</strong>: Human language interpretation works the same way—we understand “power” by looking at surrounding words. Transformers formalize this intuition mathematically.</p>
</section>
<section id="from-words-to-documents" class="level3" data-number="13.2.2">
<h3 data-number="13.2.2" class="anchored" data-anchor-id="from-words-to-documents"><span class="header-section-number">13.2.2</span> From Words to Documents</h3>
<p>Sentence-transformers extend this to <strong>document-level embeddings</strong>. Let’s load the inaugural corpus and see this in action:</p>
<div id="d058cb9f" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load inaugural addresses</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>url <span class="op">=</span> <span class="st">"https://raw.githubusercontent.com/browndw/humanities_analytics/main/data/data_tables/inaugural.tsv"</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>inaugural <span class="op">=</span> pl.read_csv(url, separator<span class="op">=</span><span class="st">"</span><span class="ch">\t</span><span class="st">"</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Loaded </span><span class="sc">{</span><span class="bu">len</span>(inaugural)<span class="sc">}</span><span class="ss"> presidential speeches"</span>)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Columns: </span><span class="sc">{</span>inaugural<span class="sc">.</span>columns<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Show a few examples</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>inaugural.select([<span class="st">'year'</span>, <span class="st">'president'</span>]).head(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Loaded 60 presidential speeches
Columns: ['year', 'president', 'doc_id', 'text']
</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="3">
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (5, 2)</small>
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">year</th>
<th data-quarto-table-cell-role="th">president</th>
</tr>
<tr class="odd">
<th>i64</th>
<th>str</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1789</td>
<td>"Washington"</td>
</tr>
<tr class="even">
<td>1793</td>
<td>"Washington"</td>
</tr>
<tr class="odd">
<td>1797</td>
<td>"Adams"</td>
</tr>
<tr class="even">
<td>1801</td>
<td>"Jefferson"</td>
</tr>
<tr class="odd">
<td>1805</td>
<td>"Jefferson"</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="d92859d8" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Encode all speeches (this takes ~30-60 seconds)</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Suppressing output during rendering to save memory</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>speech_embeddings <span class="op">=</span> model.encode(inaugural[<span class="st">'text'</span>].to_list(), show_progress_bar<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="5539f712" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Encoded </span><span class="sc">{</span><span class="bu">len</span>(speech_embeddings)<span class="sc">}</span><span class="ss"> speeches"</span>)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Each speech → </span><span class="sc">{</span>speech_embeddings<span class="sc">.</span>shape[<span class="dv">1</span>]<span class="sc">}</span><span class="ss">-dimensional vector"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Encoded 60 speeches
Each speech → 384-dimensional vector</code></pre>
</div>
</div>
<p>Now we can:</p>
<ul>
<li><strong>Compare documents</strong>: Cosine similarity between speech vectors</li>
<li><strong>Search semantically</strong>: Find speeches most similar to a query concept</li>
<li><strong>Cluster by meaning</strong>: Group speeches by thematic content</li>
<li><strong>Track evolution</strong>: See how topics shift across time or communities</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Dimensions and Model Size
</div>
</div>
<div class="callout-body-container callout-body">
<p>Different models produce different embedding dimensions:</p>
<ul>
<li><strong>all-MiniLM-L6-v2</strong>: 384 dimensions (fast, lightweight, good quality)</li>
<li><strong>all-mpnet-base-v2</strong>: 768 dimensions (slower, higher quality)</li>
<li><strong>paraphrase-multilingual</strong>: 768 dimensions (works across languages)</li>
</ul>
<p><strong>Tradeoff</strong>: Higher dimensions capture more nuance but require more computation. For most humanities research, 384 dimensions provide excellent results.</p>
<p><strong>Computational cost</strong>: Encoding 1000 documents takes ~30-60 seconds on a laptop with all-MiniLM-L6-v2. This is dramatically faster than training your own word2vec model.</p>
</div>
</div>
</section>
</section>
<section id="research-applications" class="level2" data-number="13.3">
<h2 data-number="13.3" class="anchored" data-anchor-id="research-applications"><span class="header-section-number">13.3</span> Research Applications</h2>
<section id="semantic-search" class="level3" data-number="13.3.1">
<h3 data-number="13.3.1" class="anchored" data-anchor-id="semantic-search"><span class="header-section-number">13.3.1</span> 1. Semantic Search</h3>
<p><strong>Problem</strong>: You want to find texts about “economic recovery after crisis” in a historical corpus. Keyword search for “economic recovery” misses relevant passages using different vocabulary: “commercial resurgence,” “financial rebound,” “restoration of prosperity.”</p>
<p><strong>Solution</strong>: Encode your query and all documents, then find highest cosine similarities.</p>
<p>Let’s search for speeches about economic crisis:</p>
<div id="0388e60b" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> semantic_search(query, top_k<span class="op">=</span><span class="dv">5</span>):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Find speeches most semantically similar to a query."""</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Encode the query</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    query_embedding <span class="op">=</span> model.encode([query])[<span class="dv">0</span>].reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Compute similarities</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    similarities <span class="op">=</span> cosine_similarity(query_embedding, speech_embeddings)[<span class="dv">0</span>]</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get top results</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> inaugural.with_columns([</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        pl.Series(<span class="st">"similarity"</span>, similarities)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    ]).sort(<span class="st">"similarity"</span>, descending<span class="op">=</span><span class="va">True</span>).head(top_k)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> results.select([<span class="st">'year'</span>, <span class="st">'president'</span>, <span class="st">'similarity'</span>])</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Try a search</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Query: 'economic crisis and national recovery'</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>semantic_search(<span class="st">"economic crisis and national recovery"</span>, top_k<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Query: 'economic crisis and national recovery'
</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="6">
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (5, 3)</small>
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">year</th>
<th data-quarto-table-cell-role="th">president</th>
<th data-quarto-table-cell-role="th">similarity</th>
</tr>
<tr class="odd">
<th>i64</th>
<th>str</th>
<th>f32</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1901</td>
<td>"McKinley"</td>
<td>0.440758</td>
</tr>
<tr class="even">
<td>1933</td>
<td>"Roosevelt"</td>
<td>0.381424</td>
</tr>
<tr class="odd">
<td>1925</td>
<td>"Coolidge"</td>
<td>0.351388</td>
</tr>
<tr class="even">
<td>1937</td>
<td>"Roosevelt"</td>
<td>0.317839</td>
</tr>
<tr class="odd">
<td>2017</td>
<td>"Trump"</td>
<td>0.31327</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<div id="83b912c1" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Try another query</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Query: 'war and national defense'</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>semantic_search(<span class="st">"war and national defense"</span>, top_k<span class="op">=</span><span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Query: 'war and national defense'
</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="7">
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (5, 3)</small>
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">year</th>
<th data-quarto-table-cell-role="th">president</th>
<th data-quarto-table-cell-role="th">similarity</th>
</tr>
<tr class="odd">
<th>i64</th>
<th>str</th>
<th>f32</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1973</td>
<td>"Nixon"</td>
<td>0.384878</td>
</tr>
<tr class="even">
<td>1925</td>
<td>"Coolidge"</td>
<td>0.383298</td>
</tr>
<tr class="odd">
<td>1865</td>
<td>"Lincoln"</td>
<td>0.382346</td>
</tr>
<tr class="even">
<td>2005</td>
<td>"Bush"</td>
<td>0.374248</td>
</tr>
<tr class="odd">
<td>1993</td>
<td>"Clinton"</td>
<td>0.320786</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p><strong>Why it works</strong>: The model learns that “recovery,” “rebound,” and “resurgence” occupy nearby regions in vector space because they appear in similar contexts. Your query vector sits in this semantic neighborhood, capturing conceptually related texts regardless of vocabulary.</p>
<p><strong>Research use cases</strong>:</p>
<ul>
<li>Finding relevant passages in massive archives (millions of documents)</li>
<li>Cross-linguistic search (with multilingual models)</li>
<li>Discovering unexpected connections across discourses</li>
<li>Building custom search engines for specialized corpora</li>
</ul>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Semantic Search vs.&nbsp;Topic Modeling
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Topic modeling</strong> (LDA, NMF) finds <strong>word co-occurrence patterns</strong>: documents sharing words like {economy, growth, market, trade} form a topic.</p>
<p><strong>Semantic search</strong> finds <strong>meaning relationships</strong>: documents about economic themes using entirely different vocabularies (19th-century “commercial prosperity” vs.&nbsp;modern “GDP growth”).</p>
<p><strong>When to use which</strong>:</p>
<ul>
<li>Topic modeling: Discover latent themes, understand vocabulary structure</li>
<li>Semantic search: Find specific concepts, cross-vocabulary retrieval</li>
<li>Both together: Topic modeling reveals corpus structure, semantic search finds examples</li>
</ul>
</div>
</div>
</section>
<section id="clustering-by-semantic-content" class="level3" data-number="13.3.2">
<h3 data-number="13.3.2" class="anchored" data-anchor-id="clustering-by-semantic-content"><span class="header-section-number">13.3.2</span> 2. Clustering by Semantic Content</h3>
<p><strong>Problem</strong>: You have 60 presidential speeches spanning 236 years. Do they cluster by <strong>historical era</strong> (19th vs.&nbsp;20th century) or by <strong>theme</strong> (wartime, economic crisis, unity)?</p>
<p><strong>Solution</strong>: Use embeddings as features for clustering algorithms.</p>
<div id="d9b792f9" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Cluster speeches into 4 groups</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>n_clusters <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>n_clusters, random_state<span class="op">=</span><span class="dv">42</span>, n_init<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>clusters <span class="op">=</span> kmeans.fit_predict(speech_embeddings)</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Add cluster labels to dataframe</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>inaugural_clustered <span class="op">=</span> inaugural.with_columns([</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    pl.Series(<span class="st">"cluster"</span>, clusters)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="co"># Show distribution</span></span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Speeches per cluster:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(inaugural_clustered.group_by(<span class="st">'cluster'</span>).agg(pl.count()).sort(<span class="st">'cluster'</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Speeches per cluster:

shape: (4, 2)
┌─────────┬───────┐
│ cluster ┆ count │
│ ---     ┆ ---   │
│ i32     ┆ u32   │
╞═════════╪═══════╡
│ 0       ┆ 9     │
│ 1       ┆ 24    │
│ 2       ┆ 10    │
│ 3       ┆ 17    │
└─────────┴───────┘</code></pre>
</div>
</div>
<div id="65523f52" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Show samples from each cluster</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">Sample speeches from each cluster:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"="</span><span class="op">*</span><span class="dv">60</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cluster_id <span class="kw">in</span> <span class="bu">range</span>(n_clusters):</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="ch">\n</span><span class="ss">Cluster </span><span class="sc">{</span>cluster_id<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    sample <span class="op">=</span> inaugural_clustered.<span class="bu">filter</span>(pl.col(<span class="st">'cluster'</span>) <span class="op">==</span> cluster_id).sample(n<span class="op">=</span><span class="bu">min</span>(<span class="dv">3</span>, <span class="bu">len</span>(inaugural_clustered.<span class="bu">filter</span>(pl.col(<span class="st">'cluster'</span>) <span class="op">==</span> cluster_id))))</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> row <span class="kw">in</span> sample.to_dicts():</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"  </span><span class="sc">{</span>row[<span class="st">'year'</span>]<span class="sc">:4d}</span><span class="ss"> </span><span class="sc">{</span>row[<span class="st">'president'</span>]<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Sample speeches from each cluster:

============================================================

Cluster 0:
  1977 Carter
  1997 Clinton
  1921 Harding

Cluster 1:
  1953 Eisenhower
  1813 Madison
  1805 Jefferson

Cluster 2:
  1917 Wilson
  1821 Monroe
  1933 Roosevelt

Cluster 3:
  1989 Bush
  2005 Bush
  1857 Buchanan</code></pre>
</div>
</div>
<p><strong>What you find</strong>: Examine the clusters. Do they correspond to historical periods, or do they cut across time based on themes? This reveals how rhetorical patterns persist or change.</p>
<p><strong>Insight</strong>: Speeches often cluster by <strong>communicative function</strong> not chronology. A 1941 wartime speech may resemble a 2001 wartime speech more than a 1945 peace speech.</p>
<p><strong>Research implications</strong>:</p>
<ul>
<li>Challenges periodization assumptions (historical era ≠ rhetorical mode)</li>
<li>Reveals transhistorical patterns (certain situations evoke similar language)</li>
<li>Complements stylistic analysis (MDA finds <em>how</em> texts differ, embeddings find <em>what</em> they’re about)</li>
</ul>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Interpreting Clusters
</div>
</div>
<div class="callout-body-container callout-body">
<p>Clustering results depend on:</p>
<ul>
<li>Number of clusters (try 3-7 for interpretability)</li>
<li>Clustering algorithm (K-means, hierarchical, DBSCAN)</li>
<li>Preprocessing choices (which texts to include)</li>
</ul>
<p><strong>Best practice</strong>: Treat clusters as <strong>exploratory hypotheses</strong>, not objective categories. Examine sample texts, validate with close reading, test alternative cluster numbers.</p>
</div>
</div>
</section>
<section id="document-similarity" class="level3" data-number="13.3.3">
<h3 data-number="13.3.3" class="anchored" data-anchor-id="document-similarity"><span class="header-section-number">13.3.3</span> 3. Document Similarity</h3>
<p>Let’s find which speeches are most similar to a specific speech:</p>
<div id="5efffd4a" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Find speeches similar to most recent</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>target_idx <span class="op">=</span> <span class="bu">len</span>(inaugural) <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>target_speech <span class="op">=</span> inaugural[target_idx]</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Finding speeches similar to: </span><span class="sc">{</span>target_speech[<span class="st">'year'</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss"> - </span><span class="sc">{</span>target_speech[<span class="st">'president'</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ch">\n</span><span class="ss">"</span>)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute similarities</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>target_embedding <span class="op">=</span> speech_embeddings[target_idx].reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>similarities_to_target <span class="op">=</span> cosine_similarity(target_embedding, speech_embeddings)[<span class="dv">0</span>]</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Create results dataframe</span></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> inaugural.with_columns([</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    pl.Series(<span class="st">"similarity"</span>, similarities_to_target)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>]).sort(<span class="st">"similarity"</span>, descending<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Most similar speeches:</span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>results.select([<span class="st">'year'</span>, <span class="st">'president'</span>, <span class="st">'similarity'</span>]).head(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Finding speeches similar to: 2025 - Trump

Most similar speeches:
</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="10">
<div><style>
.dataframe > thead > tr,
.dataframe > tbody > tr {
  text-align: right;
  white-space: pre-wrap;
}
</style>
<small>shape: (10, 3)</small>
<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">year</th>
<th data-quarto-table-cell-role="th">president</th>
<th data-quarto-table-cell-role="th">similarity</th>
</tr>
<tr class="odd">
<th>i64</th>
<th>str</th>
<th>f32</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2025</td>
<td>"Trump"</td>
<td>1.0</td>
</tr>
<tr class="even">
<td>2021</td>
<td>"Biden"</td>
<td>0.740231</td>
</tr>
<tr class="odd">
<td>1949</td>
<td>"Truman"</td>
<td>0.724837</td>
</tr>
<tr class="even">
<td>2009</td>
<td>"Obama"</td>
<td>0.66023</td>
</tr>
<tr class="odd">
<td>2017</td>
<td>"Trump"</td>
<td>0.65667</td>
</tr>
<tr class="even">
<td>1933</td>
<td>"Roosevelt"</td>
<td>0.650834</td>
</tr>
<tr class="odd">
<td>1981</td>
<td>"Reagan"</td>
<td>0.649962</td>
</tr>
<tr class="even">
<td>1857</td>
<td>"Buchanan"</td>
<td>0.64124</td>
</tr>
<tr class="odd">
<td>1957</td>
<td>"Eisenhower"</td>
<td>0.628518</td>
</tr>
<tr class="even">
<td>2013</td>
<td>"Obama"</td>
<td>0.617471</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
<p><strong>What this reveals</strong>: The most similar speeches may come from completely different eras, showing that thematic content transcends historical period.</p>
<p><strong>Historical semantics applications</strong>:</p>
<ul>
<li>Track how “democracy,” “liberty,” “rights” shift across constitutional debates</li>
<li>Compare how different social movements invoke the same concepts<br>
</li>
<li>Identify when semantic change occurs (gradual drift vs.&nbsp;sudden shift)</li>
<li>Validate conceptual history arguments computationally</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Computational Historical Semantics
</div>
</div>
<div class="callout-body-container callout-body">
<p>Traditional historical semantics relies on:</p>
<ul>
<li>Close reading of key texts</li>
<li>Etymological dictionaries</li>
<li>Expert knowledge of historical context</li>
</ul>
<p><strong>Computational approach</strong> (word-in-context embeddings) adds:</p>
<ul>
<li><strong>Scale</strong>: Analyze thousands of uses automatically</li>
<li><strong>Objectivity</strong>: Find patterns without pre-existing hypotheses</li>
<li><strong>Visualization</strong>: See semantic space geometrically</li>
<li><strong>Comparison</strong>: Measure similarity quantitatively</li>
</ul>
<p><strong>Best results</strong>: Combine both. Use embeddings to find patterns at scale, then validate with close reading and historical expertise.</p>
</div>
</div>
</section>
</section>
<section id="methodological-considerations" class="level2" data-number="13.4">
<h2 data-number="13.4" class="anchored" data-anchor-id="methodological-considerations"><span class="header-section-number">13.4</span> Methodological Considerations</h2>
<section id="when-to-use-contextual-vs.-static-embeddings" class="level3" data-number="13.4.1">
<h3 data-number="13.4.1" class="anchored" data-anchor-id="when-to-use-contextual-vs.-static-embeddings"><span class="header-section-number">13.4.1</span> When to Use Contextual vs.&nbsp;Static Embeddings</h3>
<p><strong>Contextual embeddings (BERT, sentence-transformers)</strong> excel at:</p>
<ul>
<li>Document similarity and semantic search</li>
<li>Tasks requiring context-dependent meaning (word sense disambiguation)</li>
<li>Comparing texts across vocabularies (different time periods, discourses)</li>
<li>Building on pre-trained models (no need to train on your corpus)</li>
</ul>
<p><strong>Static embeddings (word2vec, GloVe)</strong> are preferable for:</p>
<ul>
<li>Vocabulary analysis (semantic fields, synonym detection)</li>
<li>Historical word embedding models (training on period-specific corpora)</li>
<li>Computational efficiency (smaller models, faster inference)</li>
<li>Interpretability (single vector per word is easier to inspect)</li>
</ul>
<p><strong>When to use both</strong>:</p>
<ul>
<li>Word2vec to map semantic fields → sentence-transformers to find documents expressing those concepts</li>
<li>Static embeddings for diachronic analysis (train on decade-specific corpora) → contextual for synchronic variation (how is “freedom” used differently in the same period)</li>
</ul>
</section>
<section id="limitations-and-biases" class="level3" data-number="13.4.2">
<h3 data-number="13.4.2" class="anchored" data-anchor-id="limitations-and-biases"><span class="header-section-number">13.4.2</span> Limitations and Biases</h3>
<p><strong>Pre-training data matters</strong>: Sentence-transformers are trained on massive web corpora. This means:</p>
<ul>
<li><strong>Bias</strong>: Models inherit biases from training data (gender, race, political assumptions)</li>
<li><strong>Domain mismatch</strong>: Web text ≠ historical documents. Models may not understand archaic vocabulary or specialized discourse</li>
<li><strong>Anglophone focus</strong>: Most models are English-centric. Multilingual models exist but have lower quality</li>
</ul>
<p><strong>Example</strong>: A model trained on 21st-century web text might misunderstand 18th-century uses of “virtue” (which had specific political connotations in republican discourse).</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Critical Use of Pre-trained Models
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Question to always ask</strong>: Is the model’s training data appropriate for my research questions?</p>
<p><strong>Strategies</strong>:</p>
<ul>
<li>Test model performance on sample texts from your corpus</li>
<li>Compare results to expert knowledge or close reading</li>
<li>Consider domain-specific models (e.g., BiomedBERT for medical texts)</li>
<li>Fine-tune on your corpus if you have labeled data</li>
<li>Acknowledge limitations in research write-ups</li>
</ul>
<p><strong>Remember</strong>: Embeddings find patterns in <em>how the model was trained to represent language</em>, not objective semantic truth. Treat them as powerful exploratory tools, not oracles.</p>
</div>
</div>
</section>
<section id="interpreting-similarity-scores" class="level3" data-number="13.4.3">
<h3 data-number="13.4.3" class="anchored" data-anchor-id="interpreting-similarity-scores"><span class="header-section-number">13.4.3</span> Interpreting Similarity Scores</h3>
<p><strong>Cosine similarity</strong> ranges from -1 to 1 (in practice, usually 0 to 1 for sentence embeddings):</p>
<ul>
<li><strong>0.9-1.0</strong>: Nearly identical content (duplicates, paraphrases)</li>
<li><strong>0.7-0.9</strong>: High semantic overlap (same topic, similar argument)</li>
<li><strong>0.5-0.7</strong>: Moderate thematic relationship (related but distinct)</li>
<li><strong>0.3-0.5</strong>: Weak connection (tangentially related)</li>
<li><strong>&lt;0.3</strong>: Unrelated or opposite</li>
</ul>
<p><strong>Context matters</strong>: What counts as “similar” depends on your research question:</p>
<ul>
<li>Comparing different editions of the same book: expect &gt;0.9</li>
<li>Finding thematically related articles: 0.6-0.8 is strong</li>
<li>Cross-genre comparison: 0.4-0.6 might indicate meaningful connection</li>
<li>Historical comparison: 0.5 between 18th and 21st-century texts on “liberty” might be surprisingly high (semantic stability) or surprisingly low (conceptual shift)</li>
</ul>
<p><strong>Calibration</strong>: Always examine sample pairs at different similarity levels to understand what scores mean in your corpus.</p>
<p><strong>Humanities perspective</strong>: Unlike quantitative sciences where statistical significance determines validity, humanities research values <strong>interpretive significance</strong>. A similarity score of 0.4 might reveal a fascinating connection worth exploring through close reading—the number is a prompt for interpretation, not a verdict.</p>
</section>
<section id="computational-costs-and-scalability" class="level3" data-number="13.4.4">
<h3 data-number="13.4.4" class="anchored" data-anchor-id="computational-costs-and-scalability"><span class="header-section-number">13.4.4</span> Computational Costs and Scalability</h3>
<p><strong>Encoding time</strong> (all-MiniLM-L6-v2 on a laptop):</p>
<ul>
<li>100 documents: ~5 seconds</li>
<li>1,000 documents: ~30 seconds</li>
<li>10,000 documents: ~5 minutes</li>
<li>100,000 documents: ~50 minutes</li>
</ul>
<p><strong>Memory requirements</strong>:</p>
<ul>
<li>10,000 documents × 384 dimensions × 4 bytes/float = ~15 MB</li>
<li>100,000 documents = ~150 MB</li>
<li>1 million documents = ~1.5 GB</li>
</ul>
<p><strong>Practical implications</strong>:</p>
<ul>
<li>Encode once, save embeddings, reuse for multiple analyses</li>
<li>Use batch encoding for efficiency</li>
<li>For truly massive corpora (millions of texts), consider approximate nearest neighbor libraries (FAISS, Annoy)</li>
</ul>
</section>
</section>
<section id="connecting-to-other-methods" class="level2" data-number="13.5">
<h2 data-number="13.5" class="anchored" data-anchor-id="connecting-to-other-methods"><span class="header-section-number">13.5</span> Connecting to Other Methods</h2>
<section id="embeddings-multi-dimensional-analysis-mda" class="level3" data-number="13.5.1">
<h3 data-number="13.5.1" class="anchored" data-anchor-id="embeddings-multi-dimensional-analysis-mda"><span class="header-section-number">13.5.1</span> Embeddings + Multi-Dimensional Analysis (MDA)</h3>
<p><strong>MDA</strong> (Mini Lab 10) clusters texts by <strong>stylistic features</strong>: pronouns, passives, nominalizations, sentence length. It answers: <em>How do texts differ in style?</em></p>
<p><strong>Contextual embeddings</strong> cluster by <strong>semantic content</strong>: themes, topics, arguments. They answer: <em>What are texts about?</em></p>
<p><strong>Combining both</strong>:</p>
<ol type="1">
<li>Use embeddings to identify thematically similar texts</li>
<li>Apply MDA to see if they use similar or different styles</li>
<li><strong>Example finding</strong>: 19th and 21st century inaugural speeches about unity might share themes (high embedding similarity) but differ stylistically (different MDA profiles)</li>
</ol>
<p><strong>Insight</strong>: Separates <strong>what</strong> is said from <strong>how</strong> it’s said—crucial for understanding rhetorical choices.</p>
</section>
<section id="embeddings-topic-modeling" class="level3" data-number="13.5.2">
<h3 data-number="13.5.2" class="anchored" data-anchor-id="embeddings-topic-modeling"><span class="header-section-number">13.5.2</span> Embeddings + Topic Modeling</h3>
<p><strong>Topic modeling</strong> (Mini Lab 9) discovers <strong>word co-occurrence patterns</strong> (latent topics).</p>
<p><strong>Contextual embeddings</strong> find <strong>semantic relationships</strong> regardless of shared vocabulary.</p>
<p><strong>Complementary uses</strong>:</p>
<ul>
<li>Topic modeling: “What topics structure this corpus?”</li>
<li>Embeddings: “For each topic, find examples using different vocabularies”</li>
<li><strong>Example</strong>: Topic modeling finds {economy, growth, market, trade} topic. Embeddings find 19th-century speeches expressing economic themes with entirely different words {commerce, prosperity, manufacture}.</li>
</ul>
</section>
<section id="embeddings-keyness-analysis" class="level3" data-number="13.5.3">
<h3 data-number="13.5.3" class="anchored" data-anchor-id="embeddings-keyness-analysis"><span class="header-section-number">13.5.3</span> Embeddings + Keyness Analysis</h3>
<p><strong>Keyness</strong> identifies words that distinguish one corpus from another (statistically overrepresented).</p>
<p><strong>Embeddings</strong> identify texts that are semantically similar despite different vocabulary.</p>
<p><strong>Workflow</strong>:</p>
<ol type="1">
<li>Use keyness to find distinctive vocabulary (what makes Group A different from Group B)</li>
<li>Use embeddings to find if Group A texts are semantically coherent (do they share themes?) or heterogeneous</li>
<li><strong>Example</strong>: Keyness shows female authors overuse “felt,” “seemed,” “thought.” Embeddings reveal these words appear in narratives focused on interiority (not just random distribution).</li>
</ol>
</section>
</section>
<section id="ethical-and-critical-considerations" class="level2" data-number="13.6">
<h2 data-number="13.6" class="anchored" data-anchor-id="ethical-and-critical-considerations"><span class="header-section-number">13.6</span> Ethical and Critical Considerations</h2>
<section id="understanding-model-biases" class="level3" data-number="13.6.1">
<h3 data-number="13.6.1" class="anchored" data-anchor-id="understanding-model-biases"><span class="header-section-number">13.6.1</span> Understanding Model Biases</h3>
<p>Transformer models are trained on massive web corpora (Reddit, Wikipedia, news sites, books). This means they <strong>encode the biases present in that data</strong>:</p>
<p><strong>Gender bias</strong>: A model might embed “doctor” closer to “he” than “she” because of training data patterns, not reality.</p>
<p><strong>Cultural bias</strong>: Concepts like “family,” “freedom,” or “justice” are embedded based on dominant cultural perspectives in the training data (predominantly English-language, Western sources).</p>
<p><strong>Historical anachronism</strong>: Models trained on 21st-century text may misrepresent 18th-century concepts (“virtue” meant something specific in republican discourse that modern embeddings might miss).</p>
<p><strong>For humanities researchers</strong>:</p>
<ul>
<li>Always ask: <strong>Whose language is this model encoding?</strong> Not universal truth, but patterns from specific corpora.</li>
<li>Validate computational findings with domain expertise and close reading</li>
<li>Consider whether your research questions require domain-specific models or careful interpretation of general-purpose models</li>
<li>Acknowledge limitations in your write-ups: “These embeddings capture how concepts relate <em>in contemporary web text</em>, not necessarily in historical discourse”</li>
</ul>
</section>
<section id="privacy-and-data-sovereignty" class="level3" data-number="13.6.2">
<h3 data-number="13.6.2" class="anchored" data-anchor-id="privacy-and-data-sovereignty"><span class="header-section-number">13.6.2</span> Privacy and Data Sovereignty</h3>
<p>When working with sensitive texts:</p>
<ul>
<li><strong>Cloud APIs</strong> (OpenAI, Anthropic) send your texts to their servers—inappropriate for unpublished manuscripts, confidential archives, or culturally sensitive materials</li>
<li><strong>Local models</strong> (via Hugging Face transformers or Ollama) process everything on your computer—maintains privacy and data sovereignty</li>
<li><strong>Considerations</strong>: Indigenous texts, unpublished correspondence, medical records, legal documents—all may require local processing</li>
</ul>
</section>
<section id="interpretive-authority" class="level3" data-number="13.6.3">
<h3 data-number="13.6.3" class="anchored" data-anchor-id="interpretive-authority"><span class="header-section-number">13.6.3</span> Interpretive Authority</h3>
<p>Embeddings are <strong>tools for discovery</strong>, not <strong>arguments in themselves</strong>:</p>
<ul>
<li>A clustering result is a pattern worth investigating, not proof of a categorical distinction</li>
<li>Similarity scores suggest connections to explore through close reading</li>
<li>Computational patterns complement humanistic interpretation—they don’t replace it</li>
</ul>
<p><strong>Best practice</strong>: Frame results as <strong>generative</strong> (“this pattern raises questions about…”) rather than <strong>conclusive</strong> (“this proves that…”).</p>
</section>
</section>
<section id="practical-workflow" class="level2" data-number="13.7">
<h2 data-number="13.7" class="anchored" data-anchor-id="practical-workflow"><span class="header-section-number">13.7</span> Practical Workflow</h2>
<section id="step-1-choose-a-model" class="level3" data-number="13.7.1">
<h3 data-number="13.7.1" class="anchored" data-anchor-id="step-1-choose-a-model"><span class="header-section-number">13.7.1</span> Step 1: Choose a Model</h3>
<p>Browse available models at <a href="https://www.sbert.net/docs/pretrained_models.html">Sentence Transformers</a> or the <a href="https://huggingface.co/models?library=sentence-transformers">Hugging Face Model Hub</a>:</p>
<p><strong>General-purpose</strong> (start here for most humanities research):</p>
<ul>
<li><code>all-MiniLM-L6-v2</code>: 384 dimensions, fast, good for most tasks
<ul>
<li>Best for: Getting started, exploratory analysis, large corpora (10,000+ documents)</li>
</ul></li>
<li><code>all-mpnet-base-v2</code>: 768 dimensions, higher quality, slower
<ul>
<li>Best for: Final analysis, smaller corpora where quality matters more than speed</li>
</ul></li>
</ul>
<p><strong>Specialized</strong> (use when you have specific needs):</p>
<ul>
<li><code>paraphrase-multilingual-MiniLM-L12-v2</code>: Cross-lingual embeddings
<ul>
<li>Best for: Comparing French novels to English translations, multilingual archives</li>
</ul></li>
<li><code>allenai-specter</code>: Scientific paper embeddings
<ul>
<li>Best for: History of science, analyzing academic discourse</li>
</ul></li>
<li><code>msmarco-distilbert-base-v4</code>: Optimized for semantic search
<ul>
<li>Best for: Building search engines for large text collections</li>
</ul></li>
</ul>
<p><strong>For historical texts</strong>: Be aware that models trained on modern web text may not capture historical vocabulary well. Consider fine-tuning (advanced) or accept that results require more validation.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sentence_transformers <span class="im">import</span> SentenceTransformer</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load model (downloads automatically first time from Hugging Face)</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> SentenceTransformer(<span class="st">'all-MiniLM-L6-v2'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="step-2-encode-your-corpus" class="level3" data-number="13.7.2">
<h3 data-number="13.7.2" class="anchored" data-anchor-id="step-2-encode-your-corpus"><span class="header-section-number">13.7.2</span> Step 2: Encode Your Corpus</h3>
<div class="sourceCode" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare texts as list</span></span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>texts <span class="op">=</span> df[<span class="st">'text'</span>].to_list()</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Encode (shows progress bar)</span></span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>embeddings <span class="op">=</span> model.encode(texts, show_progress_bar<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Save for reuse (avoid re-encoding)</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>np.save(<span class="st">'corpus_embeddings.npy'</span>, embeddings)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Later, load without re-encoding</span></span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a><span class="co"># embeddings = np.load('corpus_embeddings.npy')</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Why save embeddings</strong>: Encoding is the slow step (~30-60 seconds for 1000 documents). Once encoded, you can run dozens of analyses (searches, clusterings, comparisons) in seconds by loading the saved embeddings.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Visualization Considerations
</div>
</div>
<div class="callout-body-container callout-body">
<p>When reducing 384-dimensional embeddings to 2D for visualization (using UMAP or t-SNE):</p>
<ul>
<li><strong>Information loss</strong>: 2D projections necessarily lose information—clusters that appear close in 2D might be far apart in 384D</li>
<li><strong>Parameter sensitivity</strong>: UMAP’s <code>n_neighbors</code> and <code>min_dist</code> affect layout significantly</li>
<li><strong>Interpretation</strong>: Use visualizations for <strong>exploration</strong> (“what patterns exist?”), validate with quantitative analysis (actual cosine similarities)</li>
<li><strong>Alternative</strong>: For rigorous analysis, work in full 384D space and use metrics like silhouette score to evaluate clusters</li>
</ul>
<p>Visualizations are powerful for communication and hypothesis generation, but always verify patterns quantitatively.</p>
</div>
</div>
</section>
<section id="step-3-analyze" class="level3" data-number="13.7.3">
<h3 data-number="13.7.3" class="anchored" data-anchor-id="step-3-analyze"><span class="header-section-number">13.7.3</span> Step 3: Analyze</h3>
<p><strong>Semantic search</strong>:</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>query_embedding <span class="op">=</span> model.encode([<span class="st">"your search query"</span>])[<span class="dv">0</span>]</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>similarities <span class="op">=</span> cosine_similarity(</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    query_embedding.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), </span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    embeddings</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>)[<span class="dv">0</span>]</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>top_matches <span class="op">=</span> np.argsort(similarities)[::<span class="op">-</span><span class="dv">1</span>][:<span class="dv">10</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Clustering</strong>:</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>kmeans <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">5</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>clusters <span class="op">=</span> kmeans.fit_predict(embeddings)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p><strong>Visualization</strong>:</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> umap <span class="im">import</span> UMAP</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>reducer <span class="op">=</span> UMAP(n_components<span class="op">=</span><span class="dv">2</span>, random_state<span class="op">=</span><span class="dv">42</span>)</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>embeddings_2d <span class="op">=</span> reducer.fit_transform(embeddings)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>plt.scatter(embeddings_2d[:, <span class="dv">0</span>], embeddings_2d[:, <span class="dv">1</span>], </span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>           c<span class="op">=</span>clusters, cmap<span class="op">=</span><span class="st">'viridis'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="step-4-interpret-and-validate" class="level3" data-number="13.7.4">
<h3 data-number="13.7.4" class="anchored" data-anchor-id="step-4-interpret-and-validate"><span class="header-section-number">13.7.4</span> Step 4: Interpret and Validate</h3>
<ul>
<li><strong>Examine cluster samples</strong>: Read representative texts from each cluster</li>
<li><strong>Test edge cases</strong>: Look at low-similarity pairs—why aren’t they similar?</li>
<li><strong>Compare to metadata</strong>: Do clusters align with genres, time periods, authors?</li>
<li><strong>Validate with close reading</strong>: Do computational patterns hold up under careful reading?</li>
</ul>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Iterative Workflow
</div>
</div>
<div class="callout-body-container callout-body">
<p>Embedding analysis is rarely one-and-done:</p>
<ol type="1">
<li><strong>Explore</strong>: Run initial clustering, examine results</li>
<li><strong>Refine</strong>: Adjust preprocessing (filter short texts, combine chunks)</li>
<li><strong>Test</strong>: Try different cluster numbers, alternative models</li>
<li><strong>Validate</strong>: Compare to existing categories, close reading</li>
<li><strong>Interpret</strong>: Build argument connecting patterns to research questions</li>
</ol>
<p><strong>Expect surprises</strong>: The best insights often come from unexpected patterns that challenge your initial hypotheses.</p>
</div>
</div>
</section>
</section>
<section id="see-also" class="level2" data-number="13.8">
<h2 data-number="13.8" class="anchored" data-anchor-id="see-also"><span class="header-section-number">13.8</span> See Also</h2>
<section id="foundational-papers" class="level3" data-number="13.8.1">
<h3 data-number="13.8.1" class="anchored" data-anchor-id="foundational-papers"><span class="header-section-number">13.8.1</span> Foundational Papers</h3>
<ul>
<li><strong>Vaswani et al.&nbsp;(2017)</strong>: “Attention Is All You Need” (introduced transformers)</li>
<li><strong>Devlin et al.&nbsp;(2019)</strong>: “BERT: Pre-training of Deep Bidirectional Transformers” (BERT architecture)</li>
<li><strong>Reimers &amp; Gurevych (2019)</strong>: “Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks” (sentence-transformers)</li>
</ul>
</section>
<section id="online-resources" class="level3" data-number="13.8.2">
<h3 data-number="13.8.2" class="anchored" data-anchor-id="online-resources"><span class="header-section-number">13.8.2</span> Online Resources</h3>
<ul>
<li><strong>Sentence Transformers Documentation</strong>: <a href="https://www.sbert.net/" class="uri">https://www.sbert.net/</a></li>
<li><strong>Hugging Face</strong>: <a href="https://huggingface.co/" class="uri">https://huggingface.co/</a> (the primary platform for AI models and datasets)
<ul>
<li><strong>Model Hub</strong>: <a href="https://huggingface.co/models" class="uri">https://huggingface.co/models</a> (browse 500,000+ models including specialized humanities models)</li>
<li><strong>Datasets</strong>: <a href="https://huggingface.co/datasets" class="uri">https://huggingface.co/datasets</a> (thousands of text corpora)</li>
<li><strong>Transformers Documentation</strong>: <a href="https://huggingface.co/docs/transformers" class="uri">https://huggingface.co/docs/transformers</a> (library for using any model)</li>
</ul></li>
<li><strong>Jay Alammar’s Illustrated Transformer</strong>: <a href="https://jalammar.github.io/illustrated-transformer/" class="uri">https://jalammar.github.io/illustrated-transformer/</a> (visual explanation)</li>
<li><strong>UMAP Documentation</strong>: <a href="https://umap-learn.readthedocs.io/" class="uri">https://umap-learn.readthedocs.io/</a> (dimensionality reduction)</li>
<li><strong>Ollama</strong>: <a href="https://ollama.com/" class="uri">https://ollama.com/</a> (run LLMs locally on your computer—simple interface for Llama, Mistral, etc.)</li>
</ul>
</section>
<section id="related-tutorials" class="level3" data-number="13.8.3">
<h3 data-number="13.8.3" class="anchored" data-anchor-id="related-tutorials"><span class="header-section-number">13.8.3</span> Related Tutorials</h3>
<ul>
<li><strong>Mini Lab 11</strong>: Contextual Embeddings (hands-on exercises with inaugural corpus—start here for interactive learning)</li>
<li><strong>Mini Lab 9</strong>: Vector Models (word2vec, static embeddings)</li>
<li><strong>Mini Lab 10</strong>: Multi-Dimensional Analysis (stylistic dimensions)</li>
<li><strong>Mini Lab 8</strong>: spaCy Basics (NLP preprocessing for embeddings)</li>
</ul>
<p><strong>Relationship to Mini Lab 11</strong>: This tutorial provides conceptual depth, methodological guidance, and connections to other methods. Mini Lab 11 offers hands-on exercises with code you can run and modify. Use both together: read this tutorial for understanding, then work through the lab for practice.</p>
</section>
<section id="methodological-readings" class="level3" data-number="13.8.4">
<h3 data-number="13.8.4" class="anchored" data-anchor-id="methodological-readings"><span class="header-section-number">13.8.4</span> Methodological Readings</h3>
<ul>
<li><strong>Underwood (2015)</strong>: “The Historical Significance of Textual Distances” (applying embeddings to literary history)</li>
<li><strong>Schmidt (2015)</strong>: “Vector Space Models for the Digital Humanities” (overview of embedding methods)</li>
<li><strong>Garg et al.&nbsp;(2018)</strong>: “Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes” (bias in embeddings)</li>
<li><strong>Hamilton et al.&nbsp;(2016)</strong>: “Diachronic Word Embeddings Reveal Statistical Laws of Semantic Change” (tracking meaning shifts)</li>
</ul>
</section>
</section>
<section id="summary" class="level2" data-number="13.9">
<h2 data-number="13.9" class="anchored" data-anchor-id="summary"><span class="header-section-number">13.9</span> Summary</h2>
<p><strong>Contextual embeddings</strong> represent the foundation of modern AI language understanding. By computing context-specific vector representations, they enable:</p>
<ul>
<li><strong>Semantic search</strong>: Find meaning, not keywords</li>
<li><strong>Thematic clustering</strong>: Group by content, not vocabulary</li>
<li><strong>Word-sense analysis</strong>: Track how concepts shift across contexts</li>
<li><strong>LLM literacy</strong>: Understand how ChatGPT “knows” language</li>
</ul>
<p><strong>Key insights</strong>:</p>
<ol type="1">
<li><strong>Context matters</strong>: Same word in different contexts gets different representations</li>
<li><strong>Geometry is meaning</strong>: Semantic relationships = distances in high-dimensional space<br>
</li>
<li><strong>Pre-training enables transfer</strong>: Models trained on web text work (mostly) on historical/literary corpora</li>
<li><strong>Always validate</strong>: Computational patterns are hypotheses, not facts</li>
</ol>
<p><strong>Next steps</strong>: Apply these methods to your own research questions. What semantic patterns exist in your corpus that close reading might miss at scale? How do concepts evolve across your texts? What unexpected connections emerge from semantic clustering?</p>
<p>The goal isn’t to replace humanistic interpretation—it’s to discover patterns that inspire new interpretative questions.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../tutorials/multi-dimensional-analysis.html" class="pagination-link" aria-label="Multi-Dimensional Analysis">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Multi-Dimensional Analysis</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../tutorials/classification-federalist-papers.html" class="pagination-link" aria-label="Classification &amp; Authorship Attribution">
        <span class="nav-page-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Classification &amp; Authorship Attribution</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/browndw/humanities_analytics/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div></div></footer></body></html>