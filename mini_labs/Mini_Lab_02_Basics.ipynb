{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/browndw/humanities_analytics/blob/main/mini_labs/Mini_Lab_02_Basics.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7ztpfu9dC3b"
   },
   "source": [
    "\n",
    "# Mini Lab 2: The Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtIXBI66PM-j"
   },
   "source": [
    "## A simple processing pipeline\n",
    "\n",
    "In order to carry out any sort of computational analysis, we need to convert text into numbers. Although this is now fairly easy to do with computers, it, nonetheless, constitutes a RADICAL reorganization of text.\n",
    "\n",
    "A processing pipeline typically looks something like this:\n",
    "\n",
    "![A processing pipeline](https://raw.githubusercontent.com/browndw/humanities_analytics/refs/heads/main/data/_images/pipeline.svg)\n",
    "\n",
    "To begin seeing what this looks like in practice, let's start with a toy example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmGTsTqkPTIP"
   },
   "source": [
    "### A toy example\n",
    "\n",
    "Frist, we'll create an object consisting of a character string. In this case, the first sentence from *A Tale of Two Cities*:\n",
    "\n",
    "> It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QodcGiYocYlO"
   },
   "outputs": [],
   "source": [
    "totc_txt = \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjXGzmFEdhW8"
   },
   "source": [
    "### Tokenization\n",
    "\n",
    "ğŸ¤” How would you turn this text into something you can count? In other words, we need to convert text into numbers in order to carry out any kind of computational or statistical analysis. So how would you do that?\n",
    "\n",
    "One obvious way would be to simply split the text at spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 231,
     "status": "ok",
     "timestamp": 1736788790995,
     "user": {
      "displayName": "David Brown",
      "userId": "07301131575245799729"
     },
     "user_tz": 300
    },
    "id": "-jk6zLeDeSEq",
    "outputId": "c2d4c122-65de-4a62-fabe-a345b794c0e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'was', 'the', 'best', 'of', 'times,', 'it', 'was', 'the', 'worst', 'of', 'times,', 'it', 'was', 'the', 'age', 'of', 'wisdom,', 'it', 'was', 'the', 'age', 'of', 'foolishness,', 'it', 'was', 'the', 'epoch', 'of', 'belief,', 'it', 'was', 'the', 'epoch', 'of', 'incredulity,', 'it', 'was', 'the', 'season', 'of', 'Light,', 'it', 'was', 'the', 'season', 'of', 'Darkness,', 'it', 'was', 'the', 'spring', 'of', 'hope,', 'it', 'was', 'the', 'winter', 'of', 'despair.']\n"
     ]
    }
   ],
   "source": [
    "split_totc = totc_txt.split(\" \")\n",
    "print(split_totc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5SQGE_-ekkK"
   },
   "source": [
    "### Counting tokens\n",
    "\n",
    "To create a table of counts, we'll first install a library to help us create and manipulate tables (or data frames). For all of our labs, we'll use a Python library called [Polars](https://docs.pola.rs/). There's also a handy [introduction here](https://pbpython.com/polars-intro.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D8DvFT__feo_"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install polars matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97_fMIr6ilbI"
   },
   "source": [
    "\n",
    "Now we can [import](https://www.geeksforgeeks.org/import-module-python/) some useful things.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qUmQWfHdfxyv"
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPe67dbki3uF"
   },
   "source": [
    "Then count our tokens and put them into a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 397,
     "status": "ok",
     "timestamp": 1736788804198,
     "user": {
      "displayName": "David Brown",
      "userId": "07301131575245799729"
     },
     "user_tz": 300
    },
    "id": "9n7UnAMhejtS",
    "outputId": "7ceddcfa-1e88-4395-d560-19b240943dea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>token</th><th>count</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;It&quot;</td><td>1</td></tr><tr><td>&quot;was&quot;</td><td>10</td></tr><tr><td>&quot;the&quot;</td><td>10</td></tr><tr><td>&quot;best&quot;</td><td>1</td></tr><tr><td>&quot;of&quot;</td><td>10</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 2)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ token â”† count â”‚\n",
       "â”‚ ---   â”† ---   â”‚\n",
       "â”‚ str   â”† i64   â”‚\n",
       "â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
       "â”‚ It    â”† 1     â”‚\n",
       "â”‚ was   â”† 10    â”‚\n",
       "â”‚ the   â”† 10    â”‚\n",
       "â”‚ best  â”† 1     â”‚\n",
       "â”‚ of    â”† 10    â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totc_counts = Counter(split_totc)\n",
    "counts_df = pl.DataFrame(totc_counts).transpose(include_header=True, header_name=\"token\").rename({\"column_0\": \"count\"})\n",
    "counts_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHWjto3EsQcv"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "ğŸ“ Coding note: For these labs we'll largely be using a library called [polars](https://docs.pola.rs/api/python/stable/reference/index.html) to construct and manipulate data frames, which are just tabular data structures (i.e., they have rows and columns). The first part of the code (`pl.DataFrame(totc_counts)`) [creates the polars data frame](https://docs.pola.rs/api/python/stable/reference/dataframe/index.html). The second (`transpose(include_header=True, header_name=\"token\")`) [pivots the data frame](https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.transpose.html) so that the rows become the columns and the columns the rows. And the third (`rename({\"column_0\": \"count\"})`) assigns the name \"count\" to the column that has been automatically labeled \"column_0\" when we transposed the data frame.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWh25mhugms9"
   },
   "source": [
    "The process of splitting the string vector into constituent parts is called **tokenization** or [**word segmentation**](https://en.wikipedia.org/wiki/Text_segmentation#Word_segmentation). Think of this as telling the computer how to define a word (or a \"token\", which is a more precise, technical term). In this case, we've done it in an extremely simple way--by defining a token as any string that is bounded by spaces. As a result, we have different tokens for the third-person pronoun *it*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "executionInfo": {
     "elapsed": 120,
     "status": "ok",
     "timestamp": 1736788975282,
     "user": {
      "displayName": "David Brown",
      "userId": "07301131575245799729"
     },
     "user_tz": 300
    },
    "id": "OCegH0bxrpIm",
    "outputId": "2ebd729a-7636-4a81-e938-51f1046b1d24"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (2, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>token</th><th>count</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;It&quot;</td><td>1</td></tr><tr><td>&quot;it&quot;</td><td>9</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (2, 2)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ token â”† count â”‚\n",
       "â”‚ ---   â”† ---   â”‚\n",
       "â”‚ str   â”† i64   â”‚\n",
       "â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
       "â”‚ It    â”† 1     â”‚\n",
       "â”‚ it    â”† 9     â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_df.filter(pl.col(\"token\").str.contains(r\"(?i)^it$\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEn8F1WYvg2V"
   },
   "source": [
    "---\n",
    "\n",
    "ğŸ“ Coding note: The polars library has powerful tools for [filtering/selecting data](https://docs.pola.rs/api/python/stable/reference/dataframe/api/polars.DataFrame.filter.html). Here, we filter on the \"token\" column (`pl.col(\"token\")`) and we want a [string](https://docs.pola.rs/api/python/stable/reference/expressions/string.html) that contains \"it\". The \"(?i)\" signals that the search should be [case insensitive](https://stackoverflow.com/questions/75911005/case-insensitive-search-in-polars-python) (i.e., should include both upper and lower case) and the sybols \"^\" and \"$\" are [regular expression symbols](https://www.sitepoint.com/learn-regex/) indicating the beginning and end of a string respectively. There is [a regex tutorial here](https://regexlearn.com/learn).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8HN2C1-q34w"
   },
   "source": [
    "## Using models to tokenize at scale\n",
    "\n",
    "In order to execute this process at scale, we have a couple of options:\n",
    "\n",
    "\n",
    "1.   We could manipulate our text by, for example, coverting everything to lower case, deleting any character sequences that don't contain a letter, deleting symbols, and deleting punctuation. Then, we could simply simply split on spaces as we did in our simple experiment above. This is called [pre-processing or text cleaning](https://medium.com/@maleeshadesilva21/preprocessing-steps-for-natural-language-processing-nlp-a-beginners-guide-d6d9bf7689c9).\n",
    "2.   Alternatively, we could pass our data to an alogrithm or model with a complex set of rules or probabilities encoded into it.\n",
    "\n",
    "The second option tends to be more computationally intensive. However, model-based parsing allows us to extract additional information from texts. Depending on the model, we can retrieve part-of-speech tags, named entities, sentiment scores, or dependency (i.e., syntactic) relations.\n",
    "\n",
    "For most of these labs, we will be using [spaCy](https://spacy.io/) models to tokenize and parse our data. These models are relative efficient, well-documented, and widely used in industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WMlbDrX_MCy"
   },
   "source": [
    "### Install libraries\n",
    "\n",
    "We'll begin by installing [docuscospacy](https://docuscospacy.readthedocs.io/en/latest/index.html), which we'll use for tokenizing and tagging and great_tables, which (as the name suggests) is using for designing and writing tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iq6bsr--lkCG"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install docuscospacy>=0.3.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQ3em65Y_9UA"
   },
   "source": [
    "### Install the spaCy model\n",
    "\n",
    "Next, we'll download the model ([en_docusco_spacy](https://huggingface.co/browndw/en_docusco_spacy)) that we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8IVhyYIAHFg"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install \"en_docusco_spacy @ https://huggingface.co/browndw/en_docusco_spacy/resolve/main/en_docusco_spacy-1.5-py3-none-any.whl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "in5mJg7ImlHu"
   },
   "source": [
    "### Load the libraries\n",
    "\n",
    "And finally load the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "icE1ZPU-AtlI"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'docuscospacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdocuscospacy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mds\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'docuscospacy'"
     ]
    }
   ],
   "source": [
    "import docuscospacy as ds\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30mZ_IM5BDHY"
   },
   "source": [
    "### Parsing text\n",
    "\n",
    "Parsing text with spaCy requires:\n",
    "\n",
    "\n",
    "1.   Initializing an \"instance\" of our model.\n",
    "2.   Loading some text.\n",
    "3.   Passing the text to the model.\n",
    "\n",
    "So let's do that with our *Tale of Two Cities* example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQicUvIIBAnK"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_docusco_spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ysGnd3TrCKRk"
   },
   "outputs": [],
   "source": [
    "totc_txt = \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of Light, it was the season of Darkness, it was the spring of hope, it was the winter of despair.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcQQo4AyDGpg"
   },
   "outputs": [],
   "source": [
    "doc = nlp(totc_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YS4wsNhEeYd"
   },
   "source": [
    "Now we can see some of what the model generates as outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 290,
     "status": "ok",
     "timestamp": 1736795056208,
     "user": {
      "displayName": "David Brown",
      "userId": "07301131575245799729"
     },
     "user_tz": 300
    },
    "id": "p7Su9Nx3CSax",
    "outputId": "5e1fcee9-666a-4f2c-9bd5-b1067980d103"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It  PPH1 B Narrative\n",
      "was  VBDZ I Narrative\n",
      "the  AT I Narrative\n",
      "best  JJT O \n",
      "of  IO B Narrative\n",
      "times  NNT2 I Narrative\n",
      ",  Y O \n",
      "it  PPH1 B Narrative\n",
      "was  VBDZ I Narrative\n",
      "the  AT I Narrative\n",
      "worst  JJT B Negative\n",
      "of  IO B Narrative\n",
      "times  NNT2 I Narrative\n",
      ",  Y O \n",
      "it  PPH1 B Narrative\n",
      "was  VBDZ I Narrative\n",
      "the  AT I Narrative\n",
      "age  NN1 B Narrative\n",
      "of  IO I Narrative\n",
      "wisdom  NN1 B Positive\n",
      ",  Y O \n",
      "it  PPH1 B Narrative\n",
      "was  VBDZ I Narrative\n",
      "the  AT I Narrative\n",
      "age  NN1 B Narrative\n",
      "of  IO I Narrative\n",
      "foolishness  NN1 B Negative\n",
      ",  Y O \n",
      "it  PPH1 B Narrative\n",
      "was  VBDZ I Narrative\n",
      "the  AT I Narrative\n",
      "epoch  NN1 B Narrative\n",
      "of  IO O \n",
      "belief  NN1 B Character\n",
      ",  Y O \n",
      "it  PPH1 B Narrative\n",
      "was  VBDZ I Narrative\n",
      "the  AT I Narrative\n",
      "epoch  NN1 B Narrative\n",
      "of  IO O \n",
      "incredulity  NN1 O \n",
      ",  Y O \n",
      "it  PPH1 B Narrative\n",
      "was  VBDZ I Narrative\n",
      "the  AT I Narrative\n",
      "season  NNT1 B Narrative\n",
      "of  IO O \n",
      "Light  NN1 O \n",
      ",  Y O \n",
      "it  PPH1 B Narrative\n",
      "was  VBDZ I Narrative\n",
      "the  AT I Narrative\n",
      "season  NNT1 B Narrative\n",
      "of  IO O \n",
      "Darkness  NN1 B Description\n",
      ",  Y O \n",
      "it  PPH1 B Narrative\n",
      "was  VBDZ I Narrative\n",
      "the  AT I Narrative\n",
      "spring  NN1 B Description\n",
      "of  IO I Description\n",
      "hope  NN1 B Future\n",
      ",  Y O \n",
      "it  PPH1 B Narrative\n",
      "was  VBDZ I Narrative\n",
      "the  AT I Narrative\n",
      "winter  NNT1 O \n",
      "of  IO O \n",
      "despair  NN1 B Negative\n",
      ".  Y O \n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.tag_, token.ent_iob_, token.ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F6WWfJONEMP0"
   },
   "source": [
    "### Using docuscospacy to automate the process\n",
    "\n",
    "To use the docuscospacy library we first need a data frame with one column with document ids and another with text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jK8yTaraDvP0"
   },
   "outputs": [],
   "source": [
    "totc_corpus = pl.DataFrame({\"doc_id\": \"totc\", \"text\": [totc_txt]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJbLVNUFF2-t"
   },
   "source": [
    "Now we can pass that corpus to our spaCy instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3BrxKFZbDip_"
   },
   "outputs": [],
   "source": [
    "totc_tokens = ds.docuscope_parse(totc_corpus, nlp_model=nlp, n_process=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7hzpSDCF9Dh"
   },
   "source": [
    "After processing, we can create a number of useful stuctures, like a table of frquency counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 341,
     "status": "ok",
     "timestamp": 1736795815729,
     "user": {
      "displayName": "David Brown",
      "userId": "07301131575245799729"
     },
     "user_tz": 300
    },
    "id": "SpBm1uofEBNy",
    "outputId": "bcba1849-eda8-4a34-ab27-d020a5371aff"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Token</th><th>Tag</th><th>AF</th><th>RF</th><th>Range</th></tr><tr><td>str</td><td>str</td><td>u32</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;it&quot;</td><td>&quot;PPH1&quot;</td><td>10</td><td>166666.666667</td><td>100.0</td></tr><tr><td>&quot;of&quot;</td><td>&quot;IO&quot;</td><td>10</td><td>166666.666667</td><td>100.0</td></tr><tr><td>&quot;the&quot;</td><td>&quot;AT&quot;</td><td>10</td><td>166666.666667</td><td>100.0</td></tr><tr><td>&quot;was&quot;</td><td>&quot;VBDZ&quot;</td><td>10</td><td>166666.666667</td><td>100.0</td></tr><tr><td>&quot;age&quot;</td><td>&quot;NN1&quot;</td><td>2</td><td>33333.333333</td><td>100.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ Token â”† Tag  â”† AF  â”† RF            â”† Range â”‚\n",
       "â”‚ ---   â”† ---  â”† --- â”† ---           â”† ---   â”‚\n",
       "â”‚ str   â”† str  â”† u32 â”† f64           â”† f64   â”‚\n",
       "â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
       "â”‚ it    â”† PPH1 â”† 10  â”† 166666.666667 â”† 100.0 â”‚\n",
       "â”‚ of    â”† IO   â”† 10  â”† 166666.666667 â”† 100.0 â”‚\n",
       "â”‚ the   â”† AT   â”† 10  â”† 166666.666667 â”† 100.0 â”‚\n",
       "â”‚ was   â”† VBDZ â”† 10  â”† 166666.666667 â”† 100.0 â”‚\n",
       "â”‚ age   â”† NN1  â”† 2   â”† 33333.333333  â”† 100.0 â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.frequency_table(totc_tokens).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrHi_-7CImIM"
   },
   "source": [
    "### Processing a larger data set\n",
    "\n",
    "To load in a larger data set, we can read in data that is either on you Google Drive or data that we can link to from the web.\n",
    "\n",
    "Here we will read in a corpus from our course GitHub repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3OL1xobXJDMq"
   },
   "outputs": [],
   "source": [
    "df = pl.read_parquet(\"https://github.com/browndw/humanities_analytics/raw/refs/heads/main/data/data_tables/sample_corpus.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhrrLCxUJLzr"
   },
   "source": [
    "---\n",
    "\n",
    "ğŸ“ Coding note: The polars library has a variety of functions for [reading in data](https://docs.pola.rs/api/python/stable/reference/io.html). Data can also be written into your Google Drive, which we will do in another lab.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNpmAEspJhjt"
   },
   "source": [
    "Now we can use the same `docuscope_parse` function to process the corpus. This will take about 2 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zq_NCVKBJiGH"
   },
   "outputs": [],
   "source": [
    "ds_tokens = ds.docuscope_parse(df, nlp_model=nlp, n_process=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXwucm_1Jz8I"
   },
   "source": [
    "And create a frequency table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vARBgQRJ0kY"
   },
   "outputs": [],
   "source": [
    "wc = ds.frequency_table(ds_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 193,
     "status": "ok",
     "timestamp": 1736796929145,
     "user": {
      "displayName": "David Brown",
      "userId": "07301131575245799729"
     },
     "user_tz": 300
    },
    "id": "oAkNeYMzKeFv",
    "outputId": "0d44dc05-e7c6-4835-f379-9228a82284e3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Token</th><th>Tag</th><th>AF</th><th>RF</th><th>Range</th></tr><tr><td>str</td><td>str</td><td>u32</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;the&quot;</td><td>&quot;AT&quot;</td><td>51030</td><td>49907.38332</td><td>99.75</td></tr><tr><td>&quot;and&quot;</td><td>&quot;CC&quot;</td><td>25288</td><td>24731.685467</td><td>99.5</td></tr><tr><td>&quot;of&quot;</td><td>&quot;IO&quot;</td><td>22492</td><td>21997.195094</td><td>99.75</td></tr><tr><td>&quot;a&quot;</td><td>&quot;AT1&quot;</td><td>22033</td><td>21548.292704</td><td>99.25</td></tr><tr><td>&quot;to&quot;</td><td>&quot;TO&quot;</td><td>16269</td><td>15911.095811</td><td>99.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ Token â”† Tag â”† AF    â”† RF           â”† Range â”‚\n",
       "â”‚ ---   â”† --- â”† ---   â”† ---          â”† ---   â”‚\n",
       "â”‚ str   â”† str â”† u32   â”† f64          â”† f64   â”‚\n",
       "â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
       "â”‚ the   â”† AT  â”† 51030 â”† 49907.38332  â”† 99.75 â”‚\n",
       "â”‚ and   â”† CC  â”† 25288 â”† 24731.685467 â”† 99.5  â”‚\n",
       "â”‚ of    â”† IO  â”† 22492 â”† 21997.195094 â”† 99.75 â”‚\n",
       "â”‚ a     â”† AT1 â”† 22033 â”† 21548.292704 â”† 99.25 â”‚\n",
       "â”‚ to    â”† TO  â”† 16269 â”† 15911.095811 â”† 99.0  â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 156,
     "status": "ok",
     "timestamp": 1736796948589,
     "user": {
      "displayName": "David Brown",
      "userId": "07301131575245799729"
     },
     "user_tz": 300
    },
    "id": "O0GN6GWkKinO",
    "outputId": "1a36f7e2-f2f1-4e10-c4a9-a0ae5dc86368"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>Token</th><th>Tag</th><th>AF</th><th>RF</th><th>Range</th></tr><tr><td>str</td><td>str</td><td>u32</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;zuni&quot;</td><td>&quot;NP1&quot;</td><td>1</td><td>0.978001</td><td>0.25</td></tr><tr><td>&quot;zur&quot;</td><td>&quot;NN1&quot;</td><td>1</td><td>0.978001</td><td>0.25</td></tr><tr><td>&quot;zvezda&quot;</td><td>&quot;NP1&quot;</td><td>1</td><td>0.978001</td><td>0.25</td></tr><tr><td>&quot;zwit&quot;</td><td>&quot;NP1&quot;</td><td>1</td><td>0.978001</td><td>0.25</td></tr><tr><td>&quot;zymo&quot;</td><td>&quot;NP1&quot;</td><td>1</td><td>0.978001</td><td>0.25</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ Token  â”† Tag â”† AF  â”† RF       â”† Range â”‚\n",
       "â”‚ ---    â”† --- â”† --- â”† ---      â”† ---   â”‚\n",
       "â”‚ str    â”† str â”† u32 â”† f64      â”† f64   â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•¡\n",
       "â”‚ zuni   â”† NP1 â”† 1   â”† 0.978001 â”† 0.25  â”‚\n",
       "â”‚ zur    â”† NN1 â”† 1   â”† 0.978001 â”† 0.25  â”‚\n",
       "â”‚ zvezda â”† NP1 â”† 1   â”† 0.978001 â”† 0.25  â”‚\n",
       "â”‚ zwit   â”† NP1 â”† 1   â”† 0.978001 â”† 0.25  â”‚\n",
       "â”‚ zymo   â”† NP1 â”† 1   â”† 0.978001 â”† 0.25  â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhx44IZkLIWS"
   },
   "source": [
    "From the table, it is relatively easy to extract important information like the total word count (or size) of a corpus. Here we simply [sum](https://docs.pola.rs/api/python/dev/reference/expressions/api/polars.sum.html) the \"AF\" (or absolute frequency) column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "executionInfo": {
     "elapsed": 247,
     "status": "ok",
     "timestamp": 1736797081615,
     "user": {
      "displayName": "David Brown",
      "userId": "07301131575245799729"
     },
     "user_tz": 300
    },
    "id": "32DeiIfQKq_Z",
    "outputId": "3b81f885-cc97-4a3a-c4ce-352f4912b416"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 1)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>AF</th></tr><tr><td>u32</td></tr></thead><tbody><tr><td>1022494</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 1)\n",
       "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
       "â”‚ AF      â”‚\n",
       "â”‚ ---     â”‚\n",
       "â”‚ u32     â”‚\n",
       "â•â•â•â•â•â•â•â•â•â•â•¡\n",
       "â”‚ 1022494 â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.select(\"AF\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion Questions\n",
    "\n",
    "1. **Tokenization choices**: Why might simple space-splitting not work well for all languages or text types? What problems did you notice with punctuation in our example?\n",
    "\n",
    "2. **Model complexity**: What additional information does the spaCy model provide compared to simple splitting? How might this be useful for analysis?\n",
    "\n",
    "3. **Trade-offs**: The spaCy approach is more computationally expensive than simple splitting. When might the extra computational cost be worth it? When might it not be?\n",
    "\n",
    "4. **Scale considerations**: We processed a small corpus quickly. What challenges might arise when processing millions of documents? How would you approach that?\n",
    "\n",
    "5. **Critical thinking**: The model assigns part-of-speech tags and other linguistic features. How confident should we be in these assignments? What kinds of errors might the model make?\n",
    "\n",
    "## Experimentation Ideas\n",
    "\n",
    "- Try tokenizing text in a different language or domain (tweets, legal documents, poetry). How do the challenges change?\n",
    "- Compare the output of different spaCy models (small vs. large). What differences do you notice?\n",
    "- Experiment with the frequency table - what are the most common parts of speech? Function words vs content words?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOFH1YCj6dWnlV52KQwQk1I",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "moodswing_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
